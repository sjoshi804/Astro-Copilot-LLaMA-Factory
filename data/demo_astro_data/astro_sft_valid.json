[
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm working with local spectral data and need to retrieve a file named 'stellar_spectra.fits' from my astropy.specutils package data directory. How can I get the local filename using get_pkg_data_filename for further analysis?"
         },
         {
            "from": "assistant",
            "value": "from astropy.utils.data import get_pkg_data_filename\nfrom astropy.io import fits\n\ntry:\n    # Get the path to the FITS file in the package data directory\n    filename = get_pkg_data_filename('stellar_spectra.fits', \n                                   package='astropy.specutils.tests.data')\n    \n    # Open the FITS file to verify access (optional)\n    with fits.open(filename) as hdul:\n        # File is accessible and can be used for further analysis\n        print(f\"Successfully loaded: {filename}\")\n        \nexcept Exception as e:\n    print(f\"Error accessing file: {str(e)}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "Working with X-ray source counts data and need to compute meaningful statistics while handling background fluctuations. How should I implement this?"
         },
         {
            "from": "assistant",
            "value": "from astropy.stats import poisson_conf_interval\nimport numpy as np\n\ndef xray_source_stats(source_counts, background_counts, exposure_time):\n    \"\"\"\n    Compute X-ray source statistics with background subtraction and error handling\n    \n    Parameters:\n    source_counts: Total counts in source region\n    background_counts: Counts in background region\n    exposure_time: Observation exposure time in seconds\n    \"\"\"\n    \n    # Calculate net counts (background-subtracted)\n    net_counts = source_counts - background_counts\n    \n    # Get confidence intervals using frequentist-confidence method\n    conf_intervals = poisson_conf_interval(source_counts, interval='frequentist-confidence')\n    lower_bound, upper_bound = conf_intervals\n    \n    # Calculate count rate and its error\n    count_rate = net_counts / exposure_time\n    rate_error = np.sqrt(source_counts + background_counts) / exposure_time\n    \n    # Compute signal-to-noise ratio\n    if background_counts > 0:\n        snr = net_counts / np.sqrt(source_counts + background_counts)\n    else:\n        snr = net_counts / np.sqrt(source_counts)\n    \n    return {\n        'net_counts': net_counts,\n        'count_rate': count_rate,\n        'rate_error': rate_error,\n        'snr': snr,\n        'conf_interval_lower': lower_bound,\n        'conf_interval_upper': upper_bound\n    }\n\n# Example usage\nsource = 150  # counts in source region\nbkg = 30     # counts in background region\nexp_time = 1000  # seconds\n\nresults = xray_source_stats(source, bkg, exp_time)"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm analyzing a Hubble Space Telescope image and need to determine the pixel scale in arcseconds for both x and y directions. How can I calculate this using my WCS information?"
         },
         {
            "from": "assistant",
            "value": "from astropy.wcs import WCS\nfrom astropy.io import fits\nimport numpy as np\nimport sys\n\ndef calculate_pixel_scale(fits_file):\n    try:\n        # Load the FITS file and WCS information\n        hdul = fits.open(fits_file)\n        wcs = WCS(hdul[0].header)\n\n        # Calculate pixel scales in arcsec for both axes\n        # Get the CD matrix elements (transformation matrix)\n        if hasattr(wcs.wcs, 'cd'):\n            cd = wcs.wcs.cd\n        else:\n            # If CD matrix not present, try CDELT\n            cd = np.array([[wcs.wcs.cdelt[0], 0], \n                          [0, wcs.wcs.cdelt[1]]])\n\n        # Calculate pixel scales using the CD matrix\n        x_scale = np.sqrt(cd[0,0]**2 + cd[0,1]**2) * 3600  # Convert degrees to arcsec\n        y_scale = np.sqrt(cd[1,0]**2 + cd[1,1]**2) * 3600  # Convert degrees to arcsec\n\n        print(f\"Pixel scale in x-direction: {abs(x_scale):.3f} arcsec/pixel\")\n        print(f\"Pixel scale in y-direction: {abs(y_scale):.3f} arcsec/pixel\")\n\n        hdul.close()\n\n    except FileNotFoundError:\n        print(f\"Error: The file '{fits_file}' was not found.\")\n        sys.exit(1)\n    except Exception as e:\n        print(f\"An error occurred: {str(e)}\")\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    # Replace this with your actual FITS file path\n    fits_file = 'path/to/your/hst_image.fits'\n    calculate_pixel_scale(fits_file)"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "How can I create a model set of multiple Gaussians to fit a spectral line that shows evidence of multiple velocity components? I need to set different initial parameters for each component."
         },
         {
            "from": "assistant",
            "value": "from astropy.modeling import models, fitting\nimport numpy as np\n\n# Create sample spectral data (replace with your actual data)\nx = np.linspace(-50, 50, 200)  # Velocity range in km/s\ny = np.random.normal(0, 0.1, x.shape)  # Add some noise to the data\n\n# Create multiple Gaussian components with different initial parameters\n# Component 1: Strong central component\ng1 = models.Gaussian1D(amplitude=1.0, mean=0, stddev=5)\n# Component 2: Blueshifted component\ng2 = models.Gaussian1D(amplitude=0.5, mean=-20, stddev=3)\n# Component 3: Redshifted component\ng3 = models.Gaussian1D(amplitude=0.3, mean=15, stddev=4)\n\n# Combine components into a compound model\ncompound_model = g1 + g2 + g3\n\n# Set parameter constraints (optional)\n# Restrict amplitude to positive values\ncompound_model.amplitude_0.min = 0\ncompound_model.amplitude_1.min = 0\ncompound_model.amplitude_2.min = 0\n\n# Initialize fitter\nfitter = fitting.LevMarLSQFitter()\n\n# Fit the model to the data\nfitted_model = fitter(compound_model, x, y)\n\n# Access individual component parameters\nprint(f\"Component 1: Center={fitted_model.mean_0.value:.2f}, Width={fitted_model.stddev_0.value:.2f}\")\nprint(f\"Component 2: Center={fitted_model.mean_1.value:.2f}, Width={fitted_model.stddev_1.value:.2f}\")\nprint(f\"Component 3: Center={fitted_model.mean_2.value:.2f}, Width={fitted_model.stddev_2.value:.2f}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm developing a telescope control system and need to transform target coordinates to alt-az for the mount. How can I implement real-time coordinate conversion that includes local atmospheric conditions for precise pointing?"
         },
         {
            "from": "assistant",
            "value": "from astropy.coordinates import SkyCoord, EarthLocation, AltAz\nfrom astropy.time import Time\nimport astropy.units as u\n\ndef convert_to_altaz(ra, dec, observatory_info, pressure=0, temperature=20):\n    \"\"\"\n    Convert RA/Dec to Altitude-Azimuth with atmospheric corrections\n    \n    Parameters:\n    ra, dec: Target coordinates in degrees\n    observatory_info: (latitude, longitude, elevation) in degrees and meters\n    pressure: Atmospheric pressure in hPa\n    temperature: Temperature in Celsius\n    \"\"\"\n    # Create observer location\n    lat, lon, elev = observatory_info\n    observer = EarthLocation(lat=lat*u.deg, lon=lon*u.deg, height=elev*u.m)\n    \n    # Create target coordinates\n    target = SkyCoord(ra*u.deg, dec*u.deg, frame='icrs')\n    \n    # Get current time\n    current_time = Time.now()\n    \n    # Create Alt/Az frame with atmospheric conditions\n    altaz_frame = AltAz(obstime=current_time, location=observer,\n                        pressure=pressure*u.hPa,\n                        temperature=temperature*u.deg_C)\n    \n    # Transform coordinates\n    altaz = target.transform_to(altaz_frame)\n    \n    return altaz.alt.deg, altaz.az.deg\n\n# Example usage\nif __name__ == \"__main__\":\n    # Example: Mauna Kea Observatory\n    observatory = (19.8208, -155.4681, 4205)  # lat, lon, elevation\n    \n    # Target: M31 coordinates\n    ra, dec = 10.68458, 41.26917\n    \n    # Get altitude and azimuth\n    alt, az = convert_to_altaz(ra, dec, observatory,\n                              pressure=615,  # typical pressure at altitude\n                              temperature=0)  # typical temperature\n    print(f\"Altitude: {alt:.2f}\u00b0\\nAzimuth: {az:.2f}\u00b0\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm studying stellar populations in globular clusters and have photometric measurements in multiple wavelength bands. How can I clean the data in each band to remove spurious detections while maintaining the true stellar population distribution?"
         },
         {
            "from": "assistant",
            "value": "from astropy.stats import sigma_clip\nfrom astropy.table import Table\nimport numpy as np\n\ndef clean_photometry(data_table, bands, sigma=3, max_iters=5):\n    \"\"\"\n    Clean photometric data using sigma-clipping in multiple bands\n    \n    Parameters:\n    data_table: Astropy Table containing photometry\n    bands: List of column names for different wavelength bands\n    sigma: Number of standard deviations for clipping\n    max_iters: Maximum number of iterations for sigma-clipping\n    \"\"\"\n    \n    # Create mask for valid data points\n    global_mask = np.ones(len(data_table), dtype=bool)\n    \n    for band in bands:\n        # Apply sigma clipping to each band\n        clipped = sigma_clip(data_table[band], \n                           sigma=sigma,\n                           maxiters=max_iters,\n                           masked=True)\n        \n        # Update global mask\n        global_mask &= ~clipped.mask\n    \n    # Return cleaned data\n    return data_table[global_mask]\n\n# Example usage\nif __name__ == \"__main__\":\n    # Create sample data\n    data = Table({\n        'u_mag': np.random.normal(20, 1, 1000),  # U band\n        'g_mag': np.random.normal(19, 1, 1000),  # G band\n        'r_mag': np.random.normal(18, 1, 1000),  # R band\n    })\n    \n    # Add some outliers\n    data['u_mag'][0:10] = 30\n    data['g_mag'][20:30] = 25\n    \n    # Clean the photometry\n    cleaned_data = clean_photometry(data, \n                                  bands=['u_mag', 'g_mag', 'r_mag'],\n                                  sigma=3)\n    \n    print(f\"Original data points: {len(data)}\")\n    print(f\"Cleaned data points: {len(cleaned_data)}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "\"I'm studying a periodic signal in my light curve data that appears to follow a cosine pattern. How can I create a simple model to fit this oscillating signal with adjustable amplitude and frequency?\""
         },
         {
            "from": "assistant",
            "value": "import numpy as np\nfrom astropy.modeling import models, fitting\nfrom astropy.modeling.models import custom_model\nfrom astropy import units as u\n\n# Generate sample time points (e.g., observation times)\nt = np.linspace(0, 10, 100)  # 100 points over 10 time units\n\n# Create synthetic data with some noise\n# True parameters: amplitude=2, frequency=1.5, phase=0.5\ntrue_signal = 2 * np.cos(2 * np.pi * 1.5 * t + 0.5)\nnoise = np.random.normal(0, 0.2, len(t))\ndata = true_signal + noise\n\n# Define a custom periodic model using the decorator\n@custom_model\ndef periodic_model(x, amplitude=1.0, frequency=1.0, phase=0.0):\n    return amplitude * np.cos(2 * np.pi * frequency * x + phase)\n\n# Initialize the model with starting parameters\nmodel = periodic_model(amplitude=1.5, frequency=1.0, phase=0.0)\n\n# Create a fitter and perform the fit\nfitter = fitting.LevMarLSQFitter()\nbest_fit = fitter(model, t, data)\n\n# Print the best-fit parameters\nprint(f\"Best fit parameters:\")\nprint(f\"Amplitude: {best_fit.amplitude.value:.3f}\")\nprint(f\"Frequency: {best_fit.frequency.value:.3f}\")\nprint(f\"Phase: {best_fit.phase.value:.3f}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I've split my exoplanet data into two tables: one containing planetary masses and orbital periods (300 rows), and another with host star properties (300 rows). How can I combine them horizontally while verifying they have exactly the same number of rows?"
         },
         {
            "from": "assistant",
            "value": "from astropy.table import Table, hstack\n\n# Create example tables with sample data instead of reading from files\n# Planets table\nplanets = Table({\n    'planet_name': ['Jupiter', 'Saturn', 'Mars'],\n    'mass': [1.89813e27, 5.68319e26, 6.39e23],\n    'period': [11.862, 29.457, 1.881]\n})\n\n# Stars table\nstars = Table({\n    'star_name': ['Sun', 'Sun', 'Sun'],\n    'temperature': [5778, 5778, 5778],\n    'radius': [696340, 696340, 696340]\n})\n\n# Check if tables have same length\nif len(planets) != len(stars):\n    raise ValueError(f\"Tables have different lengths: {len(planets)} vs {len(stars)} rows\")\n\n# Combine tables horizontally\ncombined_data = hstack([planets, stars])\n\n# Verify the combination worked correctly\nprint(f\"Combined table has {len(combined_data)} rows and {len(combined_data.columns)} columns\")\n\n# Display the combined table\nprint(\"\\nCombined table contents:\")\nprint(combined_data)"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I need to smooth my spectroscopic data of a quasar using a triangular filter. Can you show me how to create a custom 1D kernel with values [0.5, 1.0, 1.5, 1.0, 0.5] using CustomKernel and apply it to my spectrum?"
         },
         {
            "from": "assistant",
            "value": "from astropy.convolution import CustomKernel, convolve\nimport numpy as np\n\n# Create sample spectroscopic data (replace with your actual data)\nspectrum = np.random.normal(1.0, 0.2, 100)  # Example noisy spectrum\n\n# Define triangular kernel weights\nkernel_weights = np.array([0.5, 1.0, 1.5, 1.0, 0.5])\n\n# Create custom kernel and normalize it\ncustom_kernel = CustomKernel(kernel_weights / np.sum(kernel_weights))\n\n# Apply convolution to smooth the spectrum\nsmoothed_spectrum = convolve(spectrum, custom_kernel, boundary='extend')\n\n# Note: 'boundary=extend' handles edge effects by extending the input array"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm studying proper motions from several historical star catalogs with different epoch references. How can I standardize all the epochs to compare the data properly?"
         },
         {
            "from": "assistant",
            "value": "from astropy.time import Time\nfrom astropy.coordinates import SkyCoord\nimport astropy.units as u\n\ndef standardize_epochs(catalogs_data):\n    # Define reference epoch (J2000.0 is common standard)\n    reference_epoch = Time('J2000.0')\n    \n    standardized_positions = []\n    \n    for star_data in catalogs_data:\n        # Unpack star data (example format)\n        ra, dec = star_data['position']\n        pm_ra, pm_dec = star_data['proper_motion']  # in mas/yr\n        catalog_epoch = Time(star_data['epoch'])\n        \n        # Create SkyCoord object for the star's position\n        star_coord = SkyCoord(\n            ra=ra * u.deg,\n            dec=dec * u.deg,\n            pm_ra_cosdec=pm_ra * u.mas/u.yr,\n            pm_dec=pm_dec * u.mas/u.yr,\n            obstime=catalog_epoch\n        )\n        \n        # Calculate time difference in years\n        # Convert to Julian years by accessing the jyear attribute of both Time objects\n        time_diff = (reference_epoch.jyear - catalog_epoch.jyear)\n        \n        # Apply proper motion to get position at reference epoch\n        adjusted_coord = star_coord.apply_space_motion(dt=time_diff * u.yr)\n        \n        standardized_positions.append({\n            'ra': adjusted_coord.ra.deg,\n            'dec': adjusted_coord.dec.deg,\n            'epoch': 'J2000.0'\n        })\n    \n    return standardized_positions\n\n# Example usage\ncatalogs_data = [\n    {'position': (45.0, 30.0), 'proper_motion': (10.0, -5.0), 'epoch': 'B1950.0'},\n    {'position': (46.0, 31.0), 'proper_motion': (12.0, -4.0), 'epoch': 'J1975.0'}\n]\n\nstandardized_data = standardize_epochs(catalogs_data)"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I have measured the radial velocities of 1000 stars in a globular cluster, ranging from -50 to +50 km/s. How can I create a histogram with optimal bin width using Knuth's rule to visualize the velocity distribution? Please provide code that combines knuth_bin_width with matplotlib."
         },
         {
            "from": "assistant",
            "value": "import numpy as np\nfrom astropy.stats import knuth_bin_width\nimport matplotlib.pyplot as plt\n\n# Generate sample radial velocity data (since real data wasn't provided)\nnp.random.seed(42)  # For reproducibility\nvelocities = np.random.normal(0, 20, 1000)  # Simulating 1000 velocities\nvelocities = np.clip(velocities, -50, 50)  # Limiting range to -50 to +50 km/s\n\n# Calculate optimal bin width using Knuth's rule\noptimal_width = knuth_bin_width(velocities)\nnum_bins = int(np.ceil((velocities.max() - velocities.min()) / optimal_width))\n\n# Create and customize the histogram\nplt.figure(figsize=(10, 6))\nplt.hist(velocities, bins=num_bins, density=True, alpha=0.7, color='navy',\n         edgecolor='black')\nplt.xlabel('Radial Velocity (km/s)')\nplt.ylabel('Normalized Frequency')\nplt.title(f'Radial Velocity Distribution\\n(Knuth bin width = {optimal_width:.2f} km/s)')\nplt.grid(True, alpha=0.3)\n\n# Add vertical line at mean velocity\nmean_vel = np.mean(velocities)\nplt.axvline(mean_vel, color='red', linestyle='--', \n            label=f'Mean: {mean_vel:.1f} km/s')\nplt.legend()\n\nplt.tight_layout()\nplt.show()"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "How can I create a custom table for my stellar spectra that includes a calibration version attribute that will be preserved when I share the table with collaborators?"
         },
         {
            "from": "assistant",
            "value": "from astropy.table import Table, QTable\nfrom astropy.units import Quantity\nimport numpy as np\n\n# Create example spectral data\nwavelengths = np.linspace(3000, 7000, 1000)  # Wavelengths in Angstroms\nfluxes = np.random.normal(1e-14, 1e-15, 1000)  # Example flux values\nerrors = np.random.normal(1e-16, 1e-17, 1000)  # Example error values\n\n# Create QTable with metadata\nspectra = QTable([wavelengths, fluxes, errors],\n                names=('wavelength', 'flux', 'error'),\n                meta={'calibration_version': '2.1.3',  # Add calibration version to metadata\n                     'calibration_date': '2023-11-15',\n                     'instrument': 'Example Spectrograph'})\n\n# Add units to columns\nspectra['wavelength'].unit = 'angstrom'\nspectra['flux'].unit = 'erg/(s cm2 angstrom)'\nspectra['error'].unit = 'erg/(s cm2 angstrom)'\n\n# Save table with metadata preserved\nspectra.write('stellar_spectra.fits', overwrite=True)\n\n# Verify metadata is preserved when reading\nloaded_spectra = Table.read('stellar_spectra.fits')\nprint(f\"Calibration version: {loaded_spectra.meta['calibration_version']}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I have a sample of 5000 redshift measurements ranging from 0.1 to 3.0 for galaxies in a cosmological survey. How do I combine scott_bin_width with astropy's histogram function to create a properly binned visualization of the redshift distribution?"
         },
         {
            "from": "assistant",
            "value": "from astropy.stats import scott_bin_width\nfrom astropy.visualization import hist\nimport numpy as np\n\n# Generate sample data (replacing this with your actual data)\nnp.random.seed(42)\nredshifts = np.random.uniform(0.1, 3.0, 5000)\n\n# Calculate optimal bin width using Scott's rule\nbin_width = scott_bin_width(redshifts)\n\n# Calculate number of bins needed\nn_bins = int(np.ceil((redshifts.max() - redshifts.min()) / bin_width))\n\n# Create histogram using astropy's visualization\nhist(redshifts, bins=n_bins, \n     histtype='stepfilled', \n     alpha=0.5,\n     density=True,\n     label=f'Bin width: {bin_width:.3f}')\n\n# Add labels and title\nimport matplotlib.pyplot as plt\nplt.xlabel('Redshift (z)')\nplt.ylabel('Normalized Frequency')\nplt.title('Galaxy Redshift Distribution')\nplt.legend()\nplt.grid(True, alpha=0.3)"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm working on radial velocity measurements and need to correct for the barycentric motion by shifting the observed velocities. How can I apply this velocity offset to my data?"
         },
         {
            "from": "assistant",
            "value": "from astropy.time import Time\nfrom astropy.coordinates import SkyCoord, EarthLocation, get_body_barycentric\nimport astropy.units as u\nimport numpy as np\n\n# Set up observation parameters\nobs_time = Time('2023-12-01 10:30:00')  # Time of observation\ntarget = SkyCoord('12h34m56.7s +12d34m56.7s')  # Target coordinates\nobservatory = EarthLocation.of_site('apo')  # Apache Point Observatory\n\n# Calculate barycentric correction\n# Get Earth's position at two closely spaced times\ndt = 1 * u.minute\ntimes = Time([obs_time.jd - dt.to(u.day).value/2, \n             obs_time.jd + dt.to(u.day).value/2], format='jd')\n\nearth_pos1 = get_body_barycentric('earth', times[0])\nearth_pos2 = get_body_barycentric('earth', times[1])\n\n# Calculate velocity by finite difference\nearth_velocity = (earth_pos2 - earth_pos1) / dt\n\n# Project Earth's velocity onto the line of sight to the target\ntarget_cart = target.cartesian.xyz.value\nbarycorr = np.dot(earth_velocity.xyz.value, target_cart) * u.au/u.d\n# Convert to km/s\nbarycorr = barycorr.to(u.km/u.s)\n\n# Example observed velocities (as a list or array)\nobserved_velocities = [-25.3, -24.8, -26.1] * u.km/u.s\n\n# Apply the correction to get barycentric velocities\n# Subtract because we want to remove Earth's motion\nbarycentric_velocities = observed_velocities - barycorr\n\nprint(f\"Barycentric correction: {barycorr.value:.2f} {barycorr.unit}\")\n# Format each velocity separately\nprint(\"Corrected velocities:\", end=\" \")\nprint([f\"{v.value:.2f}\" for v in barycentric_velocities], barycentric_velocities.unit)"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I have detected a transient object at RA=23h 34m 12s, Dec=-15d 23m 14s. Write code to determine which constellation this object is located in using astropy's coordinate system and get_constellation function."
         },
         {
            "from": "assistant",
            "value": "from astropy.coordinates import SkyCoord\nfrom astropy.coordinates import get_constellation\nimport astropy.units as u\n\n# Create SkyCoord object from the given RA and Dec\ncoords = SkyCoord('23h34m12s', '-15d23m14s', frame='icrs')\n\n# Get the constellation for these coordinates\nconstellation = get_constellation(coords)\n\n# Print result\nprint(f\"The object is located in the constellation: {constellation}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "For my pulsar timing analysis, I want to model the pulse profile as a sum of three Gaussian components with initial peaks at phases 0.2, 0.5, and 0.8, all with width=0.1. How do I create this composite model?"
         },
         {
            "from": "assistant",
            "value": "from astropy.modeling import models, fitting\nimport numpy as np\n\n# Create phase array for the pulse profile\nphase = np.linspace(0, 1, 1000)\n\n# Define three Gaussian components for the pulse profile\ng1 = models.Gaussian1D(amplitude=1.0, mean=0.2, stddev=0.1)\ng2 = models.Gaussian1D(amplitude=1.0, mean=0.5, stddev=0.1)\ng3 = models.Gaussian1D(amplitude=1.0, mean=0.8, stddev=0.1)\n\n# Combine the Gaussians into a composite model\npulse_model = g1 + g2 + g3\n\n# Generate the composite pulse profile\npulse_profile = pulse_model(phase)\n\n# Note: To fit this model to real data, you would use:\n# fitter = fitting.LevMarLSQFitter()\n# fitted_model = fitter(pulse_model, phase_data, intensity_data)"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "For my exoplanet research, I want to define a custom unit \\\"hot_jupiter\\\" that represents 1.5 times Jupiter's mass (about 2.85e27 kg). How can I create this unit using def_unit with both a short name 'hj' and alias 'hot_jup'?"
         },
         {
            "from": "assistant",
            "value": "from astropy import units as u\n\n# Define hot_jupiter unit as 1.5 Jupiter masses\n# Base value is relative to kg (SI unit)\nhot_jupiter = u.def_unit('hot_jupiter', 1.5 * u.M_jup, \n                        doc='Unit representing 1.5 Jupiter masses')\n\n# Register the unit and its aliases\nu.add_enabled_units([hot_jupiter])\nu.add_enabled_aliases({'hj': hot_jupiter, 'hot_jup': hot_jupiter})\n\n# Example usage\nmass = 2.0 * hot_jupiter\nprint(f\"Mass in hot_jupiters: {mass}\")\nprint(f\"Mass in Jupiter masses: {mass.to(u.M_jup)}\")\nprint(f\"Mass in kg: {mass.to(u.kg)}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I have photometric measurements of 200 stars in 5 different filters stored in a 2D array 'phot_data' (shape: 200x5). How can I write code to apply sigma clipping separately to each filter's measurements using a 3.5 sigma threshold and return the clipping bounds for each filter?"
         },
         {
            "from": "assistant",
            "value": "from astropy.stats import sigma_clip\nimport numpy as np\n\ndef get_filter_clipping_bounds(phot_data, sigma=3.5):\n    \"\"\"\n    Apply sigma clipping to photometric measurements in each filter\n    and return the clipping bounds.\n    \n    Parameters:\n    phot_data : 2D array of shape (n_stars, n_filters)\n    sigma : threshold for sigma clipping\n    \n    Returns:\n    Dictionary with lower and upper bounds for each filter\n    \"\"\"\n    \n    n_filters = phot_data.shape[1]\n    bounds = {}\n    \n    # Apply sigma clipping to each filter column\n    for i in range(n_filters):\n        # Get masked array from sigma_clip\n        clipped_data = sigma_clip(phot_data[:, i], sigma=sigma, maxiters=None)\n        \n        # Get the bounds from the mask\n        valid_data = phot_data[:, i][~clipped_data.mask]\n        lower_bound = valid_data.min()\n        upper_bound = valid_data.max()\n        \n        bounds[f'filter_{i}'] = {\n            'lower': lower_bound,\n            'upper': upper_bound\n        }\n    \n    return bounds\n\n# Example usage:\n# clipping_bounds = get_filter_clipping_bounds(phot_data)"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "For my parallax measurements of nearby stars, I need to convert their coordinates while accounting for my observatory's specific location on Earth. What's the code to handle this?"
         },
         {
            "from": "assistant",
            "value": "from astropy.coordinates import EarthLocation, SkyCoord, AltAz\nfrom astropy.time import Time\nimport astropy.units as u\n\n# Define observatory location (example: Mauna Kea Observatory)\nobservatory = EarthLocation(\n    lat=19.8208 * u.deg,\n    lon=-155.4681 * u.deg,\n    height=4207.0 * u.m\n)\n\n# Example star coordinates (RA and Dec)\nstar = SkyCoord(\n    ra='04h35m55.2s',\n    dec='+16d30m33.5s',\n    frame='icrs'\n)\n\n# Set observation time\nobs_time = Time('2023-11-14 10:00:00')\n\n# Create AltAz frame for the specific time and location\naltaz_frame = AltAz(obstime=obs_time, location=observatory)\n\n# Convert to apparent coordinates from observatory's perspective\n# This accounts for parallax, proper motion, and atmospheric refraction\napparent_coords = star.transform_to(altaz_frame)\n\nprint(f\"Altitude: {apparent_coords.alt:.2f}\")\nprint(f\"Azimuth: {apparent_coords.az:.2f}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm analyzing a spectroscopic dataset of binary stars. How can I create a structured quantity that combines radial velocities and orbital periods with their respective units?"
         },
         {
            "from": "assistant",
            "value": "from astropy import units as u\nfrom astropy.table import QTable\nimport numpy as np\n\n# Create sample data for binary stars\nradial_velocities = [150, 200, 175, 225] * u.km/u.s  # Radial velocities with units\norbital_periods = [2.5, 3.1, 4.2, 2.8] * u.day       # Orbital periods with units\n\n# Create a QTable to store the structured data\nbinary_stars = QTable({\n    'radial_velocity': radial_velocities,\n    'orbital_period': orbital_periods,\n    'binary_id': np.arange(1, len(radial_velocities) + 1)\n})\n\n# Calculate derived quantities (example: orbital velocity amplitude)\nbinary_stars['velocity_amplitude'] = binary_stars['radial_velocity'] / 2\n\n# Print the structured data with units\nprint(binary_stars)\n\n# Access specific measurements with units intact\nprint(f\"\\nFirst star velocity: {binary_stars['radial_velocity'][0]}\")\nprint(f\"Second star period: {binary_stars['orbital_period'][1]}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "When fitting the radial velocity curve of my exoplanet host star, I want to link the eccentricity parameters between multiple orbital periods. How should I set up these tied parameters?"
         },
         {
            "from": "assistant",
            "value": "from astropy.modeling import models, fitting, Parameter\nfrom astropy.modeling.core import Fittable1DModel\nfrom astropy import units as u\nimport numpy as np\n\nclass MultiKeplerModel(Fittable1DModel):\n    \"\"\"Custom model for multiple Keplerian orbits with tied parameters\"\"\"\n    \n    # Define parameters for two orbits\n    K1 = Parameter()         # RV semi-amplitude for orbit 1\n    P1 = Parameter()         # Period for orbit 1\n    e1 = Parameter(min=0, max=0.99)  # Eccentricity for orbit 1\n    omega1 = Parameter()     # Argument of periastron for orbit 1\n    \n    K2 = Parameter()         # RV semi-amplitude for orbit 2\n    P2 = Parameter()         # Period for orbit 2\n    e2 = Parameter(min=0, max=0.99)  # Eccentricity for orbit 2\n    omega2 = Parameter()     # Argument of periastron for orbit 2\n\n    def __init__(self, K1=1, P1=10, e1=0, omega1=0,\n                 K2=1, P2=20, e2=0, omega2=0, **kwargs):\n        super().__init__(K1=K1, P1=P1, e1=e1, omega1=omega1,\n                        K2=K2, P2=P2, e2=e2, omega2=omega2, **kwargs)\n        \n        # Tie e2 to e1 using the correct method\n        self.e2.tied = lambda model: model.e1\n\n    def evaluate(self, t, K1, P1, e1, omega1, K2, P2, e2, omega2):\n        # Basic Keplerian model (simplified for example)\n        rv1 = K1 * np.cos(2 * np.pi * t/P1 + omega1) * (1 - e1)\n        rv2 = K2 * np.cos(2 * np.pi * t/P2 + omega2) * (1 - e2)\n        return rv1 + rv2\n\n# Example usage:\nt = np.linspace(0, 100, 1000)  # Time array\nmodel = MultiKeplerModel(K1=50, P1=10, e1=0.1, omega1=0,\n                        K2=30, P2=25, e2=0.1, omega2=np.pi/2)\n\n# The e2 parameter is now tied to e1\nprint(f\"e1 = {model.e1.value}, e2 = {model.e2.value}\")\nmodel.e1 = 0.2  # Changing e1 automatically changes e2\nprint(f\"After change: e1 = {model.e1.value}, e2 = {model.e2.value}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm studying planetary transit data and need to fit a transit model to my light curve. How can I implement a least-squares fitting approach and get the parameter uncertainties for the transit depth and duration?"
         },
         {
            "from": "assistant",
            "value": "import numpy as np\nfrom astropy.modeling import models, fitting\nfrom astropy.modeling.core import Fittable1DModel\nfrom astropy.modeling.parameters import Parameter\nfrom astropy.time import Time\nimport astropy.units as u\n\n# Generate sample transit data (normally you'd load your own data)\nt = np.linspace(0, 2, 1000)  # Time array in days\ntrue_depth = 0.01  # 1% transit depth\ntrue_duration = 0.2  # days\ntrue_t0 = 1.0  # Transit center time\n\n# Create synthetic light curve with some noise\ndef transit_model(t, depth, duration, t0):\n    \"\"\"Simple trapezoidal transit model\"\"\"\n    flux = np.ones_like(t)\n    transit = (t >= (t0 - duration/2)) & (t <= (t0 + duration/2))\n    flux[transit] = 1 - depth\n    return flux\n\n# Add noise to synthetic data\nnp.random.seed(42)\nflux = transit_model(t, true_depth, true_duration, true_t0)\nnoise = np.random.normal(0, 0.001, len(t))\nflux_obs = flux + noise\n\n# Define model class for fitting\nclass TransitModel(Fittable1DModel):\n    depth = Parameter(default=0.01, min=0, max=1)\n    duration = Parameter(default=0.2, min=0)\n    t0 = Parameter(default=1.0)\n    \n    @staticmethod\n    def evaluate(t, depth, duration, t0):\n        return transit_model(t, depth, duration, t0)\n\n# Initialize model and fitter\ntransit = TransitModel()\nfitter = fitting.LevMarLSQFitter()\n\n# Fit the model\nfitted_model = fitter(transit, t, flux_obs)\n\n# Get parameter uncertainties from covariance matrix\ntry:\n    if fitter.fit_info['param_cov'] is not None:\n        uncertainties = np.sqrt(np.diag(fitter.fit_info['param_cov']))\n    else:\n        # If covariance matrix is None, estimate uncertainties from the residuals\n        residuals = flux_obs - fitted_model(t)\n        uncertainties = np.ones(3) * np.std(residuals)\nexcept (KeyError, ValueError):\n    # If there's an error accessing the covariance matrix, use residual-based estimate\n    residuals = flux_obs - fitted_model(t)\n    uncertainties = np.ones(3) * np.std(residuals)\n\nprint(f\"Best-fit parameters:\")\nprint(f\"Depth: {fitted_model.depth.value:.6f} \u00b1 {uncertainties[0]:.6f}\")\nprint(f\"Duration: {fitted_model.duration.value:.6f} \u00b1 {uncertainties[1]:.6f} days\")\nprint(f\"T0: {fitted_model.t0.value:.6f} \u00b1 {uncertainties[2]:.6f} days\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm studying multiple emission lines and need to fix certain parameters during fitting. How do I create a Gaussian1D model with amplitude=3.5, mean=6563, stddev=1.2, where the mean is fixed at 6563 and cannot vary during fitting?"
         },
         {
            "from": "assistant",
            "value": "from astropy.modeling import models, fitting\n\n# Create a Gaussian1D model with specified parameters\ng1 = models.Gaussian1D(amplitude=3.5, mean=6563, stddev=1.2)\n\n# Fix the mean parameter\ng1.mean.fixed = True\n\n# Now the model is ready for fitting and the mean will stay at 6563\n# Example of how to use it with some data:\n# fitter = fitting.LevMarLSQFitter()\n# fitted_model = fitter(g1, x_data, y_data)"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I have measurements of stellar metallicities from a globular cluster survey. Could you demonstrate how to determine the optimal bin width for creating a histogram that will help reveal any potential metallicity populations?"
         },
         {
            "from": "assistant",
            "value": "import numpy as np\nfrom astropy.stats import scott_bin_width\nimport matplotlib.pyplot as plt\n\n# Create sample metallicity data (replace this with your actual data)\nnp.random.seed(42)\nmetallicities = np.concatenate([\n    np.random.normal(-1.8, 0.2, 100),  # Metal-poor population\n    np.random.normal(-1.2, 0.15, 50)    # Metal-rich population\n])\n\n# Calculate optimal bin width using Scott's rule\noptimal_width = scott_bin_width(metallicities)\n\n# Calculate number of bins\nrange_of_data = np.ptp(metallicities)  # Peak-to-peak range\nn_bins = int(np.ceil(range_of_data / optimal_width))\n\n# Create histogram\nplt.figure(figsize=(10, 6))\nplt.hist(metallicities, bins=n_bins, edgecolor='black', alpha=0.7)\nplt.xlabel('[Fe/H]')\nplt.ylabel('Number of Stars')\nplt.title(f'Metallicity Distribution\\nOptimal bin width = {optimal_width:.3f} dex')\n\n# Add text box with statistics\nplt.text(0.02, 0.98, \n         f'N stars: {len(metallicities)}\\n'\n         f'Mean: {np.mean(metallicities):.2f}\\n'\n         f'Std: {np.std(metallicities):.2f}',\n         transform=plt.gca().transAxes,\n         verticalalignment='top',\n         bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n\nplt.show()"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I have a list of 20 corresponding pixel coordinates (x,y) and their sky positions (RA,Dec) for stars in my CCD image. How can I use fit_wcs_from_points to create a WCS solution with a TAN projection centered at RA=150.2, Dec=-35.4 degrees?"
         },
         {
            "from": "assistant",
            "value": "from astropy.wcs import WCS\nfrom astropy.coordinates import SkyCoord\nimport astropy.units as u\nimport numpy as np\n\n# Sample data - replace with your actual coordinates\n# Pixel coordinates (x,y)\npixel_coords = np.array([\n    [100, 200],  # Example coordinates\n    [300, 400],\n    # ... (rest of your 20 points)\n]) \n\n# Sky coordinates (RA, Dec) in degrees\nsky_coords = SkyCoord([\n    [150.1, -35.3],  # Example coordinates\n    [150.3, -35.5],\n    # ... (rest of your 20 points)\n], unit='deg')\n\n# Create initial WCS with TAN projection\nwcs = WCS(naxis=2)\nwcs.wcs.ctype = ['RA---TAN', 'DEC--TAN']\nwcs.wcs.crval = [150.2, -35.4]  # Center coordinates\nwcs.wcs.crpix = [512, 512]      # Reference pixel (assume 1024x1024 image)\nwcs.wcs.cd = [[0.001, 0], [0, 0.001]]  # Initial CD matrix guess\n\n# Fit WCS solution\ntry:\n    # Convert pixel coordinates to expected format (x,y -> x_0,x_1)\n    fitted_wcs = fit_wcs_from_points(\n        xy=pixel_coords,\n        world_coords=np.column_stack([sky_coords.ra.deg, sky_coords.dec.deg]),\n        proj_point=SkyCoord(ra=150.2*u.deg, dec=-35.4*u.deg),\n        projection='TAN',\n        init_wcs=wcs\n    )\n    print(\"WCS solution successfully created!\")\nexcept Exception as e:\n    print(f\"Error fitting WCS: {e}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I need a code snippet to analyze the surface temperature variations of solar-type stars, converting between different temperature units to compare with stellar evolution models."
         },
         {
            "from": "assistant",
            "value": "from astropy import units as u\nfrom astropy.units.quantity import Quantity\n\n# Define sample temperatures for solar-type stars (in Kelvin)\nstellar_temps = [5778, 5943, 5680, 5824, 5907] * u.K\n\n# Convert temperatures to different units for analysis\ndef analyze_stellar_temps(temperatures: Quantity):\n    \"\"\"Analyze stellar temperatures in different units\"\"\"\n    # Convert to Celsius only (as Fahrenheit is not directly supported in astropy)\n    temps_celsius = temperatures.to(u.deg_C, equivalencies=u.temperature())\n    \n    # For Fahrenheit, we can use the formula: \u00b0F = (\u00b0C \u00d7 9/5) + 32\n    temps_fahrenheit = temps_celsius.value * 9/5 + 32\n    \n    # Calculate statistical properties (in Kelvin)\n    mean_temp = temperatures.mean()\n    temp_range = temperatures.max() - temperatures.min()\n    \n    print(f\"Temperature Analysis:\")\n    print(f\"Mean temperature: {mean_temp:.1f}\")\n    print(f\"Temperature range: {temp_range:.1f}\")\n    print(\"\\nTemperatures in different units:\")\n    for k, c, f in zip(temperatures, temps_celsius, temps_fahrenheit):\n        print(f\"{k:.1f} = {c:.1f} = {f:.1f}\u00b0F\")\n\n# Analyze the stellar temperatures\nanalyze_stellar_temps(stellar_temps)\n\n# Compare with Sun's temperature\nsun_temp = 5778 * u.K\ntemp_diff = stellar_temps - sun_temp\nprint(f\"\\nDifference from Solar temperature:\")\nfor diff in temp_diff:\n    print(f\"{diff:.1f}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm comparing different ephemerides for a Venus transit study. Can you show me how to calculate Venus' position on 2024-06-01 using both the built-in and DE432S ephemerides to assess the precision difference?"
         },
         {
            "from": "assistant",
            "value": "from astropy.coordinates import get_body, solar_system_ephemeris, get_sun\nfrom astropy.time import Time\nimport astropy.units as u\n\n# Set observation time\nt = Time('2024-06-01 12:00')\n\n# Get Venus position using built-in ephemeris\nsolar_system_ephemeris.set('builtin')\nvenus_builtin = get_body('venus', t)\nsun_builtin = get_sun(t)\n\n# Get Venus position using DE432s\nsolar_system_ephemeris.set('de432s')\nvenus_de432s = get_body('venus', t)\nsun_de432s = get_sun(t)\n\n# Calculate angular separation between predictions\nsep = venus_builtin.separation(venus_de432s)\n\n# Print results\nprint(f\"Venus position (built-in): {venus_builtin.to_string('decimal')}\")\nprint(f\"Venus position (DE432s): {venus_de432s.to_string('decimal')}\")\nprint(f\"Angular separation between predictions: {sep.to_value(u.arcsec):.3f} arcsec\")\n\n# Calculate separation from Sun for transit study\nsep_sun_builtin = venus_builtin.separation(sun_builtin)\nsep_sun_de432s = venus_de432s.separation(sun_de432s)\nprint(f\"Venus-Sun separation (built-in): {sep_sun_builtin.deg:.4f} deg\")\nprint(f\"Venus-Sun separation (DE432s): {sep_sun_de432s.deg:.4f} deg\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "During a pulsar observation, we measured a companion object at position x=2500, y=-1800, z=3200 kilometers. Write code to transform these cartesian coordinates to spherical coordinates for our orbital dynamics calculations."
         },
         {
            "from": "assistant",
            "value": "from astropy.coordinates import cartesian_to_spherical\nimport astropy.units as u\nimport numpy as np\n\n# Define cartesian coordinates with units\nx = 2500 * u.km\ny = -1800 * u.km\nz = 3200 * u.km\n\n# Convert to spherical coordinates (r, theta, phi)\n# r: radial distance\n# theta: polar/colatitudinal angle\n# phi: azimuthal angle\nr, theta, phi = cartesian_to_spherical(x, y, z)\n\n# Print results in a readable format\nprint(f\"Radial distance (r): {r:.2f}\")\nprint(f\"Polar angle (theta): {theta.to(u.degree):.2f}\")\nprint(f\"Azimuthal angle (phi): {phi.to(u.degree):.2f}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "Our radio telescope data of a nearby galaxy shows some pixelation artifacts. Could you show me how to apply an isotropic smoothing filter that treats all directions equally within a specified radius?"
         },
         {
            "from": "assistant",
            "value": "from astropy.convolution import Gaussian2DKernel, convolve\nfrom astropy.io import fits\nimport numpy as np\n\n# Create sample data instead of loading from file\n# Generate a 100x100 array with some Gaussian sources\nsize = 100\ndata = np.zeros((size, size))\n# Add some artificial sources\nfor _ in range(5):\n    x = np.random.randint(0, size)\n    y = np.random.randint(0, size)\n    amplitude = np.random.uniform(1, 10)\n    data[x, y] = amplitude\n\n# Create a new FITS file with the sample data\nhdu = fits.PrimaryHDU(data)\nhdul = fits.HDUList([hdu])\n\n# Define the smoothing parameters\nkernel_radius = 3  # Size of the smoothing kernel in pixels\nstd_dev = kernel_radius / 3.0  # Standard deviation for Gaussian\n\n# Create a 2D Gaussian kernel for isotropic smoothing\nkernel = Gaussian2DKernel(x_stddev=std_dev)\n\n# Apply convolution with boundary handling\nsmoothed_data = convolve(data, kernel, boundary='extend')\n\n# Save the smoothed data\nhdul[0].data = smoothed_data\nhdul.writeto('smoothed_galaxy.fits', overwrite=True)\nhdul.close()\n\nprint(\"Smoothing completed. Output saved to 'smoothed_galaxy.fits'\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "How can I save my calibrated CCD image array and its corresponding WCS header as a compressed FITS file for sharing with collaborators?"
         },
         {
            "from": "assistant",
            "value": "from astropy.io import fits\nimport numpy as np\n\ndef save_compressed_fits(image_array, wcs_header, output_file):\n    \"\"\"\n    Save an image array with WCS header as a compressed FITS file\n    \n    Parameters:\n    image_array: 2D numpy array of the calibrated image\n    wcs_header: astropy.wcs.WCS header object\n    output_file: Output filename (should end in .fits.gz)\n    \"\"\"\n    # Create a PrimaryHDU with the image data and WCS header\n    hdu = fits.PrimaryHDU(image_array, header=wcs_header.to_header())\n    \n    # Create HDUList\n    hdul = fits.HDUList([hdu])\n    \n    # Save as compressed FITS with Rice compression\n    hdul.writeto(output_file, overwrite=True, \n                 compress='rice_1',  # Rice compression algorithm\n                 quantize_level=16.0)  # Quantization level for compression\n\n# Example usage:\n# Assuming you have your calibrated_image and wcs_header ready\n# calibrated_image = np.array([...])  # Your 2D image array\n# wcs_header = ...  # Your WCS header\n# save_compressed_fits(calibrated_image, wcs_header, 'calibrated_image.fits.gz')"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "Before running my galaxy classification pipeline, I need to check the format of my FITS file containing multi-band observations. What's the best way to review its structure?"
         },
         {
            "from": "assistant",
            "value": "from astropy.io import fits\nfrom astropy.table import Table\n\ndef inspect_fits(filename):\n    \"\"\"\n    Examine the structure of a FITS file containing multi-band observations\n    \"\"\"\n    try:\n        # Open the FITS file\n        with fits.open(filename) as hdul:\n            # Print basic file info\n            print(f\"Number of HDUs: {len(hdul)}\")\n            print(\"\\nFITS structure:\")\n            hdul.info()\n            \n            # Loop through each HDU\n            for i, hdu in enumerate(hdul):\n                print(f\"\\nHDU {i} Header Keywords:\")\n                # Print key header information\n                for card in hdu.header.cards[:10]:  # First 10 keywords\n                    print(f\"  {card}\")\n                \n                # If it's an image HDU, show basic statistics\n                if isinstance(hdu, fits.ImageHDU) or isinstance(hdu, fits.PrimaryHDU):\n                    if hdu.data is not None:\n                        print(f\"  Data shape: {hdu.data.shape}\")\n                        print(f\"  Data type: {hdu.data.dtype}\")\n                \n                # If it's a table HDU, show column information\n                elif isinstance(hdu, fits.BinTableHDU):\n                    print(\"  Table Columns:\")\n                    for col in hdu.columns:\n                        print(f\"    {col.name}: {col.format}\")\n\n    except Exception as e:\n        print(f\"Error reading FITS file: {e}\")\n\n# Usage example:\n# inspect_fits(\"your_galaxy_data.fits\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I need to create a table to organize my spectroscopic observations of multiple stars, including their coordinates, magnitudes, spectral types, and radial velocities. How can I structure this using astropy.table?"
         },
         {
            "from": "assistant",
            "value": "from astropy.table import Table\nfrom astropy.coordinates import SkyCoord\nimport astropy.units as u\n\n# Create example data for demonstration\nstar_data = {\n    'name': ['Vega', 'Sirius', 'Betelgeuse', 'Proxima Cen'],\n    'ra': ['18h36m56.3s', '06h45m08.9s', '05h55m10.3s', '14h29m42.9s'],\n    'dec': ['+38d47m01s', '-16d42m58s', '+07d24m25s', '-62d40m46s'],\n    'vmag': [0.03, -1.46, 0.42, 11.13],\n    'spec_type': ['A0V', 'A1V', 'M2Iab', 'M5Ve'],\n    'rv': [(-13.9 * u.km/u.s), (-7.6 * u.km/u.s), (21.9 * u.km/u.s), (-22.4 * u.km/u.s)]\n}\n\n# Convert coordinates to SkyCoord objects\ncoords = SkyCoord(star_data['ra'], star_data['dec'], frame='icrs')\n\n# Create the table\nstars = Table()\nstars['Name'] = star_data['name']\nstars['RA'] = coords.ra\nstars['DEC'] = coords.dec\nstars['V_mag'] = star_data['vmag']\nstars['Spec_Type'] = star_data['spec_type']\nstars['RV'] = star_data['rv']\n\n# Set units and formats\nstars['RA'].format = '.2f'\nstars['DEC'].format = '.2f'\nstars['V_mag'].format = '.2f'\n\n# Display the table\nprint(stars)\n\n# Optionally save to file\n# stars.write('spectroscopic_observations.fits', format='fits', overwrite=True)"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "What's the code to fit a power-law model to my supernova light curve data and determine the confidence intervals of the fitted parameters?"
         },
         {
            "from": "assistant",
            "value": "from astropy.modeling import models, fitting\nfrom astropy.stats import sigma_clip\nimport numpy as np\n\n# Sample data (replace with your actual data)\ntime = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])  # Days since explosion\nflux = np.array([100, 80, 65, 52, 42, 35, 29, 24, 20, 17])  # Flux measurements\nflux_err = np.array([5, 4, 4, 3, 3, 2, 2, 2, 1, 1])  # Flux uncertainties\n\n# Remove outliers using sigma clipping\nmasked_data = sigma_clip(flux, sigma=3)\ngood_data = ~masked_data.mask\n\n# Initialize power-law model: f(t) = amplitude * t^(-alpha)\npower_law = models.PowerLaw1D(amplitude=100, x_0=0, alpha=1)\n\n# Initialize fitter using least squares with weights from uncertainties\nfitter = fitting.LevMarLSQFitter()\n\n# Fit the model to data\nfitted_model = fitter(power_law, time[good_data], flux[good_data], \n                     weights=1/flux_err[good_data])\n\n# Calculate confidence intervals using covariance matrix\ncov = fitter.fit_info['param_cov']\nif cov is not None:\n    uncertainties = np.sqrt(np.diag(cov))\n    alpha_err = uncertainties[2]  # Error in power-law index\n    amp_err = uncertainties[0]    # Error in amplitude\n    \n    print(f\"Power-law index: {fitted_model.alpha.value:.2f} \u00b1 {alpha_err:.2f}\")\n    print(f\"Amplitude: {fitted_model.amplitude.value:.2f} \u00b1 {amp_err:.2f}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm analyzing a stellar point spread function and need to create a code snippet using Moffat1D to model it with amplitude=2.5, centered at x_0=0, gamma=1.5, and alpha=2.0. Can you show me how to generate and plot this profile over a range from -10 to 10?"
         },
         {
            "from": "assistant",
            "value": "import numpy as np\nfrom astropy.modeling.models import Moffat1D\nimport matplotlib.pyplot as plt\n\n# Create x values array from -10 to 10\nx = np.linspace(-10, 10, 200)\n\n# Create Moffat1D model with specified parameters\n# amplitude: peak intensity\n# x_0: center position\n# gamma: core width parameter\n# alpha: power index that determines wing falloff\nmoffat = Moffat1D(amplitude=2.5, x_0=0, gamma=1.5, alpha=2.0)\n\n# Generate y values using the model\ny = moffat(x)\n\n# Create the plot\nplt.figure(figsize=(8, 6))\nplt.plot(x, y, 'b-', label='Moffat Profile')\nplt.xlabel('Position')\nplt.ylabel('Intensity')\nplt.title('Moffat Point Spread Function')\nplt.grid(True, alpha=0.3)\nplt.legend()\nplt.show()"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm working on a custom galactic distance unit called \\\"zgp\\\" that represents 100 parsecs. How can I define this unit and its SI prefixes in my current namespace with units.def_unit, with a LaTeX format of Z_{gp}?"
         },
         {
            "from": "assistant",
            "value": "# Import necessary components from astropy.units\nfrom astropy import units as u\nfrom astropy.units import def_unit\n\n# Define the zgp unit (1 zgp = 100 parsecs)\n# Note: We remove the prefixes parameter as it's not supported directly\nzgp = def_unit('zgp', 100 * u.pc,\n               doc='Custom galactic distance unit equal to 100 parsecs',\n               format={'latex': r'Z_{gp}'})\n\n# Register the unit so it can be used with other units\nu.add_enabled_units([zgp])\n\n# Example usage\ndistance = 2.5 * zgp  # 250 parsecs\nprint(f\"Distance: {distance}\")\nprint(f\"In parsecs: {distance.to(u.pc)}\")\n\n# Using standard metric prefixes with regular units instead\nlarge_distance = 1.2 * u.kpc  # kiloparsec = 1000 parsecs\nprint(f\"Large distance: {large_distance}\")\nprint(f\"In parsecs: {large_distance.to(u.pc)}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm working on a solar physics project and need to express stellar radii in both solar radii and astronomical units. How can I ensure these units are available for conversion?"
         },
         {
            "from": "assistant",
            "value": "# Import required astropy modules for units and constants\nfrom astropy import units as u\n\n# Define a sample stellar radius (e.g., for a red giant)\nstellar_radius = 50 * u.R_sun  # 50 solar radii\n\n# Convert to astronomical units\nradius_in_au = stellar_radius.to(u.au)\n\n# Print results with units\nprint(f\"Stellar radius: {stellar_radius:.2f}\")\nprint(f\"Same radius in AU: {radius_in_au:.4f}\")\n\n# Demonstrate reverse conversion\nradius_back = radius_in_au.to(u.R_sun)\nprint(f\"Converting back to solar radii: {radius_back:.2f}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "How can I fit a Lorentzian profile to my spectral line data from a quasar emission feature to determine its central wavelength and width? I'd like to visualize the fit alongside the data."
         },
         {
            "from": "assistant",
            "value": "import numpy as np\nfrom astropy.modeling import models, fitting\nfrom astropy.modeling.models import Lorentz1D\nimport matplotlib.pyplot as plt\n\n# Generate sample spectral data (replace with your actual data)\nwavelength = np.linspace(4800, 5200, 200)  # Wavelength in Angstroms\nflux = Lorentz1D(amplitude=10, x_0=5000, fwhm=50)(wavelength)\n# Add some noise to simulate real data\nnp.random.seed(42)\nflux += np.random.normal(0, 0.5, wavelength.shape)\n\n# Initialize Lorentzian model with initial guesses\nlorentz_init = Lorentz1D(amplitude=8, x_0=5000, fwhm=40)\n\n# Create fitter and perform the fit\nfitter = fitting.LevMarLSQFitter()\nlorentz_fit = fitter(lorentz_init, wavelength, flux)\n\n# Plot the results\nplt.figure(figsize=(10, 6))\nplt.plot(wavelength, flux, 'ko', label='Data')\nplt.plot(wavelength, lorentz_fit(wavelength), 'r-', label='Lorentzian fit')\nplt.xlabel('Wavelength (\u00c5)')\nplt.ylabel('Flux')\nplt.legend()\n\n# Print fit parameters\nprint(f\"Central wavelength: {lorentz_fit.x_0.value:.2f} \u00c5\")\nprint(f\"FWHM: {lorentz_fit.fwhm.value:.2f} \u00c5\")\nprint(f\"Amplitude: {lorentz_fit.amplitude.value:.2f}\")\n\nplt.show()"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "Working with globular cluster data (numpy array 'cluster_intensity' with values 0-50000), I need to compare different asinh stretch parameters. How do I apply AsinhStretch with a=0.05 and a=0.5 to see which better reveals the cluster's core and outer regions?"
         },
         {
            "from": "assistant",
            "value": "from astropy.visualization import AsinhStretch\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Create sample data (simulated cluster intensity)\nx, y = np.meshgrid(np.linspace(-5, 5, 100), np.linspace(-5, 5, 100))\nr = np.sqrt(x**2 + y**2)\ncluster_intensity = np.exp(-r**2/2.0)  # Create a Gaussian-like intensity distribution\n\n# Create two different asinh stretches\nstretch_small_a = AsinhStretch(a=0.05)\nstretch_large_a = AsinhStretch(a=0.5)\n\n# Apply stretches to the data\n# Note: stretch expects values between 0-1, so we normalize first\nnorm_data = cluster_intensity / cluster_intensity.max()\nstretched_small_a = stretch_small_a(norm_data)\nstretched_large_a = stretch_large_a(norm_data)\n\n# Visualize results side by side\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n\n# Plot both stretched versions\nim1 = ax1.imshow(stretched_small_a, origin='lower')\nax1.set_title('AsinhStretch with a=0.05')\n\nim2 = ax2.imshow(stretched_large_a, origin='lower')\nax2.set_title('AsinhStretch with a=0.5')\n\n# Add colorbars\nfig.colorbar(im1, ax=ax1)\nfig.colorbar(im2, ax=ax2)\n\nplt.tight_layout()\nplt.show()"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm studying variable stars and need to create a data table column for magnitude measurements where some data points are flagged as unreliable due to poor observing conditions. How can I create this column with appropriate masking?"
         },
         {
            "from": "assistant",
            "value": "from astropy.table import Table, Column, MaskedColumn\nimport numpy as np\n\n# Create sample magnitude data with some unreliable measurements\nmagnitudes = [12.3, 12.5, 12.1, 12.8, 12.4, 12.9, 12.2, 12.6]\nis_unreliable = [False, False, True, False, True, False, False, False]\n\n# Create a masked column for magnitudes\n# Mask (hide) values where is_unreliable is True\nmag_column = MaskedColumn(\n    data=magnitudes,\n    name='magnitude',\n    mask=is_unreliable,\n    description='V-band magnitude',\n    unit='mag'\n)\n\n# Create a table with the masked magnitude column\nvariable_star = Table()\nvariable_star.add_column(mag_column)\n\n# Print the table - masked values will show as \"--\"\nprint(variable_star)\n\n# Calculate mean magnitude excluding masked values\nmean_mag = np.ma.mean(variable_star['magnitude'])\nprint(f\"\\nMean magnitude (excluding unreliable data): {mean_mag:.2f}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm calibrating my radio telescope data and need to scale the observed flux densities by 1.25 to match the standard source. Show me how to create this scaling transformation using astropy.modeling.models.Scale."
         },
         {
            "from": "assistant",
            "value": "from astropy.modeling import models\n\n# Create a scaling transformation to adjust flux densities\n# Scale factor of 1.25 will multiply all input values by 1.25\nflux_scale = models.Scale(factor=1.25)\n\n# Example usage with sample flux density data\nobserved_flux = [2.5, 3.1, 4.2, 1.8]  # Jy (example flux densities)\n\n# Apply scaling transformation\ncalibrated_flux = flux_scale(observed_flux)\n\n# Print results\nprint(f\"Original flux densities (Jy): {observed_flux}\")\nprint(f\"Calibrated flux densities (Jy): {calibrated_flux}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm studying supernovae and have gathered two samples of peak luminosity data from different redshift ranges. What's the code to determine if these samples are drawn from the same parent distribution using Astropy's statistical tools?"
         },
         {
            "from": "assistant",
            "value": "from scipy.stats import ks_2samp\nimport numpy as np\n\n# Sample data of peak luminosities (in log solar luminosities)\n# Replace these with your actual data arrays\nlow_z_luminosities = np.array([8.5, 8.7, 8.6, 8.8, 8.4, 8.9, 8.3])\nhigh_z_luminosities = np.array([8.7, 8.9, 8.8, 9.0, 8.6, 8.8, 8.7, 8.9])\n\n# Perform Kolmogorov-Smirnov test\n# Returns statistic (D) and p-value\nstatistic, pvalue = ks_2samp(low_z_luminosities, high_z_luminosities)\n\n# Print results\nprint(f\"KS test statistic: {statistic:.3f}\")\nprint(f\"p-value: {pvalue:.3f}\")\n\n# Interpret results\nalpha = 0.05  # Standard significance level\nif pvalue < alpha:\n    print(\"Samples likely come from different distributions (reject H0)\")\nelse:\n    print(\"Cannot reject that samples come from the same distribution\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "During my research on galaxy clusters, I need to verify the content structure of my observation file 'cluster_A1689.fits'. What code can show me the summary of each HDU's properties?"
         },
         {
            "from": "assistant",
            "value": "import os\nfrom astropy.io import fits\n\ndef read_fits_file(file_path):\n    # Check if file exists\n    if not os.path.exists(file_path):\n        print(f\"Error: File '{file_path}' not found!\")\n        print(\"Please check if:\")\n        print(\"1. The file name is correct\")\n        print(\"2. The file is in the correct directory\")\n        print(\"3. You have provided the full path if the file is in a different directory\")\n        return\n\n    try:\n        # Open the FITS file\n        with fits.open(file_path) as hdul:\n            # Print a summary of all HDUs (Header Data Units)\n            print(\"\\nFITS file summary:\")\n            hdul.info()\n            \n            # Optional: Print detailed header information for each HDU\n            for i, hdu in enumerate(hdul):\n                print(f\"\\n=== Detailed header for HDU {i} ===\")\n                print(hdu.header)\n    except Exception as e:\n        print(f\"An error occurred while reading the FITS file: {str(e)}\")\n\n# Example usage\n# Replace with the actual path to your FITS file\nfile_path = 'path/to/your/cluster_A1689.fits'\nread_fits_file(file_path)"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "As part of my black hole research, I need to calculate the total energy equivalent of matter being accreted into the black hole. Can you show me how to convert between mass and energy units for this scenario?"
         },
         {
            "from": "assistant",
            "value": "from astropy import units as u\nfrom astropy import constants as const\n\ndef mass_to_energy(mass_value, mass_unit='solMass'):\n    \"\"\"\n    Convert mass to energy using E = mc^2\n    Parameters:\n        mass_value: numerical value of mass\n        mass_unit: string specifying mass unit (default: solar masses)\n    Returns:\n        energy in ergs and joules\n    \"\"\"\n    # Create mass quantity with units\n    mass = mass_value * u.Unit(mass_unit)\n    \n    # Calculate energy using E = mc^2\n    # const.c is speed of light from astropy constants\n    energy = mass * const.c * const.c\n    \n    # Convert to ergs and joules separately\n    energy_erg = energy.to(u.erg)\n    energy_joule = energy.to(u.J)\n    \n    return energy_erg, energy_joule\n\n# Example calculation for 1 solar mass\naccretion_mass = 1.0\nenergy_erg, energy_joule = mass_to_energy(accretion_mass)\n\nprint(f\"Energy equivalent of {accretion_mass} solar mass:\")\nprint(f\"In ergs: {energy_erg:.2e}\")\nprint(f\"In joules: {energy_joule:.2e}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm calibrating my telescope's detector response and need to model the linear relationship between incident flux and measured counts. Show me how to create a model with bounds on both slope and intercept parameters."
         },
         {
            "from": "assistant",
            "value": "from astropy.modeling import models, fitting, Parameter\nfrom astropy.modeling.fitting import LevMarLSQFitter\nimport numpy as np\n\n# Create a custom bounded linear model class\nclass BoundedLinearModel(models.Linear1D):\n    \"\"\"Linear model with bounded parameters for detector calibration\"\"\"\n    \n    # Override parameters with bounds\n    slope = Parameter(default=1.0, bounds=(0.0, 2.0))  # Positive slope only\n    intercept = Parameter(default=0.0, bounds=(-100.0, 100.0))\n\n# Create sample data (replace with real measurements)\ntrue_flux = np.linspace(0, 1000, 50)\nmeasured_counts = 1.2 * true_flux + 10.0  # Example relationship\nnoise = np.random.normal(0, 20, 50)\nmeasured_counts += noise  # Add some noise\n\n# Initialize model and fitter\ninit_model = BoundedLinearModel()\nfitter = LevMarLSQFitter()\n\n# Fit the model\nfitted_model = fitter(init_model, true_flux, measured_counts)\n\n# Print results\nprint(f\"Fitted slope: {fitted_model.slope.value:.3f}\")\nprint(f\"Fitted intercept: {fitted_model.intercept.value:.3f}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "For my exoplanet transit depth measurements [0.0156, 0.0158, 0.0155, 0.0157, 0.0154], show me how to use jackknife_resampling to assess the statistical reliability of the depth estimation."
         },
         {
            "from": "assistant",
            "value": "from astropy.stats import jackknife_resampling, jackknife_stats\nimport numpy as np\n\n# Transit depth measurements\ndepths = np.array([0.0156, 0.0158, 0.0155, 0.0157, 0.0154])\n\n# Perform jackknife resampling\n# This creates n subsamples, each with one observation removed\nresamples = jackknife_resampling(depths)\n\n# Calculate statistics (mean, variance, std error, bias) using jackknife method\nmean_estimate, var_estimate, std_error, bias = jackknife_stats(depths, np.mean)\n\nprint(f\"Jackknife mean estimate: {mean_estimate:.6f}\")\nprint(f\"Standard error: {std_error:.6f}\")\nprint(f\"95% confidence interval: ({mean_estimate - 2*std_error:.6f}, {mean_estimate + 2*std_error:.6f})\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm studying star formation in the Orion Nebula and have coordinates and distances for 150 young stellar objects. I need code to match these against a reference catalog of 500 known protostars to identify potential matches within 1 parsec in 3D space."
         },
         {
            "from": "assistant",
            "value": "from astropy.coordinates import SkyCoord\nimport astropy.units as u\nimport numpy as np\n\ndef match_ysos_3d(yso_coords, yso_distances, ref_coords, ref_distances, max_dist=1*u.pc):\n    \"\"\"\n    Match Young Stellar Objects (YSOs) against reference catalog in 3D space\n    \n    Parameters:\n    -----------\n    yso_coords, ref_coords : SkyCoord\n        Coordinates of YSOs and reference stars\n    yso_distances, ref_distances : Quantity\n        Distances to YSOs and reference stars in parsecs\n    max_dist : Quantity\n        Maximum matching distance (default: 1 parsec)\n    \"\"\"\n    # Convert sky coordinates and distances to 3D cartesian coordinates\n    yso_xyz = yso_coords.cartesian.xyz * yso_distances[:, np.newaxis]\n    ref_xyz = ref_coords.cartesian.xyz * ref_distances[:, np.newaxis]\n    \n    # Initialize arrays for matches\n    matches = []\n    \n    # For each YSO, find all reference stars within max_dist\n    for i, yso_pos in enumerate(yso_xyz.T):\n        # Calculate 3D distances to all reference stars\n        distances = np.sqrt(np.sum((ref_xyz.T - yso_pos)**2, axis=1))\n        \n        # Find matches within threshold\n        match_indices = np.where(distances < max_dist.to(u.pc).value)[0]\n        \n        if len(match_indices) > 0:\n            matches.append((i, match_indices, distances[match_indices]))\n            \n    return matches\n\n# Example usage (assuming you have your data):\n\"\"\"\n# Create sample coordinates (you would load your actual data instead)\nyso_coords = SkyCoord(ra=[83.8, 83.9]*u.deg, dec=[-5.4, -5.5]*u.deg)\nyso_distances = [400, 410] * u.pc\nref_coords = SkyCoord(ra=[83.82, 83.85]*u.deg, dec=[-5.38, -5.45]*u.deg)\nref_distances = [405, 408] * u.pc\n\nmatches = match_ysos_3d(yso_coords, yso_distances, ref_coords, ref_distances)\n\"\"\""
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "For my exoplanet transit observations, I need to compute the time differences between multiple transit events and convert them from UTC to TDB time scale. How should I implement this?"
         },
         {
            "from": "assistant",
            "value": "from astropy.time import Time\nimport numpy as np\n\n# Sample transit times in UTC (replace with your actual observations)\ntransit_times_utc = [\n    '2023-01-01 12:00:00',\n    '2023-01-08 13:30:00',\n    '2023-01-15 15:00:00'\n]\n\n# Create Time object with UTC scale\ntimes_utc = Time(transit_times_utc, scale='utc', format='iso')\n\n# Convert to TDB (Barycentric Dynamical Time)\ntimes_tdb = times_utc.tdb\n\n# Calculate time differences between consecutive transits in days\ntime_differences = np.diff(times_tdb.jd)\n\n# Print results\nprint(\"Transit times in TDB:\")\nfor t in times_tdb:\n    print(f\"UTC: {t.utc.iso} -> TDB: {t.iso}\")\n\nprint(\"\\nTime differences between consecutive transits (days):\")\nfor i, diff in enumerate(time_differences):\n    print(f\"Between transit {i+1} and {i+2}: {diff:.6f} days\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I have an image array of galactic radio emission ranging from 0 to 255. How can I apply a logarithmic stretch using LogStretch with a=1000 to enhance the visibility of faint structures?"
         },
         {
            "from": "assistant",
            "value": "from astropy.visualization import LogStretch\nimport numpy as np\n\n# Create a sample image array (you should replace this with your actual image)\n# Here creating a 100x100 example array with random values between 0 and 255\nimage = np.random.randint(0, 256, size=(100, 100), dtype=np.uint8)\n\n# Create the logarithmic stretch transform with a=1000\nlog_stretch = LogStretch(a=1000)\n\n# Normalize the image to [0,1] range first\n# since LogStretch expects values in this range\nimage_normalized = image / 255.0\n\n# Apply the logarithmic stretch\n# This will enhance faint features while compressing bright ones\nimage_stretched = log_stretch(image_normalized)\n\n# Scale back to 0-255 range\nimage_stretched_255 = (image_stretched * 255).astype(np.uint8)"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I need to create a structured quantity for a binary star system that combines the orbital period (2.7 years) and separation distance (24 AU). Show me how to use astropy.units.structured to represent this data."
         },
         {
            "from": "assistant",
            "value": "from astropy import units as u\nfrom astropy.table import QTable\nimport numpy as np\n\n# Create a QTable to store binary star properties\nbinary_table = QTable()\nbinary_table['period'] = [2.7] * u.year\nbinary_table['separation'] = [24.0] * u.AU\n\n# Example of accessing the components\nprint(f\"Orbital Period: {binary_table['period'][0]}\")  # Shows period with units\nprint(f\"Separation: {binary_table['separation'][0]}\")  # Shows separation with units\n\n# We can also convert units if needed\nperiod_days = binary_table['period'].to(u.day)\nseparation_km = binary_table['separation'].to(u.km)\nprint(f\"Period in days: {period_days[0]}\")\nprint(f\"Separation in km: {separation_km[0]}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "How do I apply a box smoothing filter to my noisy galaxy image to reduce pixel-to-pixel variations while preserving the overall structure?"
         },
         {
            "from": "assistant",
            "value": "from astropy.io import fits\nfrom astropy.convolution import Box2DKernel, convolve\nimport numpy as np\nimport os\nimport sys\n\ndef smooth_galaxy_image(input_file, output_file, kernel_width=3):\n    try:\n        # Check if input file exists\n        if not os.path.exists(input_file):\n            raise FileNotFoundError(f\"Input file '{input_file}' not found. Please check the file path.\")\n\n        # Read the galaxy image\n        hdul = fits.open(input_file)\n        data = hdul[0].data\n\n        # Create a 2D box kernel for smoothing\n        box_kernel = Box2DKernel(width=kernel_width)\n\n        # Apply the smoothing filter\n        smoothed_data = convolve(data, box_kernel, boundary='extend')\n\n        # Save the smoothed image\n        hdul[0].data = smoothed_data\n        \n        # Check if output directory exists, if not create it\n        output_dir = os.path.dirname(output_file)\n        if output_dir and not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n            \n        hdul.writeto(output_file, overwrite=True)\n        hdul.close()\n\n        # Calculate and print noise reduction\n        original_std = np.std(data)\n        smoothed_std = np.std(smoothed_data)\n        print(f\"Noise reduction: {(1 - smoothed_std/original_std)*100:.1f}%\")\n        \n        return True\n\n    except FileNotFoundError as e:\n        print(f\"Error: {str(e)}\")\n        return False\n    except Exception as e:\n        print(f\"An unexpected error occurred: {str(e)}\")\n        return False\n\nif __name__ == \"__main__\":\n    # Example usage\n    input_fits = \"path/to/your/galaxy_image.fits\"  # Replace with actual path\n    output_fits = \"path/to/your/smoothed_galaxy.fits\"  # Replace with desired output path\n    \n    success = smooth_galaxy_image(input_fits, output_fits)\n    if not success:\n        sys.exit(1)"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm studying stellar positions from Santiago, Chile (33.4489\u00b0 S, 70.6693\u00b0 W). How can I convert the position of Antares (RA=16h29m24s, Dec=-26\u00b025'55\") to altitude and azimuth coordinates for 2024-07-15 03:00 UT, considering full atmospheric effects (temperature=15\u00b0C, pressure=1013 hPa, relative_humidity=0.5, wavelength=5500 \u00c5)?"
         },
         {
            "from": "assistant",
            "value": "from astropy.coordinates import SkyCoord, EarthLocation, AltAz\nfrom astropy.time import Time\nimport astropy.units as u\n\n# Define the observer's location (Santiago, Chile)\nsantiago = EarthLocation(lat=-33.4489*u.deg, \n                        lon=-70.6693*u.deg, \n                        height=570*u.m)  # approximate elevation\n\n# Create time object for the observation\nobs_time = Time('2024-07-15 03:00:00')\n\n# Define Antares' position\nantares = SkyCoord('16h29m24s', '-26d25m55s', frame='icrs')\n\n# Create AltAz frame with full atmospheric conditions\naltaz_frame = AltAz(obstime=obs_time, \n                    location=santiago,\n                    pressure=1013*u.hPa,\n                    temperature=15*u.deg_C,\n                    relative_humidity=0.5,\n                    obswl=5500*u.AA)\n\n# Transform Antares' position to altitude/azimuth\nantares_altaz = antares.transform_to(altaz_frame)\n\n# Print results with atmospheric refraction included\nprint(f\"Altitude: {antares_altaz.alt:.2f}\")\nprint(f\"Azimuth: {antares_altaz.az:.2f}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "How can I determine the optimal bin width for creating a histogram of my star's radial velocity measurements while assuming the data follows a roughly normal distribution?"
         },
         {
            "from": "assistant",
            "value": "from astropy.stats import scott_bin_width\nimport numpy as np\n\ndef get_optimal_bins(radial_velocities):\n    \"\"\"\n    Determine optimal histogram bin width for radial velocity data\n    using Scott's rule, which is optimal for normally distributed data\n    \"\"\"\n    \n    # Calculate optimal bin width using Scott's rule\n    optimal_width = scott_bin_width(radial_velocities)\n    \n    # Calculate number of bins based on data range and optimal width\n    n_bins = int(np.ceil((np.max(radial_velocities) - np.min(radial_velocities)) / optimal_width))\n    \n    # Return both bin width and number of bins\n    return optimal_width, n_bins\n\n# Example usage:\n# Simulate some example radial velocity measurements (in km/s)\nrv_data = np.random.normal(loc=30, scale=5, size=1000)\n\n# Get optimal binning parameters\nbin_width, num_bins = get_optimal_bins(rv_data)\n\nprint(f\"Optimal bin width: {bin_width:.2f} km/s\")\nprint(f\"Recommended number of bins: {num_bins}\")\n\n# These values can be used in histogram plotting:\n# plt.hist(rv_data, bins=num_bins)"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "As part of my spectroscopic analysis, I need to create a table containing stellar parameters for 3 stars: temperatures [5800, 6200, 5600] K, metallicities [-0.1, 0.2, -0.3] dex, and radii [1.1, 1.5, 0.9] solar radii. How can I create an Astropy Table with these values and add appropriate metadata describing the observation date as '2023-05-15'?"
         },
         {
            "from": "assistant",
            "value": "from astropy.table import Table\nfrom astropy.time import Time\nimport astropy.units as u\n\n# Create Table with stellar parameters including units\nstars = Table([\n    [5800, 6200, 5600],\n    [-0.1, 0.2, -0.3],\n    [1.1, 1.5, 0.9]\n],\n    names=('temperature', 'metallicity', 'radius'),\n    units=(u.K, None, u.solRad),  # Set units: Kelvin, dimensionless (dex), solar radii\n    meta={'observation_date': Time('2023-05-15').iso}  # Add metadata with observation date\n)\n\n# Add column descriptions\nstars['temperature'].description = 'Effective temperature'\nstars['metallicity'].description = 'Metallicity [Fe/H] in dex'\nstars['radius'].description = 'Stellar radius'\n\n# Print the formatted table with metadata\nprint(\"Observation Date:\", stars.meta['observation_date'])\nprint(stars)"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm studying a white dwarf star with a radius of 0.015 solar radii. Can you write a code snippet using astropy units to express this in terms of Earth radii and kilometers?"
         },
         {
            "from": "assistant",
            "value": "from astropy import units as u\n\n# Define the white dwarf radius in solar radii\nwd_radius = 0.015 * u.solRad\n\n# Convert to Earth radii\nradius_earth = wd_radius.to(u.earthRad)\n\n# Convert to kilometers\nradius_km = wd_radius.to(u.km)\n\n# Print results with proper units\nprint(f\"White Dwarf Radius:\")\nprint(f\"In Solar Radii: {wd_radius:.4f}\")\nprint(f\"In Earth Radii: {radius_earth:.4f}\")\nprint(f\"In Kilometers: {radius_km:.1f}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm working on modeling variable star light curves with a combination of periodic functions. What's the proper way to use SLSQPLSQFitter to optimize the model parameters while handling the non-linear nature of the problem?"
         },
         {
            "from": "assistant",
            "value": "from astropy.modeling import models, fitting\nfrom astropy.modeling.models import custom_model\nimport numpy as np\n\n# Create a custom model combining periodic functions\n@custom_model\ndef variable_star_model(x, amp1=1.0, period1=0.5, phase1=0.0, \n                       amp2=0.5, period2=0.25, phase2=0.0, offset=0.0):\n    \"\"\"\n    Custom model for variable star combining two sinusoids\n    Parameters represent amplitudes, periods, phases and vertical offset\n    \"\"\"\n    return (amp1 * np.sin(2*np.pi/period1 * x + phase1) + \n            amp2 * np.sin(2*np.pi/period2 * x + phase2) + offset)\n\n# Generate sample data (replace with real data)\nnp.random.seed(0)\nx = np.linspace(0, 10, 200)\ntrue_model = variable_star_model(amp1=2.0, period1=1.0, phase1=0.1,\n                               amp2=0.5, period2=0.3, phase2=0.2, offset=1.0)\ny = true_model(x) + np.random.normal(0, 0.1, x.shape)\n\n# Initialize model with starting parameters\nmodel_init = variable_star_model(amp1=1.5, period1=1.2, phase1=0.0,\n                               amp2=0.3, period2=0.4, phase2=0.0, offset=0.8)\n\n# Set up SLSQP fitter with bounds\nfitter = fitting.SLSQPLSQFitter()\n# Set parameter bounds to constrain the solution\nmodel_init.amp1.bounds = (0, 5)\nmodel_init.period1.bounds = (0.1, 2)\nmodel_init.phase1.bounds = (-np.pi, np.pi)\nmodel_init.amp2.bounds = (0, 2)\nmodel_init.period2.bounds = (0.1, 1)\nmodel_init.phase2.bounds = (-np.pi, np.pi)\n\n# Fit the model\nfitted_model = fitter(model_init, x, y)\n\n# Print results\nprint(\"Fitted parameters:\")\nfor param in fitted_model.param_names:\n    print(f\"{param}: {getattr(fitted_model, param).value:.3f}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "For my solar physics research, I'm tracking coronal mass ejection directions and need to find their average propagation angle. What's the best way to compute this mean angle?"
         },
         {
            "from": "assistant",
            "value": "from astropy.stats import circmean\nimport numpy as np\nfrom astropy import units as u\n\ndef compute_mean_cme_angle(angles):\n    \"\"\"\n    Compute mean propagation angle of CMEs using circular statistics\n    \n    Parameters:\n    angles : array-like\n        List of CME propagation angles in degrees from solar north\n    \n    Returns:\n    mean_angle : Quantity\n        Mean propagation angle with units of degrees\n    \"\"\"\n    # Convert angles to radians for computation\n    angles_rad = np.radians(angles)\n    \n    # Compute circular mean (handles wraparound at 360\u00b0)\n    mean_angle_rad = circmean(angles_rad)\n    \n    # Convert back to degrees and attach units\n    mean_angle = np.degrees(mean_angle_rad) * u.deg\n    \n    return mean_angle\n\n# Example usage\ncme_angles = [45, 270, 315, 360] # Example CME angles in degrees\nmean_direction = compute_mean_cme_angle(cme_angles)\nprint(f\"Mean CME propagation angle: {mean_direction:.1f}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "For my asteroseismology research, I need to create a superposition of three cosine waves with frequencies of 0.1, 0.2, and 0.3 Hz, and amplitudes of 1.0, 0.5, and 0.25 respectively. Show me how to model and visualize this using multiple Cosine1D instances."
         },
         {
            "from": "assistant",
            "value": "from astropy.modeling.models import Cosine1D\nfrom astropy.modeling import Model\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Create time array (10 seconds with 1000 points)\nt = np.linspace(0, 10, 1000)\n\n# Create three Cosine1D models with different frequencies and amplitudes\n# Frequency = 2\u03c0 * Hz, so we multiply by 2\u03c0\nwave1 = Cosine1D(amplitude=1.0, frequency=2*np.pi*0.1, phase=0)\nwave2 = Cosine1D(amplitude=0.5, frequency=2*np.pi*0.2, phase=0)\nwave3 = Cosine1D(amplitude=0.25, frequency=2*np.pi*0.3, phase=0)\n\n# Compute superposition\nsuperposition = wave1(t) + wave2(t) + wave3(t)\n\n# Plotting\nplt.figure(figsize=(10, 6))\nplt.plot(t, superposition, 'b-', label='Superposition')\nplt.plot(t, wave1(t), 'r--', alpha=0.5, label='f=0.1 Hz')\nplt.plot(t, wave2(t), 'g--', alpha=0.5, label='f=0.2 Hz')\nplt.plot(t, wave3(t), 'y--', alpha=0.5, label='f=0.3 Hz')\n\nplt.xlabel('Time (seconds)')\nplt.ylabel('Amplitude')\nplt.title('Superposition of Three Cosine Waves')\nplt.legend()\nplt.grid(True)\nplt.show()"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "During my exoplanet transit analysis, I fitted a Gaussian to the seeing variations and got sigma = 1.75 arcsec. How can I convert this to FWHM to standardize my reporting with other observatories?"
         },
         {
            "from": "assistant",
            "value": "from astropy import units as u\nimport numpy as np\n\n# Input seeing sigma\nseeing_sigma = 1.75 * u.arcsec\n\n# Calculate FWHM using the conversion factor 2*sqrt(2*ln(2))\nfwhm = 2 * np.sqrt(2 * np.log(2)) * seeing_sigma\n\n# Print result with units\nprint(f\"Seeing FWHM = {fwhm:.2f}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm working on high-resolution spectroscopy data and need to convert between wavelength units from nanometers to Angstroms. How can I implement this conversion?"
         },
         {
            "from": "assistant",
            "value": "from astropy import units as u\n\n# Define wavelength in nanometers\nwavelength_nm = 656.3 * u.nm  # Example: H-alpha wavelength\n\n# Convert to Angstroms\nwavelength_angstrom = wavelength_nm.to(u.AA)\n\n# Print both values for comparison\nprint(f\"Wavelength in nanometers: {wavelength_nm}\")\nprint(f\"Wavelength in Angstroms: {wavelength_angstrom}\")\n\n# For an array of wavelengths\nwavelength_array_nm = [656.3, 486.1, 434.0] * u.nm  # H-alpha, H-beta, H-gamma\nwavelength_array_angstrom = wavelength_array_nm.to(u.AA)\n\n# Print array conversions\nprint(f\"\\nArray of wavelengths:\")\nprint(f\"In nanometers: {wavelength_array_nm}\")\nprint(f\"In Angstroms: {wavelength_array_angstrom}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "Analyzing galaxy brightness profile with surface brightness values [20.5, 19.8, 19.2, 18.5, 18.0] at radii [0, 2, 4, 6, 8] arcseconds. How can I use SimplexLSQFitter to fit a Sersic profile model with initial parameters amplitude=18, r_eff=5, and n=4?"
         },
         {
            "from": "assistant",
            "value": "from astropy.modeling import models, fitting\nimport numpy as np\n\n# Create data points\nradius = np.array([0, 2, 4, 6, 8])  # radii in arcseconds\nbrightness = np.array([20.5, 19.8, 19.2, 18.5, 18.0])  # surface brightness\n\n# Initialize Sersic1D model with initial parameters\nsersic_init = models.Sersic1D(\n    amplitude=18,  # central surface brightness\n    r_eff=5,      # effective radius in arcseconds\n    n=4           # Sersic index (n=4 for de Vaucouleurs profile)\n)\n\n# Create Simplex LSQ fitter\nfitter = fitting.SimplexLSQFitter()\n\n# Fit the model to the data\ntry:\n    fitted_model = fitter(sersic_init, radius, brightness)\n    \n    # Print fitted parameters\n    print(f\"Fitted amplitude: {fitted_model.amplitude.value:.2f}\")\n    print(f\"Fitted r_eff: {fitted_model.r_eff.value:.2f}\")\n    print(f\"Fitted n: {fitted_model.n.value:.2f}\")\n    \nexcept Exception as e:\n    print(f\"Fitting failed: {str(e)}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm analyzing the motion of stars in a globular cluster and need to represent both their positions and velocities in Cartesian coordinates. How can I create a representation that includes both position and velocity data?"
         },
         {
            "from": "assistant",
            "value": "from astropy.coordinates import CartesianRepresentation, CartesianDifferential\nimport astropy.units as u\nimport numpy as np\n\n# Sample data for 3 stars (positions in parsec, velocities in km/s)\nx = [10., -5., 2.] * u.pc\ny = [-2., 8., 4.] * u.pc\nz = [5., 3., -1.] * u.pc\nv_x = [25., -12., 8.] * u.km/u.s\nv_y = [-15., 20., 5.] * u.km/u.s\nv_z = [10., 5., -3.] * u.km/u.s\n\n# Create position representation\npositions = CartesianRepresentation(x=x, y=y, z=z)\n\n# Create velocity differential\nvelocities = CartesianDifferential(d_x=v_x, d_y=v_y, d_z=v_z)\n\n# Combine positions and velocities\nstars = positions.with_differentials(velocities)\n\n# Example: accessing components\nprint(\"Positions (x):\", stars.x)\nprint(\"Velocities (v_x):\", stars.differentials['s'].d_x)"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I need to deconvolve atmospheric seeing effects from my planetary observation. Show me how to use convolve_fft with dealias=True on my (256, 256) image array using a measured PSF kernel of size (15, 15), while ensuring proper normalization."
         },
         {
            "from": "assistant",
            "value": "from astropy.convolution import convolve_fft\nimport numpy as np\n\n# Create example data arrays (replace with your actual data)\nimage = np.random.random((256, 256))  # Your planetary observation\npsf_kernel = np.random.random((15, 15))  # Your measured PSF kernel\n\n# Normalize the PSF kernel to sum to 1\npsf_kernel = psf_kernel / np.sum(psf_kernel)\n\n# Option 1: Keep dealias=True but use different boundary condition\ndeconvolved_image = convolve_fft(\n    image,\n    psf_kernel,\n    boundary='fill',  # Changed from 'wrap' to 'fill'\n    normalize_kernel=True,  # Ensure kernel is normalized\n    dealias=True,  # Prevent aliasing artifacts\n    preserve_nan=True  # Maintain any NaN values in input\n)\n\n# Alternative Option 2: Keep wrap boundary but disable dealiasing\n# deconvolved_image = convolve_fft(\n#     image,\n#     psf_kernel,\n#     boundary='wrap',  # Periodic boundary conditions\n#     normalize_kernel=True,\n#     dealias=False,  # Disabled dealiasing\n#     preserve_nan=True\n# )\n\n# Note: The deconvolved image may need additional post-processing\n# such as non-negative constraints or noise filtering"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm analyzing the motion of a star cluster and need to create a CartesianRepresentation of three points with coordinates [(1,2,3), (4,5,6), (7,8,9)] in astronomical units (au). Show me how to construct this representation."
         },
         {
            "from": "assistant",
            "value": "# Import necessary modules from astropy\nfrom astropy.coordinates import CartesianRepresentation\nimport astropy.units as u\n\n# Create CartesianRepresentation for three points\n# Each point is specified as (x,y,z) with units of astronomical units\npoints = CartesianRepresentation(\n    x=[1, 4, 7] * u.au,  # x coordinates\n    y=[2, 5, 8] * u.au,  # y coordinates\n    z=[3, 6, 9] * u.au   # z coordinates\n)\n\n# Print the representation\nprint(points)"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm comparing planetary transit observations from three different telescopes. Each dataset is in a Table with columns 'time' and 'relative_flux' (50 measurements each). How can I stack these observations depth-wise while handling any missing data points?"
         },
         {
            "from": "assistant",
            "value": "from astropy.table import Table, vstack\nimport numpy as np\n\n# Create sample data for demonstration\n# In practice, these would be loaded from files\nt1 = Table({\n    'time': np.linspace(0, 5, 50),\n    'relative_flux': np.random.normal(1.0, 0.01, 50)\n})\nt2 = Table({\n    'time': np.linspace(0, 5, 50),\n    'relative_flux': np.random.normal(1.0, 0.01, 50)\n})\nt3 = Table({\n    'time': np.linspace(0, 5, 50),\n    'relative_flux': np.random.normal(1.0, 0.01, 50)\n})\n\n# Add source identifier column to each table\nt1['telescope'] = 'telescope1'\nt2['telescope'] = 'telescope2'\nt3['telescope'] = 'telescope3'\n\n# Stack tables vertically\n# masked=True ensures proper handling of missing data\ncombined_data = vstack([t1, t2, t3], join_type='outer')\n\n# Sort by time for easier analysis\ncombined_data.sort('time')\n\n# Calculate mean flux at each timepoint (handling masked values)\nunique_times = np.unique(combined_data['time'])\nmean_flux = []\nfor t in unique_times:\n    flux_at_t = combined_data[combined_data['time'] == t]['relative_flux']\n    mean_flux.append(np.ma.mean(flux_at_t))\n\n# Create final table with averaged values\nresult = Table({\n    'time': unique_times,\n    'mean_flux': mean_flux,\n    'n_measurements': [len(combined_data[combined_data['time'] == t]) for t in unique_times]\n})"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm working with a deep-field image from my telescope that has a large dynamic range of pixel values. How can I create a visualization that better shows both bright and faint objects using a square root scaling?"
         },
         {
            "from": "assistant",
            "value": "from astropy.io import fits\nfrom astropy.visualization import (ImageNormalize, SqrtStretch)\nimport matplotlib.pyplot as plt\n\n# Read the FITS image\n# Replace 'image.fits' with your actual filename\ntry:\n    hdul = fits.open('image.fits')\n    image_data = hdul[0].data\n    \n    # Create normalization object with square root stretch\n    norm = ImageNormalize(image_data, stretch=SqrtStretch())\n    \n    # Create the visualization\n    plt.figure(figsize=(10, 8))\n    plt.imshow(image_data, norm=norm, cmap='viridis')\n    plt.colorbar(label='Sqrt(Flux)')\n    plt.title('Deep Field Image with Square Root Scaling')\n    \n    # Display the image\n    plt.show()\n    \n    hdul.close()\n\nexcept FileNotFoundError:\n    print(\"Error: Image file not found\")\nexcept Exception as e:\n    print(f\"An error occurred: {str(e)}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I need to calculate the distance to a galaxy with a redshift of 0.45 using the current default cosmology. Could you show me how to do this with astropy.coordinates.Distance?"
         },
         {
            "from": "assistant",
            "value": "from astropy.coordinates import Distance\nfrom astropy.cosmology import default_cosmology\nimport astropy.units as u\n\n# Set redshift value\nz = 0.45\n\n# Get the default cosmology\ncosmo = default_cosmology.get()\n\n# Calculate distances using cosmology\nluminosity_distance = cosmo.luminosity_distance(z)\ncomoving_distance = cosmo.comoving_distance(z)\n\n# Print results with different distance measures\nprint(f\"Luminosity distance: {luminosity_distance.to(u.Mpc):.2f}\")\nprint(f\"Comoving distance: {comoving_distance.to(u.Mpc):.2f}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm analyzing some solar observations in helioprojective coordinates, and I need to generate the appropriate WCS. Can you show me how to create a WCS object from a Helioprojective frame with a zenithal perspective projection?"
         },
         {
            "from": "assistant",
            "value": "from astropy.wcs import WCS\nfrom astropy.coordinates import SkyCoord\nimport astropy.units as u\n\n# Create a basic WCS for helioprojective coordinates (TAN projection)\nw = WCS(naxis=2)\n\n# Set WCS parameters for Helioprojective-Cartesian (HPC) projection\nw.wcs.crpix = [512, 512]  # Center pixel (example values)\nw.wcs.cdelt = [-1.0, 1.0]  # Pixel scale in arcsec/pixel\nw.wcs.crval = [0.0, 0.0]  # Solar disk center coordinates\nw.wcs.ctype = ['HPLN-TAN', 'HPLT-TAN']  # Helioprojective longitude/latitude\nw.wcs.cunit = ['arcsec', 'arcsec']  # Angular units\n\n# Add solar-specific WCS information as FITS header keywords\nheader = w.to_header()\nheader['DATE-OBS'] = '2023-01-01T00:00:00'\nheader['DSUN_OBS'] = 1.5e11  # Distance to Sun in meters (approximate)\nheader['RSUN_REF'] = 696000000.0  # Solar radius in meters\n\n# Create new WCS from the updated header\nw = WCS(header)"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm analyzing spectra of quasars and need to compute robust statistics while excluding absorption features. Show me how to calculate sigma-clipped statistics on my spectral flux array of length 500, using 2.5-sigma lower bound and 3.5-sigma upper bound to account for asymmetric features."
         },
         {
            "from": "assistant",
            "value": "from astropy.stats import sigma_clip\nimport numpy as np\n\n# Create example spectral flux array (replace with your actual data)\nflux = np.random.normal(100, 10, 500)  # Example flux array of length 500\n\n# Apply asymmetric sigma clipping\n# Lower bound: 2.5 sigma, Upper bound: 3.5 sigma\nclipped_flux = sigma_clip(\n    flux,\n    sigma_lower=2.5,\n    sigma_upper=3.5,\n    maxiters=5,  # Maximum number of clipping iterations\n    stdfunc='mad_std'  # Using median absolute deviation for robust statistics\n)\n\n# Calculate statistics on clipped data\nmean_flux = np.ma.mean(clipped_flux)  # Mean of unmasked data\nmedian_flux = np.ma.median(clipped_flux)  # Median of unmasked data\nstd_flux = np.ma.std(clipped_flux)  # Standard deviation of unmasked data\n\n# Count number of clipped points\nn_clipped = clipped_flux.mask.sum()\n\nprint(f\"Mean flux: {mean_flux:.2f}\")\nprint(f\"Median flux: {median_flux:.2f}\")\nprint(f\"Std deviation: {std_flux:.2f}\")\nprint(f\"Number of clipped points: {n_clipped}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "How can I apply a linear transformation to my deep-sky image to better reveal the structure of globular clusters while maintaining scientific accuracy of the intensity values?"
         },
         {
            "from": "assistant",
            "value": "from astropy.io import fits\nfrom astropy.visualization import LinearStretch, ImageNormalize\nimport numpy as np\n\n# Read the FITS image\ndef transform_cluster_image(fits_file, stretch_factor=0.5):\n    \"\"\"\n    Apply linear transformation to deep-sky image optimized for globular clusters\n    \n    Parameters:\n    fits_file: str - Path to FITS image file\n    stretch_factor: float - Controls intensity of the stretch (0-1)\n    \"\"\"\n    \n    try:\n        # Open FITS file and get image data\n        with fits.open(fits_file) as hdul:\n            image_data = hdul[0].data\n            header = hdul[0].header\n        \n        # Create normalization using linear stretch\n        # LinearStretch preserves relative intensity relationships\n        norm = ImageNormalize(\n            image_data,\n            stretch=LinearStretch(stretch_factor),\n            clip=False  # Preserve data range\n        )\n        \n        # Apply transformation\n        transformed_image = norm(image_data)\n        \n        # Create new FITS file with transformed data\n        hdu = fits.PrimaryHDU(transformed_image, header=header)\n        \n        # Add processing history to header\n        hdu.header['HISTORY'] = f'Applied LinearStretch with factor {stretch_factor}'\n        \n        # Save transformed image\n        output_file = 'transformed_' + fits_file\n        hdu.writeto(output_file, overwrite=True)\n        \n        return transformed_image\n        \n    except Exception as e:\n        print(f\"Error processing image: {str(e)}\")\n        return None\n\n# Example usage\n# transformed_data = transform_cluster_image('cluster.fits', stretch_factor=0.5)"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "How can I analyze the observation coverage of a variable star's phase given multiple observation sessions with different durations and signal qualities?"
         },
         {
            "from": "assistant",
            "value": "from astropy.time import Time\nimport astropy.units as u\nimport numpy as np\n\n# Function to fold time series with a given period\ndef fold_time(times, period, t0=None):\n    \"\"\"\n    Fold time series using given period.\n    \n    Parameters:\n    times : array-like\n        Time values to fold\n    period : float\n        Period to fold the time series\n    t0 : float, optional\n        Reference time (default: minimum time)\n        \n    Returns:\n    phases : array\n        Phases between 0 and 1\n    \"\"\"\n    if t0 is None:\n        t0 = np.min(times)\n    \n    phases = ((times - t0) / period) % 1.0\n    return phases\n\n# Sample observation data (replace with your actual data)\n# Each tuple contains (start_time, duration_hours, quality_score)\nobservations = [\n    (\"2023-01-01T23:00:00\", 4.5, 0.8),\n    (\"2023-01-03T22:30:00\", 3.0, 0.9),\n    (\"2023-01-05T00:15:00\", 2.5, 0.7),\n]\n\n# Variable star period (example: RR Lyrae type star)\nperiod = 0.54 * u.day  # Period in days\n\n# Convert observations to time series\ntimes = []\nweights = []  # Using quality scores as weights\n\nfor start, duration, quality in observations:\n    # Create time points for each observation session\n    t_start = Time(start)\n    duration_days = duration / 24.0  # Convert hours to days\n    \n    # Create 10 points per observation session\n    t_points = np.linspace(0, duration_days, 10)\n    session_times = t_start + t_points * u.day\n    \n    times.extend(session_times.jd)\n    weights.extend([quality] * len(t_points))\n\n# Convert to numpy arrays\ntimes = np.array(times)\nweights = np.array(weights)\n\n# Calculate phases using our fold_time function\nphases = fold_time(times, period.value, times[0])\n\n# Analyze phase coverage\nbins = np.linspace(0, 1, 11)  # 10 phase bins\ncoverage, _ = np.histogram(phases, bins=bins, weights=weights)\nphase_centers = (bins[:-1] + bins[1:]) / 2\n\n# Print phase coverage analysis\nprint(\"Phase Coverage Analysis:\")\nfor phase, cov in zip(phase_centers, coverage):\n    print(f\"Phase {phase:.2f}: Weight sum = {cov:.2f}\")\n\n# Calculate coverage gaps\nphase_gaps = np.where(coverage == 0)[0]\nif len(phase_gaps) > 0:\n    print(\"\\nGaps in phase coverage at phases:\", \n          [f\"{phase_centers[i]:.2f}\" for i in phase_gaps])\nelse:\n    print(\"\\nNo significant gaps in phase coverage\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "How can I create a MaskedColumn for a spectral dataset with 10 empty rows, where each row will eventually contain a 3x4 array of flux values? The column should be named 'flux' and use float32 as dtype."
         },
         {
            "from": "assistant",
            "value": "from astropy.table import MaskedColumn\nimport numpy as np\n\n# Create a MaskedColumn for spectral flux data\n# Shape (10,3,4) represents 10 rows, each containing a 3x4 array\nflux_column = MaskedColumn(\n    name='flux',\n    dtype='float32',\n    shape=(10,),  # 10 rows\n    length=10,\n    data=np.zeros((10,3,4)),  # Initialize with zeros\n    mask=np.ones((10,3,4)),   # Initially mask all values as invalid\n)\n\n# The resulting column has 10 rows of 3x4 arrays\n# Each element can be accessed as flux_column[row_index][i,j]"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "\"I'm analyzing spectroscopic data from multiple stars and need to create a table containing their temperatures, magnitudes, and metallicities with appropriate units. How can I structure this data using astropy.table?\""
         },
         {
            "from": "assistant",
            "value": "from astropy.table import Table, Column\nfrom astropy import units as u\n\n# Create sample data with units\ntemperatures = [5778, 3500, 12000] * u.K  # Temperatures in Kelvin\nmagnitudes = [-26.74, 15.2, 8.3]          # Absolute magnitudes (dimensionless)\nmetallicities = [0.0, -0.5, 0.3]          # [Fe/H] values (dimensionless)\nstar_names = ['Sun', 'Proxima Cen', 'Vega']\n\n# Create the table with columns and appropriate units\nstars_table = Table([\n    Column(star_names, name='name', dtype=str),\n    Column(temperatures, name='temperature', unit=u.K),\n    Column(magnitudes, name='abs_magnitude'),\n    Column(metallicities, name='metallicity', description='[Fe/H]')\n])\n\n# Add metadata to the table\nstars_table.meta['description'] = 'Stellar parameters catalog'\nstars_table.meta['author'] = 'Astronomer'\n\n# Display the table\nprint(stars_table)\n\n# Example of accessing data with units\nprint(f\"\\nTemperature of {stars_table['name'][0]}: {stars_table['temperature'][0]}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "For my pulsar timing analysis, I need to convert a light travel time of 1500 light seconds into astronomical units (AU). Can you show me how to perform this conversion using astropy units?"
         },
         {
            "from": "assistant",
            "value": "from astropy import units as u\nfrom astropy.constants import c\n\n# Define the light travel time (1500 light seconds)\nlight_time = 1500 * u.s\n\n# Calculate distance by multiplying light time by speed of light\ndistance = light_time * c\n\n# Convert the distance to astronomical units\ndistance_au = distance.to(u.au)\n\nprint(f\"Light travel time of {light_time} corresponds to {distance_au:.4f}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "As part of my exoplanet transit observations, I need to estimate the signal-to-noise ratio for a dim host star considering both sky background and instrument noise. How can I compute this using Astropy?"
         },
         {
            "from": "assistant",
            "value": "import numpy as np\nfrom astropy import units as u\nfrom astropy.stats import sigma_clipped_stats\n\ndef calculate_transit_snr(star_flux, exposure_time, sky_background, read_noise, dark_current):\n    \"\"\"\n    Calculate SNR for transit observation considering noise sources\n    \n    Parameters:\n    - star_flux: stellar flux in electrons/second\n    - exposure_time: exposure time in seconds\n    - sky_background: sky background in electrons/pixel/second\n    - read_noise: CCD read noise in electrons/pixel\n    - dark_current: dark current in electrons/pixel/second\n    \"\"\"\n    \n    # Total signal from star\n    signal = star_flux * exposure_time\n    \n    # Noise sources (variance)\n    photon_noise = signal  # Poisson noise from star\n    sky_noise = sky_background * exposure_time  # Sky background noise\n    dark_noise = dark_current * exposure_time  # Dark current noise\n    readout_noise = read_noise**2  # Read noise\n    \n    # Total noise (square root of sum of variances)\n    total_noise = np.sqrt(photon_noise + sky_noise + dark_noise + readout_noise)\n    \n    # Calculate SNR\n    snr = signal / total_noise\n    \n    return snr\n\n# Example usage\nstar_flux = 1000 * u.electron / u.second  # Dim star\nexp_time = 300 * u.second  # 5 minute exposure\nsky_bg = 10 * u.electron / u.second  # Sky background\nread_noise = 5 * u.electron  # Read noise\ndark_current = 0.1 * u.electron / u.second  # Dark current\n\nsnr = calculate_transit_snr(\n    star_flux.value,\n    exp_time.value,\n    sky_bg.value,\n    read_noise.value,\n    dark_current.value\n)\n\nprint(f\"Estimated SNR: {snr:.1f}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I have a 2D array of shape (512, 512) containing star counts from my survey, and I need to create an ImageHDU with a header specifying 'GAIN'=1.4 and 'READNOIS'=3.2. How should I write the code?"
         },
         {
            "from": "assistant",
            "value": "import numpy as np\nfrom astropy.io import fits\n\n# Create sample 2D array (replace this with your actual data)\ndata = np.zeros((512, 512))  # Example array, use your real data instead\n\n# Create ImageHDU with data and specified header keywords\nhdu = fits.ImageHDU(data=data)\n\n# Add required keywords to header\nhdu.header['GAIN'] = (1.4, 'Detector gain in electrons/ADU')\nhdu.header['READNOIS'] = (3.2, 'Read noise in electrons RMS')\n\n# If you need to write to file:\n# hdu.writeto('output.fits', overwrite=True)"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm studying galactic dynamics and need to convert the positions of several globular clusters from galactic coordinates to the FK5 system. How should I implement this?"
         },
         {
            "from": "assistant",
            "value": "# Import required astropy modules\nfrom astropy.coordinates import SkyCoord\nimport astropy.units as u\n\n# Define example globular cluster positions in galactic coordinates\n# Format: l (longitude), b (latitude)\ncluster_positions = [\n    (0, 0),          # Galactic center reference\n    (332.9, -9.3),   # Example cluster 1 (e.g., NGC 6752)\n    (50.5, -5.8)     # Example cluster 2 (e.g., M5)\n]\n\n# Create SkyCoord object with galactic coordinates\ngalactic_coords = SkyCoord(\n    l=[pos[0] for pos in cluster_positions]*u.degree,\n    b=[pos[1] for pos in cluster_positions]*u.degree,\n    frame='galactic'\n)\n\n# Transform to FK5 (J2000) equatorial coordinates\nfk5_coords = galactic_coords.transform_to('fk5')\n\n# Print results\nfor i, coord in enumerate(fk5_coords):\n    print(f\"Cluster {i}:\")\n    print(f\"RA: {coord.ra.deg:.2f}\u00b0, Dec: {coord.dec.deg:.2f}\u00b0\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "Working on a black body radiation problem, I need to convert a peak energy of 2.5 eV to its corresponding wavelength in micrometers. How can I use Astropy's spectral equivalencies for this energy-to-wavelength conversion?"
         },
         {
            "from": "assistant",
            "value": "from astropy import units as u\nfrom astropy.constants import h, c\n\n# Define the energy in electron volts\npeak_energy = 2.5 * u.eV\n\n# Use spectral() equivalency to convert from energy to wavelength\n# spectral() enables conversions between wavelength, frequency, and energy\nwavelength = peak_energy.to(u.micron, equivalencies=u.spectral())\n\nprint(f\"Energy {peak_energy} corresponds to wavelength: {wavelength:.2f}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "For my supernova research, I need to calculate the total energy release in foe units from my measurements in ergs. What's the code to perform this conversion?"
         },
         {
            "from": "assistant",
            "value": "from astropy import units as u\n\ndef energy_to_foe(energy_ergs):\n    \"\"\"\n    Convert energy from ergs to foe units (10^51 ergs)\n    Commonly used in supernova energetics calculations\n    \"\"\"\n    # Convert input to astropy quantity in ergs\n    energy = energy_ergs * u.erg\n    \n    # Convert to foe (1 foe = 10^51 ergs)\n    foe = energy / (1e51 * u.erg)\n    \n    return foe.value  # Return numerical value without units\n\n# Example usage\nsupernova_energy = 5.2e52  # ergs\nenergy_foe = energy_to_foe(supernova_energy)\nprint(f\"Supernova energy: {energy_foe:.2f} foe\")  # Will print ~5.20 foe"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm analyzing spectra from several emission nebulae and need to convert their Gaussian sigma measurements to FWHM values. Could you show me how to do this conversion using astropy?"
         },
         {
            "from": "assistant",
            "value": "import astropy.constants as const\nimport numpy as np\n\ndef sigma_to_fwhm(sigma_values):\n    \"\"\"\n    Convert Gaussian sigma values to Full Width at Half Maximum (FWHM)\n    FWHM = 2\u221a(2ln(2)) * \u03c3 \u2248 2.355 * \u03c3\n    \n    Parameters:\n    -----------\n    sigma_values : float or array-like\n        Sigma values to convert\n        \n    Returns:\n    --------\n    fwhm : same type as input\n        FWHM values\n    \"\"\"\n    \n    # Convert input to numpy array if it isn't already\n    sigma_values = np.array(sigma_values)\n    \n    # Conversion factor from sigma to FWHM\n    conversion_factor = 2 * np.sqrt(2 * np.log(2))\n    \n    # Calculate FWHM\n    fwhm = sigma_values * conversion_factor\n    \n    return fwhm\n\n# Example usage\nsigma_measurements = [2.1, 3.4, 1.7]  # in whatever units your measurements are in\nfwhm_values = sigma_to_fwhm(sigma_measurements)\n\nprint(f\"Sigma values: {sigma_measurements}\")\nprint(f\"FWHM values: {fwhm_values}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "In my exoplanet research, I'm comparing temperature data where I have a hot Jupiter's temperature of 1200 Kelvin. Can you show me how to convert this to both Celsius and Fahrenheit in a single code snippet using astropy's temperature equivalencies?"
         },
         {
            "from": "assistant",
            "value": "# Import required astropy modules for temperature conversion\nfrom astropy.units import K, deg_C\nfrom astropy import units as u\n\n# Set the hot Jupiter's temperature in Kelvin\nhot_jupiter_temp = 1200 * u.K\n\n# Convert to Celsius using temperature equivalencies\ncelsius = hot_jupiter_temp.to(deg_C, equivalencies=u.temperature())\n\n# Convert to Fahrenheit manually\n# \u00b0F = (K - 273.15) * 9/5 + 32\nfahrenheit = ((hot_jupiter_temp.value - 273.15) * (9/5) + 32)\n\n# Print results with proper units\nprint(f\"Hot Jupiter temperature:\")\nprint(f\"Kelvin: {hot_jupiter_temp:.1f}\")\nprint(f\"Celsius: {celsius:.1f}\")\nprint(f\"Fahrenheit: {fahrenheit:.1f}\u00b0F\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm analyzing temperature variations in a protoplanetary disk, with temperatures around 300 Kelvin in the outer regions. Could you show me how to express this temperature in all available units using astropy, and include proper error handling in case of invalid temperature values?"
         },
         {
            "from": "assistant",
            "value": "from astropy import units as u\nfrom astropy.units import imperial\nfrom astropy.units.core import UnitsError\n\ndef convert_temperature(temp_value):\n    try:\n        # Create temperature quantity in Kelvin\n        temp_kelvin = temp_value * u.K\n        \n        # Convert to different temperature scales\n        conversions = {\n            'Kelvin': temp_kelvin,\n            'Celsius': temp_kelvin.to(u.deg_C),\n            'Fahrenheit': temp_kelvin.to(imperial.deg_F),\n            'Rankine': temp_kelvin.to(imperial.deg_R)\n        }\n        \n        # Print all temperature conversions\n        print(\"Temperature conversions for protoplanetary disk:\")\n        for unit_name, temp in conversions.items():\n            print(f\"{unit_name}: {temp:.2f}\")\n            \n    except UnitsError as e:\n        print(f\"Unit conversion error: {e}\")\n    except ValueError as e:\n        print(f\"Invalid temperature value: {e}\")\n    except Exception as e:\n        print(f\"Unexpected error: {e}\")\n\n# Example usage with 300 Kelvin\nconvert_temperature(300)\n\n# Example with invalid negative absolute temperature\nconvert_temperature(-300)"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm analyzing an image of a galaxy core that shows a central bright peak with some rotation in its intensity pattern. Can you show me how to create a tilted 2D Gaussian model to represent this structure?"
         },
         {
            "from": "assistant",
            "value": "import numpy as np\nfrom astropy.modeling import models, fitting\nimport astropy.units as u\n\n# Create a coordinate grid for the model\ny, x = np.mgrid[-50:51, -50:51]\n\n# Create a rotated 2D Gaussian model\n# Parameters: amplitude, x_mean, y_mean, x_stddev, y_stddev, theta\ngaussian_2d = models.Gaussian2D(\n    amplitude=100,    # Peak intensity\n    x_mean=0,        # Center x position\n    y_mean=0,        # Center y position\n    x_stddev=10,     # Width in x\n    y_stddev=15,     # Width in y (elongated)\n    theta=30 * np.pi/180  # Rotation angle (30 degrees converted to radians)\n)\n\n# Generate the model data\nmodel_data = gaussian_2d(x, y)\n\n# If you need to fit this model to actual data:\nfitter = fitting.LevMarLSQFitter()\n# Uncomment and modify the following line when you have actual data:\n# fitted_model = fitter(gaussian_2d, x, y, your_real_data)\n\n# The model_data array now contains the 2D Gaussian representation\n# of your galaxy core with rotation"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I want to visualize the period distribution of 200 exoplanets that range from 1 to 1000 days. Show me how to create a histogram using Scott's rule for binning, with customized colors (use 'skyblue' for bars) and add grid lines for better readability."
         },
         {
            "from": "assistant",
            "value": "import numpy as np\nfrom astropy.visualization import hist\nimport matplotlib.pyplot as plt\n\n# Generate sample period data (log-uniform distribution between 1 and 1000 days)\nnp.random.seed(42)  # For reproducibility\nperiods = np.exp(np.random.uniform(np.log(1), np.log(1000), 200))\n\n# Create figure and axis\nplt.figure(figsize=(10, 6))\n\n# Create histogram using Scott's rule for bin width\n# Use log scale for better visualization of period distribution\nhist(periods, bins='scott', color='skyblue', alpha=0.7, \n     label='Exoplanet Periods', log=True)\n\n# Customize plot\nplt.grid(True, alpha=0.3, linestyle='--')\nplt.xlabel('Orbital Period (days)')\nplt.ylabel('Number of Planets')\nplt.title('Distribution of Exoplanet Orbital Periods')\nplt.legend()\n\n# Add minor grid lines for better readability\nplt.grid(True, which='minor', alpha=0.15, linestyle=':')\n\nplt.show()"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "Working with ALMA data of a protoplanetary disk where the WCS has RADESYS='ICRS'. Can you demonstrate how to use wcs_to_celestial_frame to get the frame, then convert some pixel positions (x=256, y=256) to celestial coordinates using this frame?"
         },
         {
            "from": "assistant",
            "value": "from astropy.wcs import WCS\nfrom astropy.coordinates import SkyCoord\nimport astropy.units as u\n\n# Create a sample WCS (normally this would come from your FITS header)\nw = WCS(naxis=2)\nw.wcs.ctype = [\"RA---TAN\", \"DEC--TAN\"]\nw.wcs.crpix = [256, 256]  # Reference pixel position\nw.wcs.crval = [180.0, -30.0]  # RA=180 deg, Dec=-30 deg\nw.wcs.cdelt = [-0.001, 0.001]  # Degree increment per pixel\nw.wcs.radesys = 'ICRS'  # Coordinate system specification\n\n# Get the celestial frame from the WCS\nframe = w.celestial  # Changed from w.wcs_to_celestial_frame()\n\n# Convert pixel coordinates to celestial coordinates\npixel_coords = [[256, 256]]  # List of pixel coordinates to transform\nsky_coords = w.pixel_to_world(pixel_coords[0][0], pixel_coords[0][1])\n\n# Print results\nprint(f\"Celestial Frame: {frame}\")\nprint(f\"Pixel coordinates (x,y): ({pixel_coords[0][0]}, {pixel_coords[0][1]})\")\nprint(f\"Sky coordinates: RA={sky_coords.ra.deg:.6f}\u00b0, Dec={sky_coords.dec.deg:.6f}\u00b0\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm analyzing the radial velocities of stars in a globular cluster, with 500 measurements between -150 and +150 km/s. Show me how to find the ideal bin width using the Freedman-Diaconis method and create a histogram with these optimal bins using astropy."
         },
         {
            "from": "assistant",
            "value": "import numpy as np\nfrom astropy.stats import freedman_bin_width\nimport matplotlib.pyplot as plt\n\n# Generate sample radial velocity data (for demonstration)\nnp.random.seed(42)\nrv_data = np.random.normal(0, 50, 500)  # 500 measurements\nrv_data = rv_data.clip(-150, 150)  # limit to \u00b1150 km/s range\n\n# Calculate optimal bin width using Freedman-Diaconis rule\noptimal_width = freedman_bin_width(rv_data)\n\n# Calculate number of bins needed\nn_bins = int(np.ceil((rv_data.max() - rv_data.min()) / optimal_width))\n\n# Create histogram with optimal bins\nplt.hist(rv_data, bins=n_bins, edgecolor='black', alpha=0.7)\nplt.xlabel('Radial Velocity (km/s)')\nplt.ylabel('Number of Stars')\nplt.title(f'Radial Velocity Distribution\\nBin width = {optimal_width:.2f} km/s')\nplt.grid(True, alpha=0.3)\nplt.show()"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "For my exoplanet transit timing variations study, I need to calculate the scatter in the observed minus calculated (O-C) transit times, but some measurements might be affected by stellar activity. How can I compute a robust estimate of the timing variation amplitude?"
         },
         {
            "from": "assistant",
            "value": "import numpy as np\nfrom astropy.stats import median_absolute_deviation\nfrom astropy import units as u\nfrom astropy.time import Time\n\ndef calculate_ttv_amplitude(observed_times, calculated_times, time_errors=None):\n    \"\"\"\n    Calculate robust TTV amplitude using median absolute deviation\n    \n    Parameters:\n    -----------\n    observed_times : Time array\n        Observed transit times in BJD\n    calculated_times : Time array\n        Calculated (expected) transit times from linear ephemeris\n    time_errors : array, optional\n        Uncertainties in transit timing measurements\n    \n    Returns:\n    --------\n    ttv_amplitude : Quantity\n        Robust estimate of TTV amplitude in minutes\n    \"\"\"\n    # Calculate O-C values in minutes\n    o_minus_c = (observed_times - calculated_times).to(u.minute)\n    \n    # Use MAD for robust scatter estimate\n    # MAD is less sensitive to outliers than standard deviation\n    mad_scatter = median_absolute_deviation(o_minus_c)\n    \n    # Convert MAD to equivalent RMS for normal distribution\n    # Factor of 1.4826 converts MAD to standard deviation equivalent\n    ttv_amplitude = 1.4826 * mad_scatter * u.minute\n    \n    return ttv_amplitude\n\n# Example usage:\n# Create sample data\nn_transits = 20\nt0 = Time('2020-01-01T00:00:00', format='isot', scale='tdb')\nperiod = 3.4 * u.day\n\n# Generate calculated times\ncalc_times = Time([t0.tdb.jd + (i * period.to(u.day).value) for i in range(n_transits)], \n                 format='jd', scale='tdb')\n\n# Simulate observed times with some scatter and outliers\nnp.random.seed(42)\nscatter = 5.0  # minutes\nobs_times = Time(calc_times.tdb.jd + \n                np.random.normal(0, scatter/1440, n_transits) +  # Normal scatter\n                np.random.choice([0, 30/1440], n_transits, p=[0.9, 0.1]),  # Outliers\n                format='jd', scale='tdb')\n\n# Calculate TTV amplitude\nttv_amp = calculate_ttv_amplitude(obs_times, calc_times)\nprint(f\"Robust TTV amplitude: {ttv_amp:.1f}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm analyzing radial velocity measurements of a binary star system. Show me how to create a Sine1D model with amplitude=50 km/s, frequency=1/80 days^-1, phase=0.1, and generate synthetic data points for 100 days with random Gaussian noise (sigma=2 km/s) added."
         },
         {
            "from": "assistant",
            "value": "# Import required modules from astropy\nfrom astropy.modeling import models\nimport numpy as np\nfrom astropy import units as u\n\n# Create time array for 100 days\ntime = np.linspace(0, 100, 100)\n\n# Define the sine model with given parameters\n# Amplitude = 50 km/s, Period = 80 days (frequency = 1/80), phase = 0.1\nsine_model = models.Sine1D(\n    amplitude=50,\n    frequency=1/80,\n    phase=0.1\n)\n\n# Generate perfect sine wave data\nperfect_data = sine_model(time)\n\n# Add Gaussian noise with sigma = 2 km/s\nnp.random.seed(42)  # For reproducibility\nnoise = np.random.normal(0, 2, len(time))\nnoisy_data = perfect_data + noise\n\n# Convert to Quantity objects with units\ntime = time * u.day\nrv_data = noisy_data * u.km/u.s\n\n# Print first few data points\nfor t, rv in zip(time[:5], rv_data[:5]):\n    print(f\"Time: {t:.1f}, RV: {rv:.2f}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "How do I create a synthetic 2D Moffat profile to simulate telescope seeing effects on my stellar population study? I want to generate artificial star fields with realistic PSF."
         },
         {
            "from": "assistant",
            "value": "import numpy as np\nfrom astropy.modeling.models import Moffat2D\nfrom astropy.modeling.functional_models import Gaussian2D\nimport matplotlib.pyplot as plt\n\n# Create a 2D grid\nsize = 100\ny, x = np.mgrid[0:size, 0:size]\n\n# Define Moffat profile parameters\namplitude = 1.0    # Peak intensity\nx_0 = size/2      # Center x\ny_0 = size/2      # Center y\ngamma = 3.0       # Core width\nalpha = 2.5       # Power index (typical seeing values: 2-3)\n\n# Create Moffat model\nmoffat = Moffat2D(amplitude=amplitude, x_0=x_0, y_0=y_0, \n                  gamma=gamma, alpha=alpha)\n\n# Generate the 2D PSF\npsf = moffat(x, y)\n\n# Create synthetic star field (random positions)\nn_stars = 10\nstars = np.zeros((size, size))\nfor _ in range(n_stars):\n    x_pos = np.random.randint(10, size-10)\n    y_pos = np.random.randint(10, size-10)\n    brightness = np.random.uniform(0.3, 1.0)\n    \n    # Shift Moffat profile to star position\n    star_moffat = Moffat2D(amplitude=brightness, x_0=x_pos, y_0=y_pos, \n                          gamma=gamma, alpha=alpha)\n    stars += star_moffat(x, y)\n\n# Add some noise\nnoise_level = 0.01\nstars += np.random.normal(0, noise_level, stars.shape)\n\n# Plot result\nplt.imshow(stars, cmap='viridis', origin='lower')\nplt.colorbar(label='Intensity')\nplt.title('Synthetic Star Field with Moffat PSF')\nplt.show()"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm studying the mass distribution in the outer regions of a globular cluster and need to model it as a power law with fixed parameters beyond a certain radius. How can I implement this using Astropy's modeling tools?"
         },
         {
            "from": "assistant",
            "value": "from astropy.modeling import Model, Parameter\nfrom astropy.modeling.models import custom_model\nimport numpy as np\n\n# Create a custom power law model for globular cluster mass distribution\n@custom_model\ndef GlobularClusterMass(r, amplitude=1.0, r_break=10.0, alpha=-2.5):\n    \"\"\"\n    Evaluate mass distribution model:\n    - r_break: radius where power law behavior begins\n    - Inside r_break: constant density core\n    - Outside r_break: power law decay\n    \"\"\"\n    # Initialize mass distribution array\n    mass = np.zeros_like(r)\n    \n    # Inner region (r < r_break): constant density core\n    inner = r < r_break\n    mass[inner] = amplitude\n    \n    # Outer region (r >= r_break): power law decay\n    outer = r >= r_break\n    mass[outer] = amplitude * (r[outer]/r_break)**alpha\n    \n    return mass\n\n# Create model instance\ngc_model = GlobularClusterMass()\n\n# Example usage\nr_values = np.linspace(0, 50, 100)  # Radius array\nmass_distribution = gc_model(r_values)"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm analyzing a flat-field image for my CCD calibration. How do I create a 2D constant model to represent the uniform illumination pattern?"
         },
         {
            "from": "assistant",
            "value": "from astropy.modeling import models, fitting\nimport numpy as np\n\n# Create a 2D constant model\ndef create_flatfield_model(data_shape, amplitude=1.0):\n    # Initialize the 2D constant model\n    const_model = models.Const2D(amplitude=amplitude)\n    \n    # Create coordinate grid for the model\n    y, x = np.mgrid[:data_shape[0], :data_shape[1]]\n    \n    # Evaluate the model on the grid\n    flat_model = const_model(x, y)\n    \n    return flat_model, const_model\n\n# Example usage\nimage_shape = (1024, 1024)  # Typical CCD dimensions\nmodel_data, model = create_flatfield_model(image_shape)\n\n# If you need to fit the model to real data:\n# fitter = fitting.LevMarLSQFitter()\n# fitted_model = fitter(model, x, y, real_data)"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm analyzing data from a global telescope network spanning multiple time zones. How do I standardize all the observation timestamps to a single reference timezone for consistency?"
         },
         {
            "from": "assistant",
            "value": "from astropy.time import Time\nfrom astropy.coordinates import EarthLocation\nimport numpy as np\n\n# Sample observation data from different locations/timezones\n# Format: [timestamp, observatory_longitude]\nobservations = [\n    (\"2023-11-15 10:30:00\", -70.0),  # Chile telescope\n    (\"2023-11-15 22:30:00\", 155.5),   # Hawaii telescope\n    (\"2023-11-15 18:45:00\", 20.7)     # South Africa telescope\n]\n\ndef standardize_timestamps(obs_data):\n    # Convert all timestamps to UTC then to a desired standard timezone\n    standardized_times = []\n    \n    for timestamp, longitude in obs_data:\n        # Create observatory location (using only longitude for this example)\n        observatory = EarthLocation(lon=longitude, lat=0, height=0)\n        \n        # Create Time object with the local time and observatory location\n        t = Time(timestamp, format='iso', location=observatory)\n        \n        # Convert to UTC\n        t_utc = t.utc\n        \n        standardized_times.append(t_utc.iso)\n    \n    return standardized_times\n\n# Standardize all timestamps\nstandard_times = standardize_timestamps(observations)\n\n# Print standardized times\nfor orig, std in zip(observations, standard_times):\n    print(f\"Original: {orig[0]} (Long: {orig[1]}\u00b0) -> Standardized: {std} UTC\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I have two tables in my project: table_a with columns ['star_id', 'mass', 'radius'] and table_b with columns ['star_id', 'temperature', 'luminosity']. How can I properly handle merging these tables while catching potential TableMergeError when some star_ids don't match?"
         },
         {
            "from": "assistant",
            "value": "from astropy.table import Table, TableMergeError\n\ndef merge_star_tables(table_a, table_b):\n    \"\"\"\n    Merge two astronomical tables based on star_id while handling potential merge errors.\n    Returns both the merged table and list of unmatched star IDs.\n    \"\"\"\n    try:\n        # Attempt to merge tables using star_id as key\n        merged_table = Table.join(table_a, table_b, keys='star_id', join_type='inner')\n        \n        # Find unmatched star IDs\n        ids_a = set(table_a['star_id'])\n        ids_b = set(table_b['star_id'])\n        unmatched_ids = ids_a.symmetric_difference(ids_b)\n        \n        return merged_table, list(unmatched_ids)\n    \n    except TableMergeError as e:\n        print(f\"Error merging tables: {e}\")\n        return None, None\n\n# Example usage\ntry:\n    # Create sample tables (in real code, these would be your actual tables)\n    table_a = Table({'star_id': [1, 2, 3], \n                    'mass': [1.0, 2.0, 1.5],\n                    'radius': [1.0, 1.2, 0.8]})\n    \n    table_b = Table({'star_id': [1, 2, 4],\n                    'temperature': [5000, 6000, 5500],\n                    'luminosity': [1.0, 2.0, 1.5]})\n    \n    # Merge tables\n    merged, unmatched = merge_star_tables(table_a, table_b)\n    \n    if merged is not None:\n        print(\"Successfully merged tables\")\n        print(f\"Unmatched star IDs: {unmatched}\")\n        print(merged)\n    \nexcept Exception as e:\n    print(f\"Unexpected error: {e}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm working on detecting spiral arm patterns in galaxy images and need a directional filter kernel to enhance these structures. How can I create and apply such a custom kernel?"
         },
         {
            "from": "assistant",
            "value": "import numpy as np\nfrom astropy.convolution import Kernel2D, convolve\n\ndef create_spiral_kernel(size=7, angle=45):\n    \"\"\"\n    Create a directional kernel for enhancing spiral arm patterns\n    \n    Parameters:\n    size : int\n        Size of the kernel (must be odd)\n    angle : float\n        Angle of the kernel pattern in degrees\n    \"\"\"\n    # Convert angle to radians\n    theta = np.radians(angle)\n    \n    # Create coordinate grid\n    y, x = np.mgrid[-(size//2):(size//2)+1, -(size//2):(size//2)+1]\n    \n    # Create spiral-like directional kernel\n    r = np.sqrt(x**2 + y**2)\n    phi = np.arctan2(y, x)\n    \n    # Modified kernel creation to ensure positive values\n    kernel_array = np.exp(-(r**2)/8) * (1 + np.cos(phi - theta))\n    \n    # Normalize kernel to ensure sum is 1\n    kernel_array = kernel_array / kernel_array.sum()\n    \n    # Create Kernel2D object with the array parameter\n    return Kernel2D(array=kernel_array)\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Create example galaxy image (dummy data)\n    image = np.random.normal(0, 1, (100, 100))\n    \n    # Create kernels at different angles\n    angles = [0, 45, 90, 135]\n    filtered_images = []\n    \n    # Apply directional filtering\n    for angle in angles:\n        kernel = create_spiral_kernel(size=7, angle=angle)\n        # Add normalize_kernel=False to prevent additional normalization\n        filtered = convolve(image, kernel, normalize_kernel=False)\n        filtered_images.append(filtered)\n    \n    # Combine filtered images to enhance spiral patterns\n    enhanced_image = np.max(filtered_images, axis=0)"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "For my exoplanet research, I want to visualize the spatial distribution of known exoplanetary systems relative to Earth. How can I transform their celestial coordinates and distances into cartesian coordinates?"
         },
         {
            "from": "assistant",
            "value": "from astropy.coordinates import SkyCoord\nimport astropy.units as u\nimport numpy as np\n\n# Example data for some known exoplanetary systems\n# (RA, Dec, Distance in parsecs)\nexoplanets = [\n    (\"16h 42m 30s\", \"-50d 48m 12s\", 1.3),  # Proxima Centauri b\n    (\"17h 57m 48s\", \"+04d 41m 36s\", 12.4),  # Kepler-186\n    (\"18h 44m 20s\", \"+38d 56m 51s\", 15.6),  # Kepler-22\n]\n\n# Create a SkyCoord object with positions and distances\ncoords = SkyCoord(\n    ra=[ex[0] for ex in exoplanets],\n    dec=[ex[1] for ex in exoplanets],\n    distance=[ex[2] for ex in exoplanets] * u.pc,\n    frame='icrs'\n)\n\n# Transform to Cartesian coordinates (in parsecs)\n# x toward the Galactic center\n# y in direction of Galactic rotation\n# z toward the Galactic north pole\ncartesian = coords.galactocentric.cartesian\n\n# Get the x, y, z components\nx = cartesian.x.value\ny = cartesian.y.value\nz = cartesian.z.value\n\n# Print the results\nfor i, name in enumerate(['Proxima Cen b', 'Kepler-186', 'Kepler-22']):\n    print(f\"{name}: x={x[i]:.1f}, y={y[i]:.1f}, z={z[i]:.1f} pc\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm working with multi-wavelength observations of a protoplanetary disk. How can I convert the observed brightness temperatures from Celsius to Kelvin for consistent analysis across different instruments?"
         },
         {
            "from": "assistant",
            "value": "from astropy import units as u\nfrom astropy.units.equivalencies import brightness_temperature, temperature\n\n# Sample brightness temperatures in Celsius\ntemp_celsius = [23.5, 25.1, 22.8, 24.3]\n\n# Convert list to Quantity object with Celsius units\ntemps_C = temp_celsius * u.Celsius\n\n# Convert to Kelvin using astropy's temperature conversion with temperature equivalency\ntemps_K = temps_C.to(u.K, equivalencies=temperature())\n\n# Print results\nfor c, k in zip(temp_celsius, temps_K):\n    print(f\"Temperature: {c}\u00b0C = {k:.2f}\")\n\n# Example of converting brightness temperature at different frequencies\n# Assuming we have radio observations at different frequencies\nfreq1 = 230 * u.GHz  # ALMA Band 6\nfreq2 = 345 * u.GHz  # ALMA Band 7\n\n# Define beam size (example: 1 arcsecond beam)\nbeam_size = 1 * u.arcsec\n\n# Calculate beam area (assuming Gaussian beam)\nbeam_area = beam_size * beam_size * 2 * 3.14159\n\n# Convert between brightness temperature and flux density\n# The brightness_temperature equivalency needs only frequency\neq_bright_temp = brightness_temperature(freq1)\n\n# Example conversion of a brightness temperature to flux density\ntemp_K = 25 * u.K\nflux = temp_K.to(u.Jy/beam_area.unit, equivalencies=eq_bright_temp)\nprint(f\"\\nBrightness temperature {temp_K} at {freq1} with {beam_size} beam = {flux:.2e}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "How can I create a FITS header card with key='EXPTIME', value=300.5 seconds, and comment='Total exposure time' using astropy.io.fits.Card?"
         },
         {
            "from": "assistant",
            "value": "from astropy.io.fits import Card\n\n# Create a FITS header card for exposure time\n# Key: EXPTIME, Value: 300.5, Comment: Total exposure time\nexptime_card = Card('EXPTIME', 300.5, 'Total exposure time')\n\n# Optional: Print card to verify\nprint(exptime_card)"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "After processing my spectral cube data, I want to write it to a new FITS file with updated header keywords reflecting the wavelength calibration. How do I do this?"
         },
         {
            "from": "assistant",
            "value": "from astropy.io import fits\nimport astropy.units as u\nfrom astropy.wcs import WCS\nimport numpy as np\n\n# Create example spectral cube data (3D numpy array)\n# Let's create a 10x10x100 cube (spatial x spatial x spectral)\ndata_cube = np.random.random((10, 10, 100))\n\n# Create a new WCS object for wavelength calibration\nw = WCS(naxis=3)\nw.wcs.crpix = [1, 1, 1]  # Reference pixel coordinates\nw.wcs.cdelt = [0.1, 0.1, 1.0]  # Pixel scale in degrees and wavelength step\nw.wcs.crval = [30.0, 40.0, 5000.0]  # Reference values (RA, Dec, wavelength)\nw.wcs.ctype = ['RA---TAN', 'DEC--TAN', 'WAVE']  # Coordinate types\nw.wcs.cunit = ['deg', 'deg', 'Angstrom']  # Units\n\n# Create new FITS header from WCS\nnew_header = w.to_header()\n\n# Add additional keywords to header\nnew_header['OBJECT'] = 'Your target name'\nnew_header['OBSERVER'] = 'Your name'\nnew_header['DATE-OBS'] = '2023-07-20'\n\n# Write the new FITS file\nhdu = fits.PrimaryHDU(data=data_cube, header=new_header)\nhdu.writeto('calibrated_cube.fits', overwrite=True)"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "For my solar eclipse predictions, I need to determine the Sun's location at 15-minute intervals during March 8, 2024, from 15:00 to 18:00 UTC. How can I use astropy.coordinates.get_sun to get these positions?"
         },
         {
            "from": "assistant",
            "value": "from astropy.time import Time\nfrom astropy.coordinates import get_sun\nimport numpy as np\n\n# Create time array with 15-minute intervals\nstart_time = Time('2024-03-08 15:00:00')\nend_time = Time('2024-03-08 18:00:00') \ntime_steps = np.linspace(0, (end_time - start_time).to_value('day'), 13)\ntimes = start_time + time_steps\n\n# Get sun positions for each time\nsun_positions = get_sun(times)\n\n# Print results\nfor t, pos in zip(times, sun_positions):\n    print(f\"Time: {t.iso}\")\n    print(f\"RA: {pos.ra.deg:.4f}\u00b0, Dec: {pos.dec.deg:.4f}\u00b0\\n\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "During my galaxy cluster analysis, I've identified interesting X-ray emission at pixel coordinate (2048, 1536). Please show me how to transform this to ICRS coordinates, accounting for any image distortions in my WCS solution."
         },
         {
            "from": "assistant",
            "value": "from astropy.wcs import WCS\nfrom astropy.coordinates import SkyCoord\nimport astropy.units as u\nimport numpy as np\n\n# Create a WCS object with a realistic setup\n# This is an example setup - replace with your actual FITS header values\nwcs = WCS(naxis=2)\nwcs.wcs.crpix = [2048, 1536]  # Reference pixel\nwcs.wcs.crval = [180.0, 0.0]  # Reference coordinate (RA=180 deg, Dec=0 deg)\nwcs.wcs.cdelt = [-0.001, 0.001]  # Pixel scale in degrees\nwcs.wcs.ctype = ['RA---TAN', 'DEC--TAN']  # Coordinate type\n\n# Convert pixel coordinates to world coordinates (ICRS)\n# Note: pixel coordinates in Python are 0-based, so subtract 1 from FITS coordinates\npixel_x, pixel_y = 2048-1, 1536-1  # Convert from FITS to Python coordinates\n\n# Transform pixel to world coordinates\npixels = np.array([[pixel_x, pixel_y]])\nworld = wcs.all_pix2world(pixels, 0)  # 0 is the origin parameter\n\n# Extract RA and Dec from the transformation\nra, dec = world[0]\n\n# Ensure coordinates are within valid ranges\n# RA should be 0 to 360 degrees\nra = ra % 360\n# Dec should be between -90 and 90 degrees\ndec = np.clip(dec, -90, 90)\n\n# Create a SkyCoord object for easier handling\ncoords = SkyCoord(ra=ra*u.deg, dec=dec*u.deg, frame='icrs')\n\n# Display coordinates in sexagesimal format\nprint(f\"ICRS Coordinates: RA={coords.ra.to_string(unit=u.hour)}, \"\n      f\"Dec={coords.dec.to_string(unit=u.deg)}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "For my survey data, I have separate photometric measurements in different filters that I want to combine into a single FITS file. How can I append these measurements while verifying the data integrity?"
         },
         {
            "from": "assistant",
            "value": "from astropy.io import fits\nfrom astropy.table import Table, vstack\nimport numpy as np\nimport os\n\ndef combine_photometry(filter_files, output_file):\n    \"\"\"\n    Combine photometric measurements from different filters into a single FITS file\n    with data verification.\n    \n    Parameters:\n    filter_files : list of str\n        List of FITS files containing photometry in different filters\n    output_file : str\n        Output filename for combined data\n    \"\"\"\n    tables = []\n    \n    # First check if files exist\n    for file in filter_files:\n        if not os.path.exists(file):\n            print(f\"Warning: File {file} does not exist\")\n            continue\n    \n    # Read and verify each filter's data\n    for file in filter_files:\n        try:\n            # Read the FITS file\n            with fits.open(file) as hdul:\n                # Check if file has extensions\n                if len(hdul) < 2:\n                    print(f\"Warning: {file} has no extensions, trying primary HDU\")\n                    data = Table.read(hdul[0])\n                else:\n                    data = Table.read(hdul[1])  # Read first extension\n                \n                # Basic data integrity checks\n                if len(data) == 0:\n                    print(f\"Warning: Empty data in {file}\")\n                    continue\n                \n                # Check for NaN values\n                for col in data.columns:\n                    if np.any(np.isnan(data[col])):\n                        print(f\"Warning: NaN values found in {col} of {file}\")\n                \n                tables.append(data)\n                print(f\"Successfully read data from {file}\")\n                \n        except Exception as e:\n            print(f\"Error processing {file}: {str(e)}\")\n            continue\n    \n    if not tables:\n        raise ValueError(\"No valid data to combine. Please check if input files exist and contain valid data.\")\n    \n    try:\n        # Combine tables\n        combined_table = vstack(tables)\n        \n        # Sort by object ID if it exists\n        if 'ID' in combined_table.colnames:\n            combined_table.sort('ID')\n        \n        # Write combined data to new FITS file\n        combined_table.write(output_file, format='fits', overwrite=True)\n        print(f\"Successfully combined data into {output_file}\")\n        \n    except Exception as e:\n        raise Exception(f\"Error while combining tables: {str(e)}\")\n\n# Example usage with error handling\ntry:\n    filter_files = ['u_band.fits', 'g_band.fits', 'r_band.fits']\n    combine_photometry(filter_files, 'combined_photometry.fits')\nexcept Exception as e:\n    print(f\"Failed to combine photometry: {str(e)}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I need to convert these cartesian coordinates (x=150, y=230, z=85) in parsecs to a CartesianRepresentation for further analysis. How can I create this representation with astropy?"
         },
         {
            "from": "assistant",
            "value": "from astropy.coordinates import CartesianRepresentation\nfrom astropy import units as u\n\n# Create CartesianRepresentation from x, y, z coordinates in parsecs\ncoords = CartesianRepresentation(\n    x=150 * u.pc,  # x coordinate with parsec units\n    y=230 * u.pc,  # y coordinate with parsec units\n    z=85 * u.pc    # z coordinate with parsec units\n)\n\n# The representation is now ready for use in coordinate transformations\n# or other astronomical calculations"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I have optical spectra measurements of a supernova taken at 200 different times between day 0 and day 100 post-explosion, with typical measurement uncertainties of 0.05 in flux. How can I use bayesian_blocks to identify distinct phases in the spectral evolution using the 'measures' fitness function?"
         },
         {
            "from": "assistant",
            "value": "import numpy as np\nfrom astropy.stats import bayesian_blocks\n\n# Generate sample data (since real data not provided)\ntimes = np.linspace(0, 100, 200)  # 200 measurements from day 0 to 100\nnp.random.seed(42)  # For reproducibility\n# Simulate flux measurements with different phases\nfluxes = np.zeros_like(times)\nfluxes[times < 20] = 2 + 0.1*np.random.randn(len(times[times < 20]))  # Early phase\nfluxes[(times >= 20) & (times < 50)] = 1 + 0.1*np.random.randn(len(times[(times >= 20) & (times < 50)]))  # Middle phase\nfluxes[times >= 50] = 0.5 + 0.1*np.random.randn(len(times[times >= 50]))  # Late phase\n\n# Set measurement uncertainties\nsigma = 0.05 * np.ones_like(times)  # Constant uncertainty of 0.05\n\n# Apply bayesian_blocks using 'measures' fitness function\n# This is appropriate for measured data with gaussian errors\nedges = bayesian_blocks(times, fluxes, sigma, fitness='measures')\n\n# Print the identified phase change points\nphase_changes = edges[1:-1]  # Exclude first and last edges\nprint(f\"Phase changes detected at days: {phase_changes}\")\n\n# Calculate duration of each phase\nphase_durations = np.diff(edges)\nprint(f\"Duration of each phase (days): {phase_durations}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I need to define a parameter for the absolute magnitude of a star with a default value of -5.5. The parameter should be restricted between -10 and 0, and use units of mag. Show me how to create this Parameter object."
         },
         {
            "from": "assistant",
            "value": "from astropy.modeling import Parameter\nfrom astropy import units as u\n\n# Define absolute magnitude parameter with constraints\nabs_mag = Parameter('abs_mag', default=-5.5, min=-10.0, max=0.0, unit=u.mag,\n                   description='Absolute magnitude of the star')"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm studying line broadening mechanisms in stellar atmospheres and need to create a Voigt profile where I can adjust both the Gaussian and Lorentzian components independently. Can you show me how to set this up with appropriate parameter ranges for stellar spectral lines?"
         },
         {
            "from": "assistant",
            "value": "import numpy as np\nfrom astropy.modeling.models import Voigt1D\nfrom astropy import units as u\nimport matplotlib.pyplot as plt\n\n# Set up wavelength grid centered at line core (in Angstroms)\n# Typical width for stellar lines ~1-2 Angstroms\nwavelength = np.linspace(-2, 2, 1000)  # Remove units for the calculation\n\n# Create Voigt profile\n# Gaussian width (sigma) ~ thermal broadening\n# Lorentzian width (gamma) ~ pressure/natural broadening\nvoigt = Voigt1D(x_0=0, amplitude_L=1, fwhm_G=0.2, fwhm_L=0.1)\n\n# Calculate profile\nprofile = voigt(wavelength)\n\n# Plot the profile\nplt.figure(figsize=(10,6))\nplt.plot(wavelength, profile)\nplt.xlabel('Wavelength offset (\u00c5)')\nplt.ylabel('Normalized flux')\nplt.title('Voigt Profile for Stellar Spectral Line')\n\n# Example of adjusting components\n# Increase pressure broadening\nvoigt_pressure = Voigt1D(x_0=0, amplitude_L=1, fwhm_G=0.2, fwhm_L=0.3)\nplt.plot(wavelength, voigt_pressure(wavelength), '--', \n         label='Increased pressure broadening')\nplt.legend()\nplt.grid(True)\nplt.show()"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm studying the distribution of molecular clouds in a galaxy's disk. Given a point at cylindrical coordinates rho=25000 light-years, phi=120 degrees, and z=500 light-years, show me how to create this representation and then convert it to Cartesian coordinates."
         },
         {
            "from": "assistant",
            "value": "from astropy.coordinates import CylindricalRepresentation, CartesianRepresentation\nfrom astropy import units as u\n\n# Create a point in cylindrical coordinates\n# rho: distance from rotation axis\n# phi: azimuthal angle\n# z: height above galactic plane\ncloud_position = CylindricalRepresentation(\n    rho=25000 * u.lyr,\n    phi=120 * u.deg,\n    z=500 * u.lyr\n)\n\n# Convert to Cartesian coordinates (x, y, z)\ncart_position = cloud_position.to_cartesian()\n\n# Print both representations\nprint(\"Cylindrical coordinates:\")\nprint(f\"\u03c1 = {cloud_position.rho:.1f}\")\nprint(f\"\u03c6 = {cloud_position.phi:.1f}\")\nprint(f\"z = {cloud_position.z:.1f}\")\nprint(\"\\nCartesian coordinates:\")\nprint(f\"x = {cart_position.x:.1f}\")\nprint(f\"y = {cart_position.y:.1f}\")\nprint(f\"z = {cart_position.z:.1f}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm studying the age of the universe at different epochs. Can you show me how to calculate the age of the universe at redshift z=2.3 using Planck18 cosmology, and compare it with the current age (z=0)?"
         },
         {
            "from": "assistant",
            "value": "# Import required astropy modules\nfrom astropy.cosmology import Planck18\nimport astropy.units as u\n\n# Calculate age of universe at z=2.3\nage_at_z2p3 = Planck18.age(2.3)\n\n# Get current age of universe (z=0)\ncurrent_age = Planck18.age(0)\n\n# Print results with proper units\nprint(f\"Age of universe at z=2.3: {age_at_z2p3:.2f}\")\nprint(f\"Current age of universe: {current_age:.2f}\")\nprint(f\"Time elapsed since z=2.3: {(current_age - age_at_z2p3):.2f}\")\n\n# Can also convert to different units if needed\nage_in_billions = age_at_z2p3.to(u.Gyr)\nprint(f\"\\nAge at z=2.3 in billions of years: {age_in_billions:.2f}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm studying exoplanet transit timing variations and want to calculate the scatter in the timing residuals while reducing the impact of occasional poor measurements. Could you show me how to implement this using robust statistics?"
         },
         {
            "from": "assistant",
            "value": "from astropy.stats import sigma_clip, mad_std\nimport numpy as np\n\ndef calculate_ttv_scatter(timing_measurements, timing_errors=None):\n    \"\"\"\n    Calculate robust scatter in transit timing variations using sigma clipping\n    and median absolute deviation.\n    \n    Parameters:\n    timing_measurements : array-like\n        Observed transit times minus calculated linear ephemeris\n    timing_errors : array-like, optional\n        Uncertainties in the timing measurements\n    \"\"\"\n    \n    # Perform sigma clipping to remove outliers\n    # Using 3-sigma clipping over 3 iterations\n    filtered_data = sigma_clip(timing_measurements, \n                             sigma=3, \n                             maxiters=3, \n                             masked=True)\n    \n    # Calculate robust scatter using median absolute deviation\n    # mad_std is less sensitive to outliers than standard deviation\n    ttv_scatter = mad_std(filtered_data.compressed())\n    \n    # If timing errors are provided, calculate weighted statistics\n    if timing_errors is not None:\n        weights = 1.0 / (timing_errors[~filtered_data.mask])**2\n        weighted_median = np.average(filtered_data.compressed(), \n                                   weights=weights)\n        \n        return ttv_scatter, weighted_median\n    \n    return ttv_scatter\n\n# Example usage:\n# ttv_measurements = np.array([...])  # TTV measurements in minutes\n# measurement_errors = np.array([...]) # Uncertainties in minutes\n# scatter = calculate_ttv_scatter(ttv_measurements, measurement_errors)"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I need to write code to get the coordinates of Mars at midnight UTC on June 15, 2023, as observed from the Mauna Kea Observatory (latitude=19.8206\u00b0N, longitude=155.4681\u00b0W, elevation=4207.1m). How can I use astropy's get_body function?"
         },
         {
            "from": "assistant",
            "value": "from astropy.coordinates import EarthLocation, get_body\nfrom astropy.time import Time\nimport astropy.units as u\n\n# Define Mauna Kea Observatory location\nmauna_kea = EarthLocation(\n    lat=19.8206*u.deg, \n    lon=-155.4681*u.deg,\n    height=4207.1*u.m\n)\n\n# Set the observation time (midnight UTC on June 15, 2023)\nobs_time = Time('2023-06-15 00:00:00')\n\n# Get Mars' position\nmars = get_body('mars', obs_time, mauna_kea)\n\n# Convert to string representation\nra = mars.ra.to_string(unit=u.hour, sep=':', pad=True)\ndec = mars.dec.to_string(unit=u.degree, sep=':', pad=True)\n\nprint(f\"Mars' position at {obs_time.iso}:\")\nprint(f\"RA: {ra}\")\nprint(f\"Dec: {dec}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm working with a CCD image of the Orion Nebula with pixel values between 100 and 10000. Can you show me how to combine SqrtStretch with normalization to better visualize the nebula's structure in matplotlib?"
         },
         {
            "from": "assistant",
            "value": "from astropy.visualization import SqrtStretch, ImageNormalize\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Simulate a CCD image of Orion Nebula (replace this with your actual image data)\nimage_data = np.random.uniform(100, 10000, size=(500, 500))\n\n# Create normalization object with SqrtStretch\n# Using interval min/max based on the image's intensity range\nnorm = ImageNormalize(\n    image_data,\n    stretch=SqrtStretch(),\n    vmin=100,    # minimum pixel value\n    vmax=10000   # maximum pixel value\n)\n\n# Display the normalized and stretched image\nplt.figure(figsize=(8, 8))\nplt.imshow(image_data, norm=norm, cmap='viridis')\nplt.colorbar(label='Sqrt-scaled Intensity')\nplt.title('Orion Nebula - Sqrt Stretch Visualization')\nplt.show()"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm analyzing data from the Chandra X-ray Observatory and need to create a catalog of X-ray sources. How can I convert the pixel positions of detected sources into their corresponding RA and Dec coordinates?"
         },
         {
            "from": "assistant",
            "value": "from astropy.wcs import WCS\nfrom astropy.io import fits\nfrom astropy.coordinates import SkyCoord\nimport numpy as np\n\n# Load the Chandra FITS file and WCS information\ndef pixel_to_radec(fits_file, x_pixels, y_pixels):\n    \"\"\"\n    Convert pixel coordinates to RA/Dec for Chandra sources\n    \n    Parameters:\n    fits_file: str - Path to Chandra FITS file\n    x_pixels, y_pixels: arrays - Source positions in pixels\n    \"\"\"\n    try:\n        # Open FITS file and get WCS from header\n        with fits.open(fits_file) as hdul:\n            # Get WCS from primary header\n            w = WCS(hdul[0].header)\n            \n            # Convert pixel to world coordinates\n            # pixel_to_world expects zero-based coordinates\n            ra, dec = w.pixel_to_world(x_pixels, y_pixels)\n            \n            # Create source catalog as SkyCoord object\n            sources = SkyCoord(ra, dec, frame='icrs')\n            \n            return sources\n            \n    except Exception as e:\n        print(f\"Error processing FITS file: {e}\")\n        return None\n\n# Example usage\nif __name__ == \"__main__\":\n    # Example pixel coordinates (x,y)\n    x = np.array([100.5, 150.7, 200.2])\n    y = np.array([350.1, 400.3, 450.8])\n    \n    # Convert to RA/Dec (replace with your actual FITS file)\n    sources = pixel_to_radec('chandra_image.fits', x, y)\n    \n    if sources is not None:\n        # Print results in degrees\n        print(\"RA, Dec (degrees):\")\n        for src in sources:\n            print(f\"{src.ra.deg:.6f}, {src.dec.deg:.6f}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm working on subtracting instrumental background from my telescope observations. What's the code to create a simple constant background model and fit it to my data using Astropy?"
         },
         {
            "from": "assistant",
            "value": "from astropy.modeling import models, fitting\nimport numpy as np\n\n# Create sample data (replace with your actual observation data)\nx = np.linspace(0, 100, 1000)\ny = np.random.normal(10, 0.5, 1000)  # Simulated data with constant background\n\n# Create constant model for background\nbg_model = models.Const1D(amplitude=5.0)  # Initial guess of background level\n\n# Initialize fitter\nfitter = fitting.LevMarLSQFitter()\n\n# Fit the model to data\nfitted_bg = fitter(bg_model, x, y)\n\n# Print fitted background level and uncertainty\nprint(f\"Fitted background level: {fitted_bg.amplitude.value:.3f}\")\nprint(f\"Background uncertainty: {fitter.fit_info['param_cov'][0][0]**0.5:.3f}\")\n\n# Subtract background from data\nbg_subtracted = y - fitted_bg(x)"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "For my radio astronomy observations at 1420 MHz (hydrogen line), I need to convert between energy, frequency, and wavelength. Show me how to use astropy's unit equivalencies to perform these interchangeable conversions."
         },
         {
            "from": "assistant",
            "value": "from astropy import units as u\nfrom astropy.constants import h, c\n\n# Define our frequency of interest (hydrogen line)\nfreq = 1420 * u.MHz\n\n# Set up spectral equivalencies for conversions between freq/wavelength/energy\nequiv = u.spectral()\n\n# Convert frequency to wavelength\nwavelength = freq.to(u.cm, equivalencies=equiv)\nprint(f\"Wavelength: {wavelength:.3f}\")\n\n# Convert frequency to energy\nenergy = freq.to(u.eV, equivalencies=equiv)\nprint(f\"Energy: {energy:.3f}\")\n\n# Convert back to frequency from wavelength\nfreq_from_wave = wavelength.to(u.MHz, equivalencies=equiv)\nprint(f\"Frequency from wavelength: {freq_from_wave:.1f}\")\n\n# Convert back to frequency from energy\nfreq_from_energy = energy.to(u.MHz, equivalencies=equiv)\nprint(f\"Frequency from energy: {freq_from_energy:.1f}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm working on analyzing multiple exoplanet transit observations. Can you show me how to convert the timestamp '2023-09-15 18:30:45.123' into an Astropy Time object in ISO format for precise timing calculations?"
         },
         {
            "from": "assistant",
            "value": "from astropy.time import Time\n\n# Create Time object from timestamp string\n# Using ISO format for precise timing of exoplanet transit\ntimestamp = '2023-09-15 18:30:45.123'\nt = Time(timestamp, format='iso', scale='utc')\n\n# Print the time in different useful formats for transit timing\nprint(f\"ISO Format: {t.iso}\")\nprint(f\"Julian Date: {t.jd}\")  # Useful for period calculations\nprint(f\"Modified Julian Date: {t.mjd}\")  # Common in modern astronomy"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "How can I convert stellar positions given in B1900.0 coordinates from an old astronomical catalog to more current coordinates using Astropy?"
         },
         {
            "from": "assistant",
            "value": "from astropy.coordinates import FK4, FK5, SkyCoord\nfrom astropy import units as u\nfrom astropy.time import Time\n\n# Example B1900.0 coordinates (RA: 2h 31m 48.7s, Dec: +89d 15' 51\")\nold_coords = SkyCoord('02 31 48.7', '+89 15 51', \n                     unit=(u.hourangle, u.deg),\n                     frame=FK4(equinox='B1900.0'))\n\n# Convert to current epoch (J2000)\ncurrent_coords = old_coords.transform_to(FK5(equinox='J2000'))\n\n# Convert to more recent epoch (e.g., J2023.0)\nrecent_epoch = Time('J2023.0')\nmodern_coords = old_coords.transform_to(FK5(equinox=recent_epoch))\n\n# Print results\nprint(f\"Original B1900.0 coordinates:\\n{old_coords}\\n\")\nprint(f\"J2000.0 coordinates:\\n{current_coords}\\n\")\nprint(f\"J2023.0 coordinates:\\n{modern_coords}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "What's the code to create a scatter plot showing the relationship between galaxy masses and their star formation rates, with appropriate astronomical unit labels?"
         },
         {
            "from": "assistant",
            "value": "import numpy as np\nfrom astropy import units as u\nimport matplotlib.pyplot as plt\n\n# Generate sample data (in this case using random numbers)\n# Typical galaxy masses range from 10^8 to 10^12 solar masses\ngalaxy_masses = np.random.uniform(8, 12, 100)\ngalaxy_masses = (10**galaxy_masses) * u.Msun\n\n# Star formation rates typically range from 0.1 to 100 solar masses per year\nsfr = np.random.uniform(-1, 2, 100)\nsfr = (10**sfr) * u.Msun/u.yr\n\n# Create the scatter plot\nplt.figure(figsize=(10, 6))\nplt.scatter(galaxy_masses.value, sfr.value, alpha=0.5)\n\n# Set log scales (since these quantities span many orders of magnitude)\nplt.xscale('log')\nplt.yscale('log')\n\n# Labels and title\nplt.xlabel('Galaxy Mass (M$_{\\odot}$)')\nplt.ylabel('Star Formation Rate (M$_{\\odot}$ yr$^{-1}$)')\nplt.title('Galaxy Mass vs Star Formation Rate')\n\n# Add a grid for better readability\nplt.grid(True, which=\"both\", ls=\"-\", alpha=0.2)\n\nplt.show()"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm working on a global observatory network project and need to determine if my target star will be visible from multiple locations. How do I define several observatory positions in Hawaii, Chile, and South Africa?"
         },
         {
            "from": "assistant",
            "value": "from astropy.coordinates import EarthLocation\nfrom astropy import units as u\n\n# Define major observatory locations using longitude, latitude, and elevation\nmauna_kea = EarthLocation.from_geodetic(\n    lat=19.8208 * u.deg,\n    lon=-155.4681 * u.deg,\n    height=4205 * u.m\n)\n\nparanal = EarthLocation.from_geodetic(\n    lat=-24.6275 * u.deg,\n    lon=-70.4044 * u.deg,\n    height=2635 * u.m\n)\n\nsaao = EarthLocation.from_geodetic(\n    lat=-32.3783 * u.deg,\n    lon=20.8105 * u.deg,\n    height=1798 * u.m\n)\n\n# Create a dictionary of observatories with names for easy iteration\nobservatories = {\n    'Mauna Kea': mauna_kea,\n    'Paranal Observatory': paranal,\n    'South African Astronomical Observatory': saao\n}\n\n# Print observatory locations\nfor name, obs in observatories.items():\n    print(f\"{name}: Lat={obs.lat.deg:.4f}, Lon={obs.lon.deg:.4f}, Height={obs.height:.0f}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm trying to compare different profile models for my AGN observation. Can you demonstrate how to create a Moffat1D model with amplitude=3.0, x_0=0.0, gamma=1.5, alpha=2.5, and plot it alongside a Gaussian1D (amplitude=3.0, mean=0.0, stddev=1.0) to visualize their different wing behaviors in the range x=[-4, 4]?"
         },
         {
            "from": "assistant",
            "value": "import numpy as np\nfrom astropy.modeling import models\nimport matplotlib.pyplot as plt\n\n# Create x values for plotting\nx = np.linspace(-4, 4, 200)\n\n# Create the models\nmoffat = models.Moffat1D(amplitude=3.0, x_0=0.0, gamma=1.5, alpha=2.5)\ngaussian = models.Gaussian1D(amplitude=3.0, mean=0.0, stddev=1.0)\n\n# Evaluate models at x positions\nmoffat_profile = moffat(x)\ngaussian_profile = gaussian(x)\n\n# Create the plot\nplt.figure(figsize=(8, 6))\nplt.plot(x, moffat_profile, 'b-', label='Moffat1D')\nplt.plot(x, gaussian_profile, 'r--', label='Gaussian1D')\n\n# Add labels and title\nplt.xlabel('Position')\nplt.ylabel('Intensity')\nplt.title('Comparison of Moffat and Gaussian Profiles')\nplt.legend()\n\n# Add grid for better visualization\nplt.grid(True, alpha=0.3)\n\n# Display the plot\nplt.show()"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "For my stellar population study, I have metallicity measurements at different galactic radii. What's the proper way to fit a linear gradient to determine the radial metallicity distribution in this galaxy?"
         },
         {
            "from": "assistant",
            "value": "from astropy.modeling import models, fitting\nimport numpy as np\n\n# Sample data (replace with your actual measurements)\nradii_kpc = np.array([2.1, 4.3, 6.8, 8.2, 10.5, 12.7, 15.1])  # Galactic radii in kpc\nmetallicity = np.array([-0.2, -0.3, -0.4, -0.5, -0.6, -0.7, -0.8])  # [Fe/H] or Z/Z\u2299\nmetallicity_errors = np.array([0.1, 0.1, 0.12, 0.1, 0.15, 0.1, 0.13])  # Measurement uncertainties\n\n# Initialize a linear model for the metallicity gradient\nlinear_model = models.Linear1D(slope=-0.05, intercept=-0.1)  # Initial guess\nfitter = fitting.LevMarLSQFitter()\n\n# Fit the model to the data with weights from uncertainties\nfitted_model = fitter(linear_model, radii_kpc, metallicity, \n                     weights=1.0/metallicity_errors)\n\n# Calculate gradient in dex/kpc and intercept\ngradient = fitted_model.slope.value\nintercept = fitted_model.intercept.value\n\n# Print results\nprint(f\"Metallicity gradient: {gradient:.3f} dex/kpc\")\nprint(f\"Central metallicity: {intercept:.2f} dex\")\n\n# Optional: Calculate R-squared value to assess fit quality\nresiduals = metallicity - fitted_model(radii_kpc)\nss_res = np.sum(residuals**2)\nss_tot = np.sum((metallicity - np.mean(metallicity))**2)\nr_squared = 1 - (ss_res / ss_tot)\nprint(f\"R-squared value: {r_squared:.3f}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I need a code snippet to create a custom time zone at UTC+5:30 (like for India) using astropy.time.TimezoneInfo for analyzing observation times from the Indian Astronomical Observatory in Ladakh. Show how to convert an Astropy Time object at '2023-07-15 22:30:00' to this timezone."
         },
         {
            "from": "assistant",
            "value": "from astropy.time import Time, TimezoneInfo\nimport astropy.units as u\n\n# Create a timezone for UTC+5:30 (India)\n# Convert hours and minutes to a single unit in hours\nutc_offset = 5.5 * u.hour  # 5.5 hours = 5 hours and 30 minutes\nindia_tz = TimezoneInfo(utc_offset=utc_offset)\n\n# Create a Time object in UTC\nutc_time = Time('2023-07-15 22:30:00', format='iso', scale='utc')\n\n# Convert to Indian time\nindia_time = Time(utc_time + utc_offset)\n\n# Format the output\nprint(f\"UTC time: {utc_time.iso}\")\nprint(f\"India time: {india_time.iso}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm studying stellar occultation events and need to simulate the light curve when a star is blocked by an asteroid. How can I use Box1D to create this rectangular dip in brightness?"
         },
         {
            "from": "assistant",
            "value": "import numpy as np\nfrom astropy.modeling.models import Box1D\nimport matplotlib.pyplot as plt\n\n# Create time array (in minutes) for the light curve\nt = np.linspace(-10, 10, 1000)\n\n# Create Box1D model for the occultation\n# amplitude=-0.8 represents 80% drop in brightness\n# width=4 represents 4-minute occultation duration\n# x_0=0 centers the event at t=0\noccultation = Box1D(amplitude=-0.8, width=4, x_0=0)\n\n# Create the complete light curve\n# Start at 1 (full brightness) and add the dip\nlight_curve = 1 + occultation(t)\n\n# Plot the light curve\nplt.figure(figsize=(10, 6))\nplt.plot(t, light_curve, 'k-', label='Occultation Light Curve')\nplt.xlabel('Time (minutes)')\nplt.ylabel('Relative Brightness')\nplt.title('Stellar Occultation by Asteroid')\nplt.grid(True)\nplt.legend()\nplt.show()"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "How do I correct my measured galaxy positions for proper motion when combining observations from different epochs?"
         },
         {
            "from": "assistant",
            "value": "from astropy.coordinates import SkyCoord\nfrom astropy.time import Time\nimport astropy.units as u\n\ndef correct_for_proper_motion(ra, dec, pm_ra, pm_dec, ref_epoch, target_epoch):\n    \"\"\"\n    Correct galaxy positions for proper motion between epochs\n    \n    Parameters:\n    ra, dec: Position in degrees at reference epoch\n    pm_ra, pm_dec: Proper motion in mas/yr\n    ref_epoch, target_epoch: Observation epochs in ISO format (e.g. '2010-01-01')\n    \"\"\"\n    # Create coordinate object with proper motion\n    coords = SkyCoord(\n        ra=ra * u.deg,\n        dec=dec * u.deg,\n        pm_ra_cosdec=pm_ra * u.mas/u.yr,\n        pm_dec=pm_dec * u.mas/u.yr,\n        obstime=Time(ref_epoch)\n    )\n    \n    # Apply proper motion to get position at target epoch\n    coords_target = coords.apply_space_motion(new_obstime=Time(target_epoch))\n    \n    return coords_target.ra.deg, coords_target.dec.deg\n\n# Example usage\nra_ref = 150.5  # RA in degrees\ndec_ref = -23.4  # Dec in degrees\npm_ra = 10.5    # Proper motion in RA (mas/yr)\npm_dec = -15.2  # Proper motion in Dec (mas/yr)\n\n# Correct position from 2010 to 2020\nra_new, dec_new = correct_for_proper_motion(\n    ra_ref, dec_ref, pm_ra, pm_dec, \n    '2010-01-01', '2020-01-01'\n)"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm developing a script for telescope pointing and need to validate whether target coordinates fall within certain limits. How can I create a function that takes angular input in various formats (e.g., '12h30m', '45.5d') and compares them with specific angular thresholds?"
         },
         {
            "from": "assistant",
            "value": "from astropy import units as u\nfrom astropy.coordinates import Angle\n\ndef validate_angular_limits(angle_input, min_limit, max_limit):\n    \"\"\"\n    Validates if an angular value falls within specified limits.\n    \n    Parameters:\n    angle_input (str): Angular value in various formats (e.g., '12h30m', '45.5d')\n    min_limit (str): Minimum allowed angle in degrees (e.g., '0d')\n    max_limit (str): Maximum allowed angle in degrees (e.g., '90d')\n    \n    Returns:\n    bool: True if angle is within limits, False otherwise\n    \"\"\"\n    try:\n        # Convert input to Angle object\n        target_angle = Angle(angle_input)\n        \n        # Convert limits to Angle objects\n        min_angle = Angle(min_limit)\n        max_angle = Angle(max_limit)\n        \n        # Normalize all angles to degree values for comparison\n        target_deg = target_angle.deg\n        min_deg = min_angle.deg\n        max_deg = max_angle.deg\n        \n        # Check if angle falls within limits\n        return min_deg <= target_deg <= max_deg\n    \n    except ValueError as e:\n        print(f\"Error processing angles: {e}\")\n        return False\n\n# Example usage\nif __name__ == \"__main__\":\n    # Test with different formats\n    test_cases = [\n        ('12h30m', '0d', '180d'),  # Hour angle format\n        ('45.5d', '0d', '90d'),    # Decimal degrees\n        ('-30d', '-45d', '45d'),   # Negative angles\n    ]\n    \n    for test_input, test_min, test_max in test_cases:\n        result = validate_angular_limits(test_input, test_min, test_max)\n        print(f\"Input: {test_input}, Range: [{test_min}, {test_max}], Valid: {result}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I need to compute the luminosity distance to a quasar at redshift 6.7 using the most recent Planck cosmology. What's the code to do this?"
         },
         {
            "from": "assistant",
            "value": "from astropy.cosmology import Planck18\nfrom astropy import units as u\n\n# Set redshift of the quasar\nz = 6.7\n\n# Calculate luminosity distance using Planck 2018 cosmology\n# Planck18 is the latest Planck cosmology with H0=67.4 km/s/Mpc, Om0=0.315\nlum_distance = Planck18.luminosity_distance(z)\n\n# Print result in Mpc\nprint(f\"Luminosity distance to z={z} quasar: {lum_distance:.2f}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm analyzing observations from multiple observatories of the Carina Nebula and need to normalize surface brightness measurements. How can I compute the projected pixel area using the WCS information to standardize these measurements?"
         },
         {
            "from": "assistant",
            "value": "from astropy.wcs import WCS\nfrom astropy.io import fits\nimport astropy.units as u\nimport numpy as np\n\ndef get_pixel_area(fits_file):\n    \"\"\"\n    Calculate the projected pixel area using WCS information.\n    Returns area in square arcseconds.\n    \"\"\"\n    # Open FITS file and get WCS\n    with fits.open(fits_file) as hdul:\n        wcs = WCS(hdul[0].header)\n        \n        # Get image dimensions\n        ny, nx = hdul[0].data.shape\n        \n        # Create pixel coordinate arrays for center of image\n        x = nx // 2\n        y = ny // 2\n        \n        # Get pixel scale by computing small offset\n        ra1, dec1 = wcs.all_pix2world(x, y, 0)\n        ra2, dec2 = wcs.all_pix2world(x+1, y+1, 0)\n        \n        # Calculate pixel scales in arcsec\n        ra_scale = abs(ra2 - ra1) * 3600 * np.cos(np.deg2rad(dec1))\n        dec_scale = abs(dec2 - dec1) * 3600\n        \n        # Calculate pixel area\n        pixel_area = ra_scale * dec_scale * u.arcsec**2\n        \n        return pixel_area\n\n# Example usage:\n# pixel_area = get_pixel_area('carina_image.fits')\n# normalized_sb = raw_surface_brightness / pixel_area"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "For my exoplanet transit data where the flux varies between 0.97 and 1.02, I want to create a normalized light curve plot spanning 6 hours. How can I use astropy_mpl_style to format the plot according to astronomical conventions?"
         },
         {
            "from": "assistant",
            "value": "# Import required modules\nfrom astropy.visualization import astropy_mpl_style\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom astropy.time import Time\nfrom astropy import units as u\n\n# Set the style\nplt.style.use(astropy_mpl_style)\n\n# Generate sample time points spanning 6 hours\nt0 = Time('2023-01-01T00:00:00')\nduration = 6 * u.hour\ntimestamps = np.linspace(0, duration.to(u.hour).value, 200)\n\n# Generate sample normalized flux data with a transit dip\nflux = np.ones(len(timestamps))\ntransit_center = 3  # Center of transit at 3 hours\ntransit_width = 0.5  # Transit duration in hours\ndepth = 0.03  # Transit depth\n\n# Create artificial transit\nflux -= depth * np.exp(-(timestamps - transit_center)**2 / (2 * transit_width**2))\n# Add some noise\nflux += np.random.normal(0, 0.001, len(timestamps))\n\n# Create the plot\nfig, ax = plt.subplots(figsize=(10, 6))\nax.plot(timestamps, flux, 'k.', markersize=2)\n\n# Set labels and title\nax.set_xlabel('Time from Start [hours]')\nax.set_ylabel('Normalized Flux')\nax.set_title('Exoplanet Transit Light Curve')\n\n# Set y-axis limits to match the specified range\nax.set_ylim(0.97, 1.02)\n\n# Add grid for better readability\nax.grid(True, linestyle='--', alpha=0.7)\n\nplt.tight_layout()\nplt.show()"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I have brightness measurements of a variable star and want to find its average magnitude using a robust statistic. Show me how to use biweight_location with an initial guess of 15.5 mag for this data: [15.7, 15.4, 15.6, 15.5, 18.0, 15.3, 15.6]."
         },
         {
            "from": "assistant",
            "value": "from astropy.stats import biweight_location\nimport numpy as np\n\n# Create array of magnitude measurements\nmagnitudes = np.array([15.7, 15.4, 15.6, 15.5, 18.0, 15.3, 15.6])\n\n# Calculate robust mean using biweight location estimator\nrobust_mean = biweight_location(magnitudes)\n\nprint(f\"Robust mean magnitude: {robust_mean:.2f}\")\n\n# Note: biweight_location is resistant to outliers like the 18.0 measurement\n# which would skew a regular arithmetic mean"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "How do I apply jackknife resampling to estimate the statistical uncertainty in my measured stellar metallicity values from spectroscopic data?"
         },
         {
            "from": "assistant",
            "value": "import numpy as np\nfrom astropy.stats import jackknife_resampling, jackknife_stats\n\n# Example spectroscopic metallicity measurements for a sample of stars\n# In practice, these would come from your actual measurements\nmetallicities = np.array([-1.5, -2.1, -1.8, -1.6, -1.9, -2.0, -1.7, -1.4, -1.8, -1.9])\n\n# Perform jackknife resampling\n# This creates n subsamples, each with one observation removed\njackknife_samples = jackknife_resampling(metallicities)\n\n# Calculate mean metallicity and its uncertainty using jackknife statistics\n# jackknife_stats returns: (mean, bias, std_err, conf_interval)\njk_mean, jk_bias, jk_stderr, jk_ci = jackknife_stats(metallicities, \n                                                     np.mean, \n                                                     confidence_level=0.95)\n\n# Print results\nprint(f\"Mean metallicity: {jk_mean:.2f}\")\nprint(f\"Standard error: {jk_stderr:.3f}\")\nprint(f\"95% confidence interval: [{jk_ci[0]:.2f}, {jk_ci[1]:.2f}]\")\n\n# Calculate the jackknife estimate for each subsample\nsubsample_means = np.array([np.mean(sample) for sample in jackknife_samples])\n\n# The spread in subsample means provides insight into measurement stability\nprint(f\"Standard deviation of subsample means: {np.std(subsample_means):.3f}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "While observing from Cerro Tololo Observatory (latitude: -30.1697\u00b0, longitude: -70.8066\u00b0, elevation: 2200m), I need to check if the galaxy NGC 1365 (RA=03h33m36.4s, Dec=-36\u00b008'25\") will be above 30 degrees altitude during my observation window at 2024-01-15 01:00 UTC. Show me how to set up an AltAz frame with refraction correction (T=12\u00b0C, P=780 hPa, RH=0.35, \u03bb=500nm) to determine this."
         },
         {
            "from": "assistant",
            "value": "from astropy.coordinates import SkyCoord, EarthLocation, AltAz\nfrom astropy.time import Time\nimport astropy.units as u\n\n# Define observatory location\ntololo = EarthLocation(\n    lat=-30.1697*u.deg, \n    lon=-70.8066*u.deg,\n    height=2200*u.m\n)\n\n# Create coordinate object for NGC 1365\nngc1365 = SkyCoord(\n    ra='03h33m36.4s',\n    dec='-36d08m25s'\n)\n\n# Set observation time\nobs_time = Time('2024-01-15 01:00:00')\n\n# Create AltAz frame with atmospheric conditions for refraction\naltaz_frame = AltAz(\n    obstime=obs_time,\n    location=tololo,\n    pressure=780*u.hPa,\n    temperature=12*u.deg_C,\n    relative_humidity=0.35,\n    obswl=500*u.nm\n)\n\n# Transform target coordinates to AltAz\ntarget_altaz = ngc1365.transform_to(altaz_frame)\n\n# Check if target is above 30 degrees\nis_observable = target_altaz.alt >= 30*u.deg\n\nprint(f\"NGC 1365 altitude: {target_altaz.alt:.2f}\")\nprint(f\"Above 30 degrees: {is_observable}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm analyzing stellar proper motions in M31 (RA=10.68458333, Dec=41.26916667 degrees) and need to create a local reference frame centered on the galaxy's core. Show me how to set up a SkyOffsetFrame for this purpose with no rotation."
         },
         {
            "from": "assistant",
            "value": "from astropy.coordinates import SkyCoord, SkyOffsetFrame\nimport astropy.units as u\n\n# Define M31 center coordinates\nm31_center = SkyCoord(ra=10.68458333*u.deg, dec=41.26916667*u.deg)\n\n# Create a SkyOffsetFrame centered on M31\n# This frame will have (lon,lat)=(0,0) at M31's center\nm31_frame = SkyOffsetFrame(origin=m31_center)\n\n# Example: Convert a nearby star's position to the M31-centered frame\nexample_star = SkyCoord(ra=10.7*u.deg, dec=41.3*u.deg)\nstar_in_m31_frame = example_star.transform_to(m31_frame)\n\n# The star's coordinates are now relative to M31's center\nprint(f\"Star position relative to M31 center:\")\nprint(f\"lon={star_in_m31_frame.lon:.4f}\")\nprint(f\"lat={star_in_m31_frame.lat:.4f}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm studying nebula images from my telescope and the current display doesn't show the fine details well. How can I enhance the visibility of both the bright central regions and the fainter outer filaments?"
         },
         {
            "from": "assistant",
            "value": "from astropy.io import fits\nfrom astropy.visualization import MinMaxInterval, SqrtStretch, ImageNormalize\nimport numpy as np\n\n# Load the FITS image (assuming it's a FITS file)\ndef enhance_nebula_image(fits_file):\n    # Open the FITS file\n    with fits.open(fits_file) as hdul:\n        # Get the image data\n        image_data = hdul[0].data\n\n        # Create normalization using square root stretch\n        # This helps enhance fainter features while keeping bright ones visible\n        norm = ImageNormalize(\n            image_data,\n            interval=MinMaxInterval(),\n            stretch=SqrtStretch()\n        )\n        \n        # Apply additional contrast enhancement\n        # Clip extreme values to reduce overwhelming bright spots\n        enhanced_data = np.clip(norm(image_data), 0.1, 0.9)\n        \n        # Scale back to full range\n        enhanced_data = (enhanced_data - enhanced_data.min()) / (enhanced_data.max() - enhanced_data.min())\n        \n        return enhanced_data\n\n# Example usage\n# enhanced_image = enhance_nebula_image('nebula.fits')\n# You can then display enhanced_image using matplotlib or your preferred plotting library"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I have a series of stellar position angles [45, 89, 123, 267, 358, 12] degrees from my galaxy morphology study. How do I calculate their circular mean using astropy?"
         },
         {
            "from": "assistant",
            "value": "from astropy.stats import circmean\nimport numpy as np\n\n# Define position angles in degrees\nangles = np.array([45, 89, 123, 267, 358, 12])\n\n# Calculate circular mean - must convert to radians for calculation\n# then convert result back to degrees\nmean_angle = circmean(angles * np.pi/180) * 180/np.pi\n\nprint(f\"Circular mean of position angles: {mean_angle:.1f} degrees\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I need to calculate the energy released by a supernova that released 3.7 foe of energy. Could you show me how to convert this to ergs using Astropy units?"
         },
         {
            "from": "assistant",
            "value": "from astropy import units as u\n\n# Define the supernova energy in foe (10^51 ergs)\n# 1 foe = 10^51 ergs, commonly used unit for supernova energies\nsupernova_energy = 3.7 * u.foe\n\n# Convert to ergs\nenergy_in_ergs = supernova_energy.to(u.erg)\n\nprint(f\"Supernova energy: {energy_in_ergs:.2e}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I have two tables containing stellar spectra observations from different nights at our observatory. How can I combine these tables vertically to create a single dataset while keeping all columns intact?"
         },
         {
            "from": "assistant",
            "value": "from astropy.table import Table, vstack\nimport numpy as np\n\n# Create example tables (replace with your actual data loading)\n# First night's observations\ntable1 = Table({\n    'wavelength': np.linspace(4000, 7000, 10),\n    'flux': np.random.normal(1, 0.1, 10),\n    'date_obs': ['2023-01-01']*10,\n    'airmass': np.random.uniform(1.0, 1.5, 10)\n})\n\n# Second night's observations\ntable2 = Table({\n    'wavelength': np.linspace(4000, 7000, 15),\n    'flux': np.random.normal(1, 0.1, 15),\n    'date_obs': ['2023-01-02']*15,\n    'airmass': np.random.uniform(1.0, 1.5, 15)\n})\n\ntry:\n    # Combine tables vertically using vstack\n    combined_table = vstack([table1, table2])\n    \n    # Sort the combined table by wavelength (optional)\n    combined_table.sort('wavelength')\n    \n    # Display basic info about the combined table\n    print(f\"Combined table has {len(combined_table)} rows\")\n    print(\"Columns:\", combined_table.colnames)\n    \nexcept TableMergeError as e:\n    print(f\"Error merging tables: {e}\")\n    print(\"Check that both tables have matching column names and data types\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm analyzing radial velocity data of stars in a globular cluster with values [-125.3, -122.1, -124.8, -180.2, -123.5, -124.1] km/s. Write code to compute the biweight midvariance with a more conservative tuning constant of 6.0 to better handle the outlier at -180.2 km/s."
         },
         {
            "from": "assistant",
            "value": "import numpy as np\nfrom astropy.stats import biweight_midvariance\n\n# Define radial velocity data in km/s\nrv_data = np.array([-125.3, -122.1, -124.8, -180.2, -123.5, -124.1])\n\n# Calculate biweight midvariance with modified tuning constant\n# Using c=6.0 for more robust outlier resistance\nbw_variance = biweight_midvariance(rv_data, c=6.0, modify_sample_size=True)\n\n# Convert variance to dispersion (standard deviation equivalent)\nrv_dispersion = np.sqrt(bw_variance)\n\nprint(f\"Biweight velocity dispersion: {rv_dispersion:.2f} km/s\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I need to verify the dimensions and data types of each extension in my FITS cube before processing it for my radio astronomy analysis. How should I check this?"
         },
         {
            "from": "assistant",
            "value": "from astropy.io import fits\n\ndef inspect_fits_cube(fits_path):\n    \"\"\"\n    Inspect dimensions and data types of each extension in a FITS cube\n    \"\"\"\n    try:\n        # Open the FITS file\n        with fits.open(fits_path) as hdul:\n            \n            # Loop through each extension\n            for i, hdu in enumerate(hdul):\n                # Get basic info about the extension\n                data_shape = hdu.data.shape if hdu.data is not None else \"No data\"\n                data_type = hdu.data.dtype if hdu.data is not None else \"N/A\"\n                hdu_type = hdu.__class__.__name__\n                \n                print(f\"\\nExtension {i}:\")\n                print(f\"Type: {hdu_type}\")\n                print(f\"Data shape: {data_shape}\")\n                print(f\"Data type: {data_type}\")\n                \n                # If it's an image or cube, show additional details\n                if isinstance(hdu, (fits.ImageHDU, fits.PrimaryHDU)) and hdu.data is not None:\n                    print(f\"Number of axes: {hdu.header['NAXIS']}\")\n                    print(f\"Memory size: {hdu.data.nbytes / 1e6:.2f} MB\")\n\n    except Exception as e:\n        print(f\"Error reading FITS file: {str(e)}\")\n\n# Example usage\ninspect_fits_cube('your_cube.fits')"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "For my exoplanet survey, I need to determine if two potential host stars are likely to be gravitationally bound. Can you show me how to calculate the angular separation between Star A (RA=23.5 degrees, Dec=45.7 degrees) and Star B (RA=23.6 degrees, Dec=45.8 degrees) using astropy's angular_separation function?"
         },
         {
            "from": "assistant",
            "value": "from astropy.coordinates import angular_separation\nimport astropy.units as u\nimport numpy as np\n\n# Define coordinates for both stars in degrees\nra1, dec1 = 23.5, 45.7  # Star A\nra2, dec2 = 23.6, 45.8  # Star B\n\n# Calculate angular separation using angular_separation function\n# Note: angular_separation expects inputs in radians, so we convert degrees to radians\nseparation = angular_separation(\n    ra1 * np.pi/180,  # Convert RA1 to radians\n    dec1 * np.pi/180, # Convert Dec1 to radians\n    ra2 * np.pi/180,  # Convert RA2 to radians\n    dec2 * np.pi/180  # Convert Dec2 to radians\n)\n\n# Convert separation from radians to arcseconds for better readability\nseparation_arcsec = separation * 180/np.pi * 3600\n\nprint(f\"Angular separation between stars: {separation_arcsec:.2f} arcseconds\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I need a code snippet to access a FITS header file from the Astropy package that contains WCS transformation data. How can I retrieve its file path for further analysis?"
         },
         {
            "from": "assistant",
            "value": "from astropy.io import fits\nfrom astropy.utils.data import get_pkg_data_filename\nfrom astropy.wcs import WCS\n\ntry:\n    # Get path to a sample FITS file with WCS data from Astropy\n    fits_file = get_pkg_data_filename('tutorials/FITS-images/HorseHead.fits')\n    \n    # Open the FITS file and access header\n    with fits.open(fits_file) as hdul:\n        # Get header from primary HDU\n        header = hdul[0].header\n        \n        # Create WCS object from header\n        wcs = WCS(header)\n        \n        # Print file path and basic WCS info\n        print(f\"FITS file path: {fits_file}\")\n        print(f\"WCS info:\\nCoordinate type: {wcs.wcs.ctype}\")\n\nexcept Exception as e:\n    print(f\"Error accessing FITS file: {str(e)}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I have observation data of 30 variable stars from two telescopes (Tables t1 and t2, each containing columns 'magnitude', 'phase', 'period'). Write code to stack these measurements depth-wise while raising an error if there are metadata conflicts."
         },
         {
            "from": "assistant",
            "value": "from astropy.table import vstack, Table\nimport numpy as np\n\ndef stack_observations(t1, t2):\n    \"\"\"\n    Stack two observation tables depth-wise after checking metadata consistency\n    Args:\n        t1, t2: Astropy Tables containing variable star measurements\n    Returns:\n        Combined Astropy Table with stacked observations\n    \"\"\"\n    try:\n        # Check if tables have same columns\n        if set(t1.colnames) != set(t2.colnames):\n            raise ValueError(\"Column names don't match between tables\")\n            \n        # Check metadata consistency for period column\n        if not np.allclose(t1['period'], t2['period']):\n            raise ValueError(\"Period values differ between observations\")\n            \n        # Stack the tables vertically \n        combined_data = vstack([t1, t2])\n        \n        # Sort by phase for convenience\n        combined_data.sort('phase')\n        \n        return combined_data\n        \n    except Exception as e:\n        print(f\"Error stacking tables: {str(e)}\")\n        raise"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I have data from lunar observations with distances measured in yards. How can I write code to enable imperial units in astropy and convert 240,000 yards to kilometers?"
         },
         {
            "from": "assistant",
            "value": "# Import required modules from astropy\nfrom astropy import units as u\n\n# Enable imperial units (including yards)\nu.imperial.enable()\n\n# Create a quantity with yards\nlunar_distance = 240000 * u.imperial.yard\n\n# Convert to kilometers\ndistance_km = lunar_distance.to(u.km)\n\n# Print the result\nprint(f\"Distance: {distance_km:.2f}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm analyzing the frequency response of a radio telescope and need to characterize its bandwidth. Can you show me how to fit a Lorentzian function to my frequency response data and extract the FWHM?"
         },
         {
            "from": "assistant",
            "value": "import numpy as np\nfrom astropy.modeling import models, fitting\nimport astropy.units as u\n\n# Create sample frequency response data (replace with your actual data)\nfreq = np.linspace(1.0, 2.0, 100) * u.GHz  # Frequency range\n# Simulate noisy Lorentzian response\namp = 1.0\ncenter = 1.5\nfwhm = 0.2\nresponse = (amp * (fwhm/2)**2 / ((freq.value - center)**2 + (fwhm/2)**2))\nresponse += np.random.normal(0, 0.01, len(freq))  # Add some noise\n\n# Define and fit Lorentzian model\nlorentz_init = models.Lorentz1D(amplitude=amp, x_0=center, fwhm=fwhm)\nfitter = fitting.LevMarLSQFitter()\nlorentz_fit = fitter(lorentz_init, freq.value, response)\n\n# Extract FWHM from fitted model\nfitted_fwhm = lorentz_fit.fwhm.value  # Access the value attribute of Parameter\nprint(f\"Fitted FWHM: {fitted_fwhm:.3f} GHz\")\n\n# Calculate quality factor (Q)\nQ = lorentz_fit.x_0.value / fitted_fwhm  # Access the value attribute of Parameter\nprint(f\"Quality factor (Q): {Q:.1f}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm working on mapping the distribution of planetary nebulae in the Milky Way. What's the code to convert their galactic coordinates and distances into x, y, z positions relative to the galactic center?"
         },
         {
            "from": "assistant",
            "value": "from astropy.coordinates import SkyCoord\nimport astropy.units as u\nimport numpy as np\n\ndef gal_to_xyz(l, b, distance):\n    \"\"\"\n    Convert galactic coordinates to cartesian coordinates centered on galactic center\n    \n    Parameters:\n    l : Galactic longitude in degrees\n    b : Galactic latitude in degrees\n    distance : Distance from Sun in kpc\n    \n    Returns:\n    x, y, z : Cartesian coordinates in kpc relative to galactic center\n    \"\"\"\n    # Create SkyCoord object with galactic coordinates and distance\n    coords = SkyCoord(l=l*u.degree, b=b*u.degree, distance=distance*u.kpc,\n                      frame='galactic')\n    \n    # Convert to Cartesian coordinates\n    # x is toward the galactic center\n    # y is toward l = 90\u00b0\n    # z is toward the north galactic pole\n    x = coords.cartesian.x.value - 8.122  # Subtract Sun's distance from galactic center\n    y = coords.cartesian.y.value\n    z = coords.cartesian.z.value\n    \n    return x, y, z\n\n# Example usage\nl, b = 45.0, 30.0  # Galactic coordinates in degrees\ndistance = 2.5     # Distance in kpc\n\nx, y, z = gal_to_xyz(l, b, distance)\nprint(f\"Cartesian coordinates (x, y, z) in kpc: ({x:.2f}, {y:.2f}, {z:.2f})\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I have a 2D array of stellar flux measurements from multiple observations of the same field, with some observations affected by poor seeing conditions. How can I identify and mask these problematic measurements while preserving the good data?"
         },
         {
            "from": "assistant",
            "value": "import numpy as np\nfrom astropy.stats import sigma_clip\nfrom astropy.nddata import CCDData\n\ndef mask_poor_seeing(flux_array, sigma=3):\n    \"\"\"\n    Identifies and masks flux measurements affected by poor seeing conditions\n    using sigma-clipping along the time axis.\n    \n    Parameters:\n        flux_array: 2D numpy array (time x stars)\n        sigma: Number of standard deviations for sigma clipping\n    Returns:\n        masked_data: CCDData object with masked problematic measurements\n    \"\"\"\n    \n    # Convert to CCDData object for better handling of masks\n    ccd_data = CCDData(flux_array, unit='adu')\n    \n    # Calculate median flux for each star\n    median_flux = np.median(flux_array, axis=0)\n    \n    # Normalize each star's measurements by its median\n    normalized_flux = flux_array / median_flux\n    \n    # Apply sigma clipping to identify outliers\n    # These are likely affected by poor seeing\n    masked_data = sigma_clip(normalized_flux, sigma=sigma, axis=0)\n    \n    # Create final masked array\n    ccd_data.mask = masked_data.mask\n    \n    return ccd_data\n\n# Example usage:\n# flux_data = np.array([[100, 200, 300], [90, 180, 290], [50, 100, 150]])\n# masked_result = mask_poor_seeing(flux_data)"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I need a code snippet to enable unit equivalencies between 30 degrees and radians for angle calculations in my telescope pointing system. How can I do this with astropy?"
         },
         {
            "from": "assistant",
            "value": "from astropy import units as u\n\n# Define angle in degrees\nangle_deg = 30 * u.deg\n\n# Convert to radians using the equivalency\nangle_rad = angle_deg.to(u.radian)\n\n# Print both representations\nprint(f\"Angle in degrees: {angle_deg}\")\nprint(f\"Angle in radians: {angle_rad}\")\n\n# Verify conversion back to degrees\nangle_back_to_deg = angle_rad.to(u.deg)\nprint(f\"Converted back to degrees: {angle_back_to_deg}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I have spectroscopic data of a galaxy with redshift measurements [0.12, 0.13, 0.11, 0.14, 0.12] and uncertainties [0.01, 0.01, 0.02, 0.01, 0.01]. How can I fit a Gaussian model using LevMarLSQFitter to determine the peak and width of the redshift distribution?"
         },
         {
            "from": "assistant",
            "value": "from astropy.modeling import models, fitting\nimport numpy as np\n\n# Create sample data\nredshifts = np.array([0.12, 0.13, 0.11, 0.14, 0.12])\nuncertainties = np.array([0.01, 0.01, 0.02, 0.01, 0.01])\n\n# Create histogram data for fitting\nhist, bin_edges = np.histogram(redshifts, bins=10, range=(0.08, 0.16))\nbin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n\n# Initialize Gaussian model\n# Estimate initial parameters: amplitude, mean, standard deviation\ng_init = models.Gaussian1D(\n    amplitude=np.max(hist),\n    mean=np.mean(redshifts),\n    stddev=np.std(redshifts)\n)\n\n# Initialize fitter\nfitter = fitting.LevMarLSQFitter()\n\n# Fit the model to the data\ng_fit = fitter(g_init, bin_centers, hist)\n\n# Print results\nprint(f\"Peak redshift: {g_fit.mean.value:.3f}\")\nprint(f\"Distribution width (stddev): {g_fit.stddev.value:.3f}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm working with multi-extension FITS files and need to modify the 'FILTER' keyword to 'K-band' with comment 'IR filter used' in extension 2. Can you demonstrate how to use setval for this specific extension?"
         },
         {
            "from": "assistant",
            "value": "from astropy.io import fits\nimport os\n\ndef update_fits_filter(fits_file, filter_value='K-band', extension=1):\n    try:\n        # Check if file exists\n        if not os.path.exists(fits_file):\n            raise FileNotFoundError(f\"The file {fits_file} does not exist\")\n            \n        # Open the FITS file in 'update' mode to modify it\n        with fits.open(fits_file, mode='update') as hdul:\n            # Use setval to modify the FILTER keyword in specified extension\n            fits.setval(fits_file, \n                       'FILTER',\n                       ext=extension,  # Extension number (0-based indexing)\n                       value=filter_value,\n                       comment='IR filter used')\n            \n            # Ensure changes are written to file\n            hdul.flush()\n            print(f\"Successfully updated FILTER keyword to {filter_value} in extension {extension}\")\n            \n    except FileNotFoundError as e:\n        print(f\"Error: {e}\")\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n\n# Example usage\nfits_file = '/path/to/your/actual/file.fits'  # Replace with actual file path\nupdate_fits_filter(fits_file)"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm studying a star with radial velocity measurements. How do I create an ICRS coordinate object with RA = 45.0 degrees, Dec = 30.0 degrees, distance = 100 parsecs, and radial_velocity = 25 km/s?"
         },
         {
            "from": "assistant",
            "value": "from astropy.coordinates import ICRS\nfrom astropy import units as u\n\n# Create ICRS coordinate object with position, distance and radial velocity\nstar_coord = ICRS(\n    ra=45.0 * u.deg,          # Right Ascension in degrees\n    dec=30.0 * u.deg,         # Declination in degrees\n    distance=100 * u.pc,      # Distance in parsecs\n    radial_velocity=25 * u.km/u.s  # Radial velocity in km/s\n)\n\n# Print the resulting coordinate object\nprint(star_coord)"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "For analyzing planetary transit light curves, I need to create a custom kernel that can help remove systematic trends while preserving the transit signal. How can I implement this using Astro-py?"
         },
         {
            "from": "assistant",
            "value": "from astropy.modeling import models, fitting\nfrom astropy.stats import sigma_clip\nimport numpy as np\n\ndef create_transit_preserving_kernel(time, flux, period, transit_duration):\n    \"\"\"\n    Creates a custom kernel for detrending light curves while preserving transit signals\n    \n    Parameters:\n    -----------\n    time : array-like\n        Time array of observations\n    flux : array-like\n        Flux measurements\n    period : float\n        Known orbital period of the transit\n    transit_duration : float\n        Duration of the transit in same units as time\n    \n    Returns:\n    --------\n    cleaned_flux : array-like\n        Detrended flux array with preserved transit signal\n    \"\"\"\n    # Create mask for transit points to exclude from trend fitting\n    phase = (time % period) / period\n    transit_half_width = transit_duration / (2 * period)\n    transit_mask = (phase < transit_half_width) | (phase > (1 - transit_half_width))\n    \n    # Create trend model (combination of polynomial and sinusoidal)\n    trend_model = models.Polynomial1D(degree=2) + \\\n                 models.Sine1D(amplitude=0.01, frequency=1.0/period)\n    \n    # Fit model to out-of-transit points\n    fitter = fitting.LevMarLSQFitter()\n    out_of_transit_flux = flux[~transit_mask]\n    out_of_transit_time = time[~transit_mask]\n    \n    # Sigma clip to remove outliers\n    clipped_flux = sigma_clip(out_of_transit_flux, sigma=3)\n    good_points = ~clipped_flux.mask\n    \n    # Fit the model\n    fitted_model = fitter(trend_model, \n                         out_of_transit_time[good_points],\n                         out_of_transit_flux[good_points])\n    \n    # Remove trend from entire dataset\n    trend = fitted_model(time)\n    cleaned_flux = flux / trend\n    \n    return cleaned_flux\n\n# Example usage:\n\"\"\"\ntime = np.linspace(0, 10, 1000)  # 10 days of observations\nperiod = 3.0                      # 3-day orbital period\ntransit_duration = 0.1            # transit duration in days\nflux = your_flux_data            # your actual flux measurements\ndetrended_flux = create_transit_preserving_kernel(time, flux, period, transit_duration)\n\"\"\""
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm studying an eclipsing binary system and want to determine how my total observation time is distributed across different parts of the orbital period. Can you show me how to fold multiple observing sessions into a phase representation?"
         },
         {
            "from": "assistant",
            "value": "from astropy.time import Time\nimport numpy as np\n\n# Sample observation times (replace with your actual data)\nobs_times = Time(['2023-01-01T00:00:00', '2023-01-01T06:00:00',\n                  '2023-01-02T00:00:00', '2023-01-03T12:00:00',\n                  '2023-01-04T18:00:00', '2023-01-05T06:00:00'])\n\n# Binary system parameters\norbital_period = 2.4  # days\nt0 = Time('2023-01-01T00:00:00')  # reference time for phase 0.0\n\n# Convert times to numerical values in days\ntime_days = (obs_times - t0).jd\n\n# Calculate phases\nphases = np.mod(time_days/orbital_period, 1.0)\n\n# Sort phases for plotting if needed\nsorted_indices = np.argsort(phases)\nphases_sorted = phases[sorted_indices]\n\n# Print phase for each observation\nfor t, p in zip(obs_times, phases):\n    print(f\"Time: {t.iso}, Phase: {p:.3f}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I have measurements of H-alpha emission line widths from 300 active galactic nuclei, ranging from 1000 to 5000 km/s. How do I create a histogram with Knuth's rule binning, setting a maximum of 50 bins and adding axis labels?"
         },
         {
            "from": "assistant",
            "value": "import numpy as np\nfrom astropy.stats import knuth_bin_width\nfrom matplotlib import pyplot as plt\nimport astropy.units as u\n\n# Generate sample H-alpha line width data (replace with your actual data)\nhalpha_widths = np.random.uniform(1000, 5000, 300) * u.km/u.s\n\n# Calculate optimal bin width using Knuth's rule\n# Convert to value without units for calculation\nbin_width = knuth_bin_width(halpha_widths.value)\nn_bins = int(np.ceil((halpha_widths.value.max() - halpha_widths.value.min()) / bin_width))\n\n# Create histogram\nplt.figure(figsize=(10, 6))\nplt.hist(halpha_widths.value, bins=min(n_bins, 50), edgecolor='black')\n\n# Add labels and title\nplt.xlabel('H-\u03b1 Line Width (km/s)')\nplt.ylabel('Number of AGN')\nplt.title('Distribution of H-\u03b1 Line Widths in AGN Sample')\n\n# Display the plot\nplt.tight_layout()\nplt.show()"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "For my planetary dynamics research, I need to track Mercury's barycentric motion during its next transit across the Sun. How can I compute its position and velocity vectors?"
         },
         {
            "from": "assistant",
            "value": "from astropy.time import Time\nfrom astropy.coordinates import get_body_barycentric, get_body_barycentric_posvel\nimport numpy as np\nfrom astropy import units as u  # Add this import for astropy units\n\n# Set time range around Mercury's next transit\n# Using 2024 transit as example\nt_start = Time('2024-05-15 00:00')  # Transit date\ntimes = t_start + np.linspace(0, 1, 24) * u.day  # 24 hours sampling\n\n# Get Mercury's position and velocity vectors relative to Solar System Barycenter\nmercury_posvel = get_body_barycentric_posvel('mercury', times)\npositions = mercury_posvel[0]  # CartesianRepresentation object\nvelocities = mercury_posvel[1]  # CartesianDifferential object\n\n# Extract x, y, z components (in AU)\nx = positions.x.value\ny = positions.y.value\nz = positions.z.value\n\n# Extract velocity components (in AU/day)\nvx = velocities.x.value\nvy = velocities.y.value\nvz = velocities.z.value\n\n# Print position and velocity at transit start\nprint(f\"Mercury's position (AU): {x[0]:.6f}, {y[0]:.6f}, {z[0]:.6f}\")\nprint(f\"Mercury's velocity (AU/day): {vx[0]:.6f}, {vy[0]:.6f}, {vz[0]:.6f}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "As part of my exoplanet transit analysis, I need to estimate the scatter in my light curve while properly handling outliers from stellar flares. Can you show me how to compute a robust variance estimate using Astro-py?"
         },
         {
            "from": "assistant",
            "value": "from astropy.stats import mad_std\nimport numpy as np\n\ndef compute_robust_variance(flux_data):\n    \"\"\"\n    Compute robust variance estimate of light curve data using MAD\n    \n    Parameters:\n    flux_data : array-like\n        Array of flux measurements from light curve\n        \n    Returns:\n    float : Robust variance estimate\n    \"\"\"\n    try:\n        # Calculate robust standard deviation using MAD\n        # mad_std is resistant to outliers like stellar flares\n        robust_std = mad_std(flux_data, ignore_nan=True)\n        \n        # Square to get variance\n        robust_variance = robust_std**2\n        \n        return robust_variance\n        \n    except Exception as e:\n        print(f\"Error computing robust variance: {e}\")\n        return None\n\n# Example usage\nif __name__ == \"__main__\":\n    # Sample light curve data with outliers (stellar flares)\n    sample_flux = np.array([1.001, 0.999, 1.002, 0.998, 1.5, 1.001, 0.997, 2.0])\n    \n    variance = compute_robust_variance(sample_flux)\n    print(f\"Robust variance estimate: {variance:.6f}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "What's the correct way to create a FITS header card that includes complex observation parameters with proper keyword formatting and value validation?"
         },
         {
            "from": "assistant",
            "value": "from astropy.io import fits\nfrom astropy.io.fits.card import Card\nfrom astropy.io.fits.header import Header\n\n# Create a new FITS header\nhdr = Header()\n\n# Add basic observation parameters with proper formatting\nhdr['TELESCOP'] = ('VLT', 'Telescope name')\nhdr['INSTRUME'] = ('FORS2', 'Instrument used')\nhdr['OBSERVER'] = ('Smith, J.', 'Observer name')\n\n# Add numerical values with units and precision\nhdr.append(Card('EXPTIME', 1200.0, 'Exposure time [s]'))\nhdr.append(Card('AIRMASS', 1.2, 'Airmass during observation'))\n\n# Add coordinate information with proper formatting\nhdr.append(Card('RA', '14:29:43.123', 'Right Ascension [HH:MM:SS]'))\nhdr.append(Card('DEC', '-62:30:45.32', 'Declination [DD:MM:SS]'))\n\n# Add hierarchical keywords for complex parameters\nhdr.append(Card('HIERARCH ESO DET CHIP GAIN', 0.8, 'Detector gain [e-/ADU]'))\nhdr.append(Card('HIERARCH ESO TEL AMBI TEMP', 12.5, 'Ambient temperature [C]'))\n\n# Create a primary HDU with the header\nhdu = fits.PrimaryHDU(header=hdr)\n\n# Validate header using the HDU's verify method\ntry:\n    # Verify header content and formatting\n    hdu.verify('fix')\n    print(\"Header validation successful\")\nexcept fits.VerifyError as e:\n    print(f\"Header validation failed: {str(e)}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I need to apply a uniform scaling factor to my observed magnitude data to account for atmospheric extinction. What's the best way to do this with Astropy's modeling tools?"
         },
         {
            "from": "assistant",
            "value": "from astropy.modeling import models, fitting\nimport numpy as np\n\n# Create sample magnitude data (replace with your actual data)\nobserved_mags = np.array([15.2, 15.7, 16.1, 16.4, 16.8])\nairmass = np.array([1.1, 1.3, 1.5, 1.7, 1.9])  # Corresponding airmass values\n\n# Define extinction model: m = m0 + k*X\n# where m is observed magnitude, m0 is true magnitude, k is extinction coefficient, X is airmass\nextinction_model = models.Linear1D(slope=0.2, intercept=15.0)  # Initial guess\nfitter = fitting.LevMarLSQFitter()\n\n# Fit the model to get extinction coefficient\nfitted_model = fitter(extinction_model, airmass, observed_mags)\n\n# Calculate corrected magnitudes\ncorrected_mags = observed_mags - fitted_model.slope.value * airmass\n\n# The slope represents the extinction coefficient (magnitudes/airmass)\nprint(f\"Extinction coefficient: {fitted_model.slope.value:.3f} mag/airmass\")\nprint(f\"True magnitude (at airmass=0): {fitted_model.intercept.value:.3f}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "As a stellar spectroscopist, I have metallicity measurements [-0.5, -0.3, -0.2, -0.8, -0.4] for a cluster of stars. Write code to create a MaskedColumn called 'Fe_H' with units of 'dex', where values below -0.6 should be masked out. Use a fill value of -999."
         },
         {
            "from": "assistant",
            "value": "from astropy.table import MaskedColumn\nimport numpy as np\n\n# Create metallicity data\nmetallicity = [-0.5, -0.3, -0.2, -0.8, -0.4]\n\n# Create MaskedColumn with units and mask values below -0.6\nFe_H = MaskedColumn(\n    data=metallicity,\n    name='Fe_H',\n    unit='dex',\n    mask=[val < -0.6 for val in metallicity],\n    fill_value=-999\n)\n\n# Print the column to verify\nprint(Fe_H)"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I have a temperature map of a galaxy cluster stored in a 100x100 numpy array with values in Kelvin. How do I create an ImageHDU for this data and add appropriate BUNIT and OBJECT keywords in the header?"
         },
         {
            "from": "assistant",
            "value": "import numpy as np\nfrom astropy.io import fits\n\n# Create sample temperature map data (100x100 array)\ntemp_data = np.random.normal(1e7, 1e6, (100,100))  # Example temperature data\n\n# Create ImageHDU with the temperature data\nhdu = fits.ImageHDU(data=temp_data)\n\n# Add relevant keywords to the header\nhdu.header['BUNIT'] = 'K'           # Physical unit (Kelvin)\nhdu.header['OBJECT'] = 'Galaxy Cluster'  # Object name\nhdu.header['COMMENT'] = 'Temperature map of galaxy cluster'\n\n# Optional: Create primary HDU and combine into HDUList if needed\nhdul = fits.HDUList([fits.PrimaryHDU(), hdu])\n\n# Write to file (optional)\n# hdul.writeto('cluster_temp_map.fits', overwrite=True)"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "For my exoplanet research, I need to determine how far apart (in arcminutes) two potential host stars appear in the sky. Star 1 is at (ra=150.23\u00b0, dec=+32.45\u00b0) and Star 2 is at (ra=150.28\u00b0, dec=+32.52\u00b0). Please show me the code using angular_separation to calculate this."
         },
         {
            "from": "assistant",
            "value": "from astropy.coordinates import SkyCoord\nimport astropy.units as u\n\n# Create SkyCoord objects for both stars using their RA and Dec\nstar1 = SkyCoord(ra=150.23*u.deg, dec=32.45*u.deg)\nstar2 = SkyCoord(ra=150.28*u.deg, dec=32.52*u.deg)\n\n# Calculate angular separation between the stars\nseparation = star1.separation(star2)\n\n# Convert to arcminutes\nseparation_arcmin = separation.to(u.arcmin)\n\nprint(f\"Angular separation between stars: {separation_arcmin:.2f}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm analyzing radial velocity measurements of stars in a globular cluster and want to test if they follow a Gaussian distribution. How can I quantify this statistically?"
         },
         {
            "from": "assistant",
            "value": "from astropy.stats import sigma_clip\nfrom scipy import stats\nimport numpy as np\n\ndef test_velocity_distribution(radial_velocities):\n    \"\"\"\n    Test if radial velocities follow a Gaussian distribution using:\n    1. Sigma clipping to remove outliers\n    2. Shapiro-Wilk test for normality\n    3. Anderson-Darling test as additional verification\n    \"\"\"\n    # Remove outliers using sigma clipping\n    filtered_velocities = sigma_clip(radial_velocities, sigma=3, maxiters=5)\n    clean_velocities = filtered_velocities.compressed()  # Remove masked values\n    \n    # Perform Shapiro-Wilk test (better for n < 5000)\n    shapiro_stat, shapiro_p = stats.shapiro(clean_velocities)\n    \n    # Perform Anderson-Darling test\n    anderson_result = stats.anderson(clean_velocities, dist='norm')\n    \n    # Print results\n    print(f\"Shapiro-Wilk test p-value: {shapiro_p:.4f}\")\n    print(f\"Anderson-Darling statistic: {anderson_result.statistic:.4f}\")\n    \n    # Return True if distribution is likely Gaussian (using Shapiro test)\n    return shapiro_p > 0.05\n\n# Example usage:\n# velocities = np.array([...])  # Your radial velocity measurements in km/s\n# is_gaussian = test_velocity_distribution(velocities)"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm modeling the light curve of an eclipsing binary and need to ensure the period parameter stays positive throughout the fitting. How can I implement this constraint?"
         },
         {
            "from": "assistant",
            "value": "# Import required modules\nfrom astropy.modeling import Parameter\nfrom astropy.modeling import Fittable1DModel\nfrom astropy import units as u\n\nclass EclipsingBinary(Fittable1DModel):\n    \"\"\"Simple eclipsing binary light curve model with constrained positive period\"\"\"\n    \n    # Define parameters with bounds\n    amplitude = Parameter()\n    period = Parameter(bounds=(0, None))  # Period must be positive\n    t0 = Parameter()  # Time of primary eclipse\n    \n    @staticmethod\n    def evaluate(time, amplitude, period, t0):\n        \"\"\"\n        Evaluate the model at given times\n        \n        Parameters\n        ----------\n        time : array-like\n            Time points to evaluate model\n        amplitude : float\n            Eclipse depth\n        period : float\n            Orbital period (constrained positive)\n        t0 : float\n            Time of primary eclipse\n        \"\"\"\n        # Calculate phase (folded time)\n        phase = ((time - t0) % period) / period\n        \n        # Simple sinusoidal approximation for demonstration\n        return amplitude * (1 - 0.5 * (1 + np.cos(2 * np.pi * phase)))\n\n# Example usage\nif __name__ == \"__main__\":\n    import numpy as np\n    \n    # Create sample data\n    time = np.linspace(0, 10, 1000)\n    model = EclipsingBinary(amplitude=1.0, period=2.5, t0=0.0)\n    light_curve = model(time)"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "For my white dwarf research, I need to find the peak wavelength of a blackbody spectrum for a star with T=15000K. How can I use BlackBody model along with astropy's units to calculate the wavelength of maximum emission?"
         },
         {
            "from": "assistant",
            "value": "from astropy.modeling.models import BlackBody  # Updated import\nfrom astropy import units as u\nimport numpy as np\n\n# Define temperature\nT = 15000 * u.K\n\n# Use Wien's displacement law: \u03bb_max = b/T\n# where b is Wien's displacement constant\nb = 2.898e-3 * u.m * u.K  # Wien's displacement constant\n\n# Calculate peak wavelength\npeak_wavelength = b / T\n\n# Convert to more convenient units (nanometers)\npeak_wavelength_nm = peak_wavelength.to(u.nm)\n\nprint(f\"Peak wavelength for T={T}: {peak_wavelength_nm:.1f}\")\n\n# Verify using BlackBody model by finding maximum emission\n# Create wavelength array around expected peak\nwavelengths = np.linspace(150, 250, 1000) * u.nm\n\n# Create BlackBody model and evaluate\nbb = BlackBody(temperature=T)\nintensity = bb(wavelengths)\n\nmax_intensity_wavelength = wavelengths[np.argmax(intensity)]\n\nprint(f\"Peak wavelength (from intensity curve): {max_intensity_wavelength:.1f}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "For my stellar metallicity study, I need to compare the metal content distribution of two different stellar populations. How can I create a histogram using Scott's rule for bin width determination to effectively visualize this comparison?"
         },
         {
            "from": "assistant",
            "value": "import numpy as np\nfrom astropy.visualization import hist\nimport matplotlib.pyplot as plt\n\n# Sample metallicity data for two stellar populations\n# Normally this would come from your observations/calculations\npop1_metallicity = np.random.normal(-0.5, 0.3, 1000)  # Population 1 ([Fe/H])\npop2_metallicity = np.random.normal(-0.2, 0.2, 800)   # Population 2 ([Fe/H])\n\n# Create figure and axis\nplt.figure(figsize=(10, 6))\n\n# Create histogram using Astropy's histogram function with Scott's rule\n# Scott's rule: bin_width = 3.5 * std(x) / (len(x))**(1/3)\nhist(pop1_metallicity, bins='scott', alpha=0.5, label='Population 1',\n     density=True, histtype='stepfilled', color='blue')\nhist(pop2_metallicity, bins='scott', alpha=0.5, label='Population 2',\n     density=True, histtype='stepfilled', color='red')\n\n# Customize plot\nplt.xlabel('[Fe/H]')\nplt.ylabel('Normalized Frequency')\nplt.title('Metallicity Distribution of Two Stellar Populations')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.show()"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm analyzing spectral data and need to scale my continuum model by a factor of 2.5 to match the observed flux levels. Show me how to use Multiply with my existing Gaussian1D model for this adjustment."
         },
         {
            "from": "assistant",
            "value": "import numpy as np\nfrom astropy.modeling import models, Model\nfrom astropy.modeling.models import Multiply, Gaussian1D\n\n# Create base Gaussian model\ngaussian = Gaussian1D(amplitude=1.0, mean=5000, stddev=100)\n\n# Create scaling factor model\nscale = 2.5\n\n# Combine models using Multiply\nscaled_model = gaussian | Multiply(scale)\n\n# Example of using the scaled model\nx = np.linspace(4800, 5200, 100)  # wavelength array\nflux = scaled_model(x)  # Get scaled flux values\n\n# The result will be the Gaussian profile multiplied by 2.5"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "Could you show me how to create a primary HDU object containing my radio telescope intensity map data with the correct physical units and observation date in the header?"
         },
         {
            "from": "assistant",
            "value": "from astropy.io import fits\nfrom astropy.time import Time\nimport numpy as np\nfrom astropy import units as u\n\n# Create sample radio intensity data (replace with your actual data)\nintensity_data = np.random.random((100, 100)) * u.Jy/u.beam\n\n# Get current time for observation date\nobs_time = Time.now()\n\n# Create primary HDU\nprimary_hdu = fits.PrimaryHDU(data=intensity_data.value)  # Store numerical values only\n\n# Add essential header information\nprimary_hdu.header['BUNIT'] = 'Jy/beam'  # Physical unit of the data\nprimary_hdu.header['DATE-OBS'] = obs_time.iso  # Observation date in ISO format\nprimary_hdu.header['TELESCOP'] = 'My Radio Telescope'  # Telescope name\nprimary_hdu.header['OBJECT'] = 'Target Source'  # Observed object name\nprimary_hdu.header['INSTRUME'] = 'Radio Receiver'  # Instrument used\nprimary_hdu.header['OBSERVER'] = 'John Doe'  # Observer name\n\n# Optional: Add coordinate system information\nprimary_hdu.header['CTYPE1'] = 'RA---SIN'  # Coordinate type for axis 1\nprimary_hdu.header['CTYPE2'] = 'DEC--SIN'  # Coordinate type for axis 2"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "For my galaxy fitting model, I need to ensure the ellipticity parameter stays between 0 and 1, and should be fixed during the initial fitting process. How can I define this parameter?"
         },
         {
            "from": "assistant",
            "value": "from astropy.modeling import Fittable2DModel, Parameter\nfrom astropy.modeling.fitting import LevMarLSQFitter\n\nclass GalaxyModel(Fittable2DModel):\n    \"\"\"Custom 2D galaxy model with constrained ellipticity\"\"\"\n    \n    # Define ellipticity as a fixed parameter bounded between 0 and 1\n    ellipticity = Parameter(default=0.5, fixed=True, bounds=(0, 1))\n    \n    # Other parameters (example)\n    amplitude = Parameter(default=1.0)\n    x_0 = Parameter(default=0.0)\n    y_0 = Parameter(default=0.0)\n    \n    @staticmethod\n    def evaluate(x, y, ellipticity, amplitude, x_0, y_0):\n        \"\"\"Evaluate the model (simplified example)\"\"\"\n        # Basic circular gaussian modified by ellipticity\n        r_sq = (x - x_0)**2 + ((y - y_0)**2 / (1 - ellipticity))\n        return amplitude * np.exp(-0.5 * r_sq)\n\n# Create model instance\nmodel = GalaxyModel()\n\n# Verify constraints\nprint(f\"Ellipticity bounds: {model.ellipticity.bounds}\")\nprint(f\"Ellipticity fixed: {model.ellipticity.fixed}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "How can I use asinh stretch to better visualize the intensity variations in my galaxy image that contains both bright cores and faint extended features?"
         },
         {
            "from": "assistant",
            "value": "from astropy.visualization import AsinhStretch, ImageNormalize\nimport numpy as np\nfrom astropy.io import fits\nimport matplotlib.pyplot as plt\n\n# Read the galaxy image (assuming it's a FITS file)\ntry:\n    image_data = fits.getdata('galaxy.fits')\n    \n    # Create normalization object with asinh stretch\n    # 'a' parameter controls how linear the stretch is around zero\n    stretch = AsinhStretch(a=0.1)\n    norm = ImageNormalize(image_data, \n                         stretch=stretch,\n                         # Set min/max to exclude extreme outliers\n                         vmin=np.percentile(image_data, 1),\n                         vmax=np.percentile(image_data, 99))\n    \n    # Display the image\n    plt.figure(figsize=(10, 8))\n    plt.imshow(image_data, norm=norm, cmap='viridis')\n    plt.colorbar(label='Intensity')\n    plt.title('Galaxy Image with Asinh Stretch')\n    plt.show()\n\nexcept FileNotFoundError:\n    print(\"Error: Galaxy image file not found\")\nexcept Exception as e:\n    print(f\"An error occurred: {e}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "For my variable star research, I want to create a table containing the observation time, brightness measurements, and measurement uncertainties for multiple stars, then add columns for period analysis results. How can I set this up?"
         },
         {
            "from": "assistant",
            "value": "from astropy.table import Table, Column\nimport numpy as np\nfrom astropy.time import Time\n\n# Create sample data for demonstration\nn_measurements = 100\nn_stars = 3\n\n# Generate example observation times (as Julian Dates)\ntimes = Time('2023-01-01') + np.linspace(0, 30, n_measurements)\njd_times = times.jd\n\n# Create mock brightness and uncertainty data\nbrightness = {f'star_{i}': 12 + np.random.normal(0, 0.1, n_measurements) \n             for i in range(n_stars)}\nerrors = {f'star_{i}': np.abs(np.random.normal(0.02, 0.005, n_measurements)) \n         for i in range(n_stars)}\n\n# Initialize the table with time column\nvar_star_table = Table()\nvar_star_table['JD'] = Column(jd_times, description='Julian Date')\n\n# Add brightness and error columns for each star\nfor i in range(n_stars):\n    star_name = f'star_{i}'\n    var_star_table[f'{star_name}_mag'] = Column(brightness[star_name], \n                                               description=f'Magnitude of {star_name}')\n    var_star_table[f'{star_name}_err'] = Column(errors[star_name], \n                                               description=f'Magnitude error of {star_name}')\n\n# Add empty columns for period analysis results\nvar_star_table['period'] = Column(np.zeros(n_measurements), \n                                 description='Period in days')\nvar_star_table['amplitude'] = Column(np.zeros(n_measurements), \n                                   description='Peak-to-peak amplitude')\nvar_star_table['phase'] = Column(np.zeros(n_measurements), \n                                description='Phase from reference epoch')\n\n# Print first few rows of the table\nprint(var_star_table[:3])"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I have a series of spectral datacubes (each slice is 400x400) showing nebular emission, with intensity values from 1 to 1000. How can I apply ZScaleInterval with custom settings (contrast=0.15, n_samples=1500, min_npixels=10) to optimize the visualization of faint structures?"
         },
         {
            "from": "assistant",
            "value": "import numpy as np\nfrom astropy.visualization import ZScaleInterval\nimport matplotlib.pyplot as plt\n\ndef optimize_cube_visualization(datacube):\n    \"\"\"\n    Applies ZScale normalization to a spectral datacube to enhance faint structures\n    Parameters:\n        datacube: 3D numpy array (wavelength, y, x)\n    Returns:\n        normalized_cube: ZScale normalized version of input cube\n    \"\"\"\n    # Initialize ZScale with custom parameters\n    zscale = ZScaleInterval(contrast=0.15, n_samples=1500, min_npixels=10)\n    \n    # Get cube dimensions\n    nz, ny, nx = datacube.shape\n    normalized_cube = np.zeros_like(datacube, dtype=float)\n    \n    # Apply ZScale to each wavelength slice\n    for z in range(nz):\n        # Get min/max values for this slice using ZScale\n        vmin, vmax = zscale.get_limits(datacube[z])\n        \n        # Normalize slice\n        slice_norm = np.clip((datacube[z] - vmin) / (vmax - vmin), 0, 1)\n        normalized_cube[z] = slice_norm\n\n    return normalized_cube\n\n# Example usage\ndef plot_example(datacube, slice_idx=0):\n    \"\"\"Plot original vs normalized version of one slice\"\"\"\n    normalized = optimize_cube_visualization(datacube)\n    \n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n    \n    ax1.imshow(datacube[slice_idx], origin='lower')\n    ax1.set_title('Original')\n    \n    ax2.imshow(normalized[slice_idx], origin='lower')\n    ax2.set_title('ZScale normalized')\n    \n    plt.tight_layout()\n    plt.show()\n\n# Example with dummy data\ndummy_cube = np.random.uniform(1, 1000, size=(10, 400, 400))\nplot_example(dummy_cube)"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "My quasar luminosity data spans 6 orders of magnitude (10^38 to 10^44 erg/s). How do I implement PowerStretch with a=1.5 along with astropy's MinMaxInterval to create a more balanced visualization?"
         },
         {
            "from": "assistant",
            "value": "from astropy.visualization import PowerStretch, MinMaxInterval\nimport numpy as np\n\n# Create sample quasar luminosity data spanning 6 orders of magnitude\nluminosity = np.logspace(38, 44, 1000)  # Sample data between 10^38 to 10^44 erg/s\n\n# Create normalization using MinMaxInterval and PowerStretch\nnorm = MinMaxInterval() + PowerStretch(a=1.5)\n\n# Apply the normalization to scale the data to [0,1] range\nnormalized_data = norm(luminosity)\n\n# The normalized_data can now be used with plotting libraries\n# Example with matplotlib (if needed):\nimport matplotlib.pyplot as plt\nplt.plot(luminosity, normalized_data, '.')\nplt.xscale('log')\nplt.xlabel('Luminosity (erg/s)')\nplt.ylabel('Normalized Values')\nplt.show()"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "For my exoplanet transit observations, I need to track several reference stars relative to the target star. Can you show me how to create a coordinate system where the target star is at the origin?"
         },
         {
            "from": "assistant",
            "value": "from astropy.coordinates import SkyCoord\nimport astropy.units as u\nimport numpy as np  # Add numpy import\n\n# Define the target star coordinates (example coordinates used)\ntarget_star = SkyCoord(ra='14h15m39.6s', dec='+19d10m56.6s')\n\n# Define reference stars (example coordinates)\nref_stars = [\n    SkyCoord(ra='14h15m40.2s', dec='+19d11m00.0s'),\n    SkyCoord(ra='14h15m38.8s', dec='+19d10m45.3s'),\n    SkyCoord(ra='14h15m41.1s', dec='+19d10m52.2s')\n]\n\n# Create a relative coordinate system centered on target star\nrelative_positions = []\nfor ref_star in ref_stars:\n    # Calculate separation and position angle from target\n    sep = target_star.separation(ref_star)\n    pa = target_star.position_angle(ref_star)\n    \n    # Convert to cartesian coordinates (x,y) in arcseconds\n    x = sep * np.sin(pa)\n    y = sep * np.cos(pa)\n    \n    relative_positions.append((x.arcsec, y.arcsec))\n\n# Print relative positions in arcseconds\nfor i, pos in enumerate(relative_positions):\n    print(f\"Reference star {i+1}: x={pos[0]:.2f}\\\", y={pos[1]:.2f}\\\"\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm working on a pulsar observation and need to define its location in spherical coordinates. Show me how to create a SphericalRepresentation with longitude = 287.5 degrees, latitude = -4.3 degrees, and distance = 2.3 kiloparsecs, making sure to use the appropriate astropy units."
         },
         {
            "from": "assistant",
            "value": "from astropy.coordinates import SphericalRepresentation\nfrom astropy import units as u\n\n# Create SphericalRepresentation for pulsar location\n# lon/lat in degrees, distance in kiloparsecs\npulsar_location = SphericalRepresentation(\n    lon=287.5 * u.deg,\n    lat=-4.3 * u.deg, \n    distance=2.3 * u.kpc\n)\n\n# Print the representation to verify\nprint(f\"Pulsar location: {pulsar_location}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm analyzing stellar spectra and need to convert wavelengths from Angstroms to micrometers. How can I do this conversion while preserving the units?"
         },
         {
            "from": "assistant",
            "value": "from astropy import units as u\n\n# Sample wavelength values in Angstroms\nwavelengths_angstrom = [4861.33, 6562.81, 4340.47] * u.Angstrom\n\n# Convert to micrometers while preserving units\nwavelengths_micron = wavelengths_angstrom.to(u.micron)\n\n# Print both values to show the conversion\nfor ang, mic in zip(wavelengths_angstrom, wavelengths_micron):\n    print(f\"{ang:.2f} = {mic:.3f}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm studying the structure of a spiral galaxy and need to represent a point in the disk with rho=20 kpc, phi=225 degrees, and z=0.1 kpc. How can I create this using CylindricalRepresentation and then transform it to spherical coordinates for comparison with catalog data?"
         },
         {
            "from": "assistant",
            "value": "from astropy.coordinates import CylindricalRepresentation, SphericalRepresentation\nimport astropy.units as u\n\n# Create point in cylindrical coordinates\n# rho = radial distance in disk plane, phi = azimuthal angle, z = height above disk\npoint_cyl = CylindricalRepresentation(\n    rho=20 * u.kpc,\n    phi=225 * u.deg,  \n    z=0.1 * u.kpc\n)\n\n# Transform to spherical coordinates\npoint_sph = point_cyl.represent_as(SphericalRepresentation)\n\n# Print both representations\nprint(\"Cylindrical coordinates:\")\nprint(f\"\u03c1 = {point_cyl.rho:.2f}, \u03c6 = {point_cyl.phi:.2f}, z = {point_cyl.z:.2f}\")\nprint(\"\\nSpherical coordinates:\")\nprint(f\"r = {point_sph.distance:.2f}\")\nprint(f\"\u03b8 (latitude) = {point_sph.lat:.2f}\")\nprint(f\"\u03c6 (longitude) = {point_sph.lon:.2f}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I have detected 100 variable stars in M31 with coordinates in ICRS frame, and I want to find their closest counterparts in the Gaia DR3 catalog of 1000 sources in the same region. How can I write code to match these coordinates and get both the sky separation and 3D distances between matched pairs?"
         },
         {
            "from": "assistant",
            "value": "from astropy.coordinates import SkyCoord, match_coordinates_sky\nfrom astropy import units as u\nimport numpy as np\n\n# Create example data (replace with your actual data)\n# M31 variable stars (100 sources)\nra_vars = np.random.uniform(10.5, 11.0, 100)  # RA around M31\ndec_vars = np.random.uniform(40.5, 41.0, 100)  # Dec around M31\n# Gaia sources (1000 sources)\nra_gaia = np.random.uniform(10.5, 11.0, 1000)\ndec_gaia = np.random.uniform(40.5, 41.0, 1000)\nparallax_gaia = np.random.uniform(0.1, 0.3, 1000)  # mas\n\n# Create SkyCoord objects\nvars_coords = SkyCoord(ra=ra_vars*u.degree, dec=dec_vars*u.degree, frame='icrs')\ngaia_coords = SkyCoord(ra=ra_gaia*u.degree, dec=dec_gaia*u.degree, \n                      distance=1000/parallax_gaia*u.pc,  # Convert parallax to distance\n                      frame='icrs')\n\n# Find closest matches\nidx, d2d, d3d = match_coordinates_sky(vars_coords, gaia_coords)\n\n# Get matched pairs within 2 arcsec\nmax_sep = 2 * u.arcsec\nmatched_mask = d2d < max_sep\n\n# Print results for matched pairs\nfor i in np.where(matched_mask)[0]:\n    print(f\"Variable star {i}:\")\n    print(f\"  Sky separation: {d2d[i].to(u.arcsec):.3f}\")\n    print(f\"  3D distance: {d3d[i].to(u.pc):.1f}\")\n    print(f\"  Gaia source index: {idx[i]}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I need to create a flat background model with a constant value of 2.5 counts/pixel for my X-ray image analysis. How can I create and plot this constant model using Const1D over a range of 0 to 100 pixels?"
         },
         {
            "from": "assistant",
            "value": "# Import required modules from astropy\nfrom astropy.modeling import models\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Create a constant model with amplitude 2.5\nconst_model = models.Const1D(amplitude=2.5)\n\n# Generate x values from 0 to 100 pixels\nx = np.linspace(0, 100, 101)\n\n# Evaluate the model\ny = const_model(x)\n\n# Create the plot\nplt.figure(figsize=(8, 6))\nplt.plot(x, y, 'b-', label='Constant Background')\nplt.xlabel('Pixels')\nplt.ylabel('Counts/Pixel')\nplt.title('Flat Background Model (2.5 counts/pixel)')\nplt.grid(True)\nplt.legend()\nplt.show()"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "As part of my study of high-velocity stars, I need to create a Galactic coordinate object for an object with l=67.8 degrees, b=35.4 degrees, and a radial velocity of 235 km/s. What code should I use?"
         },
         {
            "from": "assistant",
            "value": "# Import required modules from astropy\nfrom astropy.coordinates import SkyCoord\nimport astropy.units as u\n\n# Create SkyCoord object in Galactic coordinates\n# l=67.8\u00b0, b=35.4\u00b0, with radial velocity=235 km/s\nstar_coord = SkyCoord(l=67.8*u.deg, \n                     b=35.4*u.deg, \n                     frame='galactic',\n                     radial_velocity=235*u.km/u.s)\n\n# Print the coordinate object to verify\nprint(star_coord)"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm analyzing spectral data of a galaxy with flux values between -50 and 500, stored in array 'spectrum_data'. Some pixels have exactly 0.0 flux due to detector issues. How can I calculate sigma-clipped statistics while ignoring these bad pixels, using 3-sigma clipping and treating 0.0 as masked values?"
         },
         {
            "from": "assistant",
            "value": "from astropy.stats import sigma_clipped_stats\nimport numpy as np\n\n# Create some sample spectrum data\n# This is an example array with some values and zeros\nspectrum_data = np.array([1.2, 0.0, 3.4, 2.1, 0.0, 5.6, 4.2, 1.8, 0.0, 2.5])\n\n# Create a masked array where 0.0 values are masked\nmasked_spectrum = np.ma.masked_values(spectrum_data, 0.0)\n\n# Calculate sigma-clipped statistics\n# sigma=3 for 3-sigma clipping\n# maxiters=5 is a reasonable number of iterations\nmean, median, stddev = sigma_clipped_stats(masked_spectrum, \n                                         sigma=3.0, \n                                         maxiters=5)\n\n# Print results\nprint(f\"Sigma-clipped mean: {mean:.2f}\")\nprint(f\"Sigma-clipped median: {median:.2f}\")\nprint(f\"Sigma-clipped standard deviation: {stddev:.2f}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   }
]