[
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm working on combining multi-wavelength data from different telescopes. How can I write code to identify the coordinate frame of my infrared image's WCS to ensure proper alignment with my optical data?"
         },
         {
            "from": "assistant",
            "value": "from astropy.io import fits\nfrom astropy.wcs import WCS\n\ndef identify_coord_frame(fits_file):\n    \"\"\"\n    Identify and print the coordinate frame information from a FITS image WCS\n    \"\"\"\n    try:\n        # Open the FITS file and extract WCS information\n        hdul = fits.open(fits_file)\n        wcs = WCS(hdul[0].header)\n        \n        # Get coordinate frame information\n        coord_frame = wcs.wcs.radesys if wcs.wcs.radesys else 'Not specified'\n        coord_epoch = wcs.wcs.equinox if wcs.wcs.equinox else 'Not specified'\n        \n        # Print relevant coordinate system information\n        print(f\"Coordinate Reference Frame: {coord_frame}\")\n        print(f\"Coordinate Epoch: {coord_epoch}\")\n        print(f\"Coordinate Type: {wcs.wcs.ctype}\")\n        \n        # Check if distortion terms are present\n        has_distortion = any(key.startswith('PC') or key.startswith('CD') \n                           for key in hdul[0].header)\n        print(f\"Contains Distortion Terms: {has_distortion}\")\n        \n        hdul.close()\n        return wcs\n        \n    except Exception as e:\n        print(f\"Error reading WCS information: {str(e)}\")\n        return None\n\n# Example usage\nwcs = identify_coord_frame('infrared_image.fits')"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I need to identify potential companion stars in my catalog of binary star systems. How can I find the closest matching stars within 3D space for each target star in my dataset?"
         },
         {
            "from": "assistant",
            "value": "from astropy.coordinates import SkyCoord\nimport astropy.units as u\nimport numpy as np\n\ndef find_companions(ra, dec, distance, max_separation=5*u.pc):\n    \"\"\"\n    Find potential companion stars based on 3D spatial proximity\n    \n    Parameters:\n    ra, dec: Arrays of RA and Dec in degrees\n    distance: Array of distances in parsecs\n    max_separation: Maximum 3D separation to consider for companions\n    \"\"\"\n    # Create SkyCoord object with 3D positions\n    coords = SkyCoord(\n        ra=ra*u.degree,\n        dec=dec*u.degree,\n        distance=distance*u.pc\n    )\n    \n    # Initialize results storage\n    companions = []\n    \n    # Search for companions for each star\n    for i, star in enumerate(coords):\n        # Calculate 3D distances to all other stars\n        separations = star.separation_3d(coords)\n        \n        # Find close companions (excluding self-matches)\n        nearby_mask = (separations < max_separation) & (separations > 0*u.pc)\n        companion_indices = np.where(nearby_mask)[0]\n        \n        # Store companion pairs\n        for comp_idx in companion_indices:\n            companions.append({\n                'star_idx': i,\n                'companion_idx': comp_idx,\n                'separation': separations[comp_idx]\n            })\n    \n    return companions\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Sample data (replace with your catalog)\n    ra = np.array([45.0, 45.1, 46.0, 200.0])  # degrees\n    dec = np.array([30.0, 30.1, 31.0, -20.0])  # degrees\n    dist = np.array([100.0, 101.0, 150.0, 500.0])  # parsecs\n    \n    # Find companions\n    companions = find_companions(ra, dec, dist)\n    \n    # Print results\n    for comp in companions:\n        print(f\"Star {comp['star_idx']} has companion {comp['companion_idx']} \"\n              f\"at separation {comp['separation']:.2f}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm working on a pulsar timing project and need to convert a list of pulse arrival times from MJD to Unix timestamp format. Can you demonstrate how to do this conversion?"
         },
         {
            "from": "assistant",
            "value": "from astropy.time import Time\nimport numpy as np\n\n# Sample list of MJD times (Modified Julian Date)\nmjd_times = [59000.5, 59001.5, 59002.5]  # Example pulse arrival times\n\n# Convert MJD to Time object\nt = Time(mjd_times, format='mjd')\n\n# Convert to Unix timestamp (seconds since 1970-01-01 00:00:00 UTC)\nunix_timestamps = t.unix\n\n# Print results for verification\nfor mjd, unix in zip(mjd_times, unix_timestamps):\n    print(f\"MJD: {mjd} \u2192 Unix timestamp: {unix}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm studying stellar associations and need to find the closest match to my candidate star (RA=210.75 deg, Dec=54.28 deg) from a list of 3 known association members with coordinates: RA=[210.73, 210.76, 210.78] deg, Dec=[54.25, 54.29, 54.31] deg. Show me how to do this and store the KD-tree for future matching with the same catalog."
         },
         {
            "from": "assistant",
            "value": "from astropy.coordinates import SkyCoord\nimport astropy.units as u\nfrom astropy.coordinates.matching import match_coordinates_sky\nimport numpy as np\nfrom scipy.spatial import KDTree\n\n# Create SkyCoord object for the candidate star\ncandidate = SkyCoord(ra=210.75*u.deg, dec=54.28*u.deg)\n\n# Create SkyCoord object for the catalog stars\ncatalog_ra = [210.73, 210.76, 210.78]*u.deg\ncatalog_dec = [54.25, 54.29, 54.31]*u.deg\ncatalog = SkyCoord(ra=catalog_ra, dec=catalog_dec)\n\n# Find closest match and separation\nidx, sep, _ = match_coordinates_sky(candidate, catalog)\n\n# Print results\nprint(f\"Closest match index: {idx}\")\nprint(f\"Separation: {sep.to(u.arcsec).value[0]:.2f} arcsec\")\n\n# Create KD-tree from catalog coordinates\n# Convert coordinates to Cartesian system for KD-tree\ncatalog_xyz = catalog.cartesian.xyz.value.T  # Transform to numpy array\nkdtree = KDTree(catalog_xyz)\n\n# Example of how to use the KD-tree for future searches\n# query_xyz = candidate.cartesian.xyz.value.reshape(1, -1)\n# distance, index = kdtree.query(query_xyz)"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I need to analyze a catalog of exoplanets with their orbital periods and masses in different units. Can you show me how to create a structured unit system to handle these mixed astronomical parameters efficiently?"
         },
         {
            "from": "assistant",
            "value": "from astropy import units as u\nfrom astropy.table import Table\nfrom astropy.constants import G  # Import gravitational constant\nimport numpy as np\n\n# Create sample exoplanet data with mixed units\nperiods = [365.256, 687.0, 88.0] * u.day  # Orbital periods in days\nmasses = [1.0, 0.107, 0.055] * u.M_jup    # Masses in Jupiter masses\n\n# Create a structured table with units\nexoplanets = Table()\nexoplanets['name'] = ['Planet A', 'Planet B', 'Planet C']\nexoplanets['period'] = periods\nexoplanets['mass'] = masses\n\n# Demonstrate unit conversions\n# Convert periods to years\nperiods_years = periods.to(u.yr)\n\n# Convert masses to Earth masses\nmasses_earth = masses.to(u.M_earth)\n\n# Print original and converted values\nprint(\"Original units:\")\nprint(exoplanets)\nprint(\"\\nConverted units:\")\nfor name, p_yr, m_earth in zip(exoplanets['name'], periods_years, masses_earth):\n    print(f\"{name}: Period = {p_yr:.2f}, Mass = {m_earth:.2f}\")\n\n# Calculate orbital parameters with units\n# Example: Calculate orbital velocity assuming circular orbits\n# Using simplified form of Kepler's Third Law\nM_sun = 1 * u.M_sun\n\n# Calculate orbital velocities\na = ((G * M_sun * (periods**2) / (4 * np.pi**2))**(1/3)).to(u.au)\nv = (2 * np.pi * a / periods).to(u.km/u.s)\n\nprint(\"\\nDerived quantities:\")\nprint(f\"Orbital velocities: {v}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm examining a globular cluster and need to find the pixel position of its center (RA=201.69712, Dec=-47.47949) in my image. Can you show me the code to convert this sky position to 1-based pixel coordinates using my WCS?"
         },
         {
            "from": "assistant",
            "value": "from astropy.wcs import WCS\nfrom astropy.coordinates import SkyCoord\nimport astropy.units as u\nfrom astropy.io import fits\nimport os\n\ndef convert_sky_to_pixel(fits_file_path, ra, dec):\n    \"\"\"\n    Convert sky coordinates to pixel coordinates for a given FITS file\n    \n    Parameters:\n    -----------\n    fits_file_path : str\n        Path to the FITS file\n    ra : float\n        Right Ascension in degrees\n    dec : float\n        Declination in degrees\n    \"\"\"\n    \n    # Check if file exists\n    if not os.path.exists(fits_file_path):\n        raise FileNotFoundError(f\"The FITS file '{fits_file_path}' does not exist. Please check the file path.\")\n    \n    try:\n        # Read the FITS file to get the header\n        with fits.open(fits_file_path) as hdul:\n            header = hdul[0].header\n        \n        # Create SkyCoord object for the globular cluster center\n        cluster_center = SkyCoord(ra=ra*u.deg, dec=dec*u.deg)\n        \n        # Create WCS object from the header\n        wcs = WCS(header)\n        \n        # Convert sky coordinates to pixel coordinates (1-based)\n        # skycoord_to_pixel returns 0-based pixels, so add 1 for 1-based coordinates\n        x, y = wcs.world_to_pixel(cluster_center)\n        x_1based = x + 1\n        y_1based = y + 1\n        \n        return x_1based, y_1based\n    \n    except Exception as e:\n        raise Exception(f\"Error processing FITS file: {str(e)}\")\n\n# Example usage\nif __name__ == \"__main__\":\n    # Replace with your actual FITS file path\n    fits_file = '/path/to/your/image.fits'\n    \n    # Coordinates for the globular cluster center\n    ra = 201.69712  # degrees\n    dec = -47.47949  # degrees\n    \n    try:\n        x, y = convert_sky_to_pixel(fits_file, ra, dec)\n        print(f\"Cluster center pixel coordinates (1-based): x={x:.2f}, y={y:.2f}\")\n    except Exception as e:\n        print(f\"Error: {str(e)}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I have a dataset of stellar magnitudes from a star cluster survey. How can I create a histogram that automatically determines the optimal bin width to visualize the magnitude distribution?"
         },
         {
            "from": "assistant",
            "value": "from astropy.stats import histogram\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Sample magnitude data (replace with your actual data)\nmagnitudes = np.random.normal(15, 2, 1000)  # Simulated magnitudes\n\n# Calculate optimal bin width using Astropy's histogram function\n# Astropy uses Knuth's rule by default for optimal bin width\nbins, edges = histogram(magnitudes, bins='knuth')\n\n# Create and customize the plot\nplt.figure(figsize=(10, 6))\nplt.hist(magnitudes, bins=edges, edgecolor='black', alpha=0.7)\nplt.xlabel('Magnitude')\nplt.ylabel('Number of Stars')\nplt.title('Stellar Magnitude Distribution')\n\n# Add grid for better readability\nplt.grid(True, alpha=0.3)\n\n# Compute and display mean magnitude\nmean_mag = np.mean(magnitudes)\nplt.axvline(mean_mag, color='red', linestyle='--', \n            label=f'Mean Magnitude: {mean_mag:.2f}')\nplt.legend()\n\nplt.show()"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I have radial velocity measurements from my spectroscopic observations and want to visualize their distribution. Can you show me how to create a properly binned histogram using Knuth's rule for the bin width?"
         },
         {
            "from": "assistant",
            "value": "import numpy as np\nfrom astropy.stats import knuth_bin_width\nimport matplotlib.pyplot as plt\n\n# Sample radial velocity data in km/s (replace with your actual data)\nrv_data = np.random.normal(20, 5, 1000)  # Example data\n\n# Calculate optimal bin width using Knuth's rule\nbin_width, bins = knuth_bin_width(rv_data, return_bins=True)\n\n# Create and plot the histogram\nplt.figure(figsize=(8, 6))\nplt.hist(rv_data, bins=bins, edgecolor='black', alpha=0.7)\nplt.xlabel('Radial Velocity (km/s)')\nplt.ylabel('Frequency')\nplt.title(f'Radial Velocity Distribution\\nBin width = {bin_width:.2f} km/s')\n\n# Add grid for better readability\nplt.grid(True, alpha=0.3)\nplt.show()"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I need to convert a star's magnitude difference of 2.5 to a linear flux ratio using Astropy units. Can you show me how to calculate this dimensionless ratio?"
         },
         {
            "from": "assistant",
            "value": "from astropy.units import Magnitude\n\n# Define magnitude difference\n# Create a magnitude value of 2.5 with Magnitude unit\nmag_diff = Magnitude(2.5)\n\n# Convert magnitude difference to flux ratio\n# Using the relationship: m1 - m2 = -2.5 * log10(F1/F2)\nflux_ratio = 10**(-mag_diff.value/2.5)\n\nprint(f\"A magnitude difference of {mag_diff.value} corresponds to a flux ratio of {flux_ratio:.3f}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "For my research on orbital periods of binary stars, I need to test if my sample [45.2, 67.8, 52.4, 58.9, 61.3] days follows a Rayleigh distribution with scale parameter 55.0. How do I perform a Kuiper test for this scenario?"
         },
         {
            "from": "assistant",
            "value": "import numpy as np\nfrom astropy.stats import kuiper\n\n# Sample orbital periods in days\nperiods = np.array([45.2, 67.8, 52.4, 58.9, 61.3])\n\n# Function to generate Rayleigh CDF\ndef rayleigh_cdf(x, scale):\n    \"\"\"\n    Calculate Rayleigh cumulative distribution function\n    x: input values\n    scale: scale parameter (sigma)\n    \"\"\"\n    return 1 - np.exp(-(x**2)/(2*scale**2))\n\n# Sort the data (required for Kuiper test)\nperiods_sorted = np.sort(periods)\n\n# Generate CDF values for the theoretical Rayleigh distribution\ntheoretical_cdf = rayleigh_cdf(periods_sorted, scale=55.0)\n\n# Perform Kuiper test\nstatistic, p_value = kuiper(periods_sorted, rayleigh_cdf, args=(55.0,))\n\nprint(f\"Kuiper test statistic: {statistic:.4f}\")\nprint(f\"P-value: {p_value:.4f}\")\n# If p_value > 0.05, data is consistent with Rayleigh distribution\nprint(f\"Data {'is' if p_value > 0.05 else 'is not'} consistent with Rayleigh distribution at 5% significance level\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm analyzing the vertical structure of a protoplanetary disk. How can I represent the positions of dust particles in cylindrical coordinates to study their distribution relative to the disk's midplane?"
         },
         {
            "from": "assistant",
            "value": "from astropy.coordinates.representation import CylindricalRepresentation\nfrom astropy import units as u\nimport numpy as np\n\n# Generate sample dust particle positions\n# Creating random distributions for demonstration\nn_particles = 1000\n\n# Generate cylindrical coordinates\n# r: distance from central axis (following exponential distribution)\nr = np.random.exponential(scale=50, size=n_particles) * u.au\n\n# phi: azimuthal angle (uniform distribution)\nphi = np.random.uniform(0, 2*np.pi, n_particles) * u.rad\n\n# z: height above/below midplane (gaussian distribution)\n# Scale height decreases with radius (typical for protoplanetary disks)\nscale_height = 0.1 * (r/u.au) # h/r = 0.1 (typical value)\nz = np.random.normal(0, scale_height, n_particles) * u.au\n\n# Convert to cartesian coordinates\n# Create a representation object\ncyl_coords = CylindricalRepresentation(r, phi, z)\ncart_coords = cyl_coords.to_cartesian()\n\n# Extract x, y, z components\nx = cart_coords.x\ny = cart_coords.y\nz = cart_coords.z\n\n# Calculate basic statistics of vertical distribution\nmean_height = np.mean(abs(z))\nmax_height = np.max(abs(z))\n\nprint(f\"Mean particle height from midplane: {mean_height:.2f}\")\nprint(f\"Maximum particle height: {max_height:.2f}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "For my research on cosmic ray energy distribution, I need to fit a power law model to my observed particle spectrum data. How can I implement this using Astropy's modeling capabilities?"
         },
         {
            "from": "assistant",
            "value": "from astropy.modeling import models, fitting\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate sample cosmic ray spectrum data\n# Energy values in GeV\nenergies = np.logspace(1, 5, 100)  \n\n# Simulated counts with some noise\n# Normalize energies to avoid numerical issues\nnorm_factor = 1000  # Reference energy for normalization\ncounts = 1e4 * ((energies/norm_factor)**(-2.7)) * (1 + np.random.normal(0, 0.1, len(energies)))\n\n# Ensure all values are finite and positive\nmask = np.isfinite(counts) & (counts > 0) & np.isfinite(energies) & (energies > 0)\nenergies = energies[mask]\ncounts = counts[mask]\n\n# Initialize power law model\npowerlaw_init = models.PowerLaw1D(\n    amplitude=1e4,      # Initial guess for amplitude\n    x_0=norm_factor,    # Reference point (pivot energy)\n    alpha=2.7           # Initial guess for power law index\n)\n\n# Create fitter using least squares algorithm\nfitter = fitting.LevMarLSQFitter()\n\ntry:\n    # Fit the model to the data\n    fitted_model = fitter(powerlaw_init, energies, counts)\n    \n    # Print the fitted parameters\n    print(f\"Fitted amplitude: {fitted_model.amplitude.value:.2e}\")\n    print(f\"Fitted power law index: {fitted_model.alpha.value:.3f}\")\n    \n    # Calculate model predictions for plotting\n    fitted_counts = fitted_model(energies)\n    \n    # Plot the results\n    plt.figure(figsize=(10, 6))\n    plt.loglog(energies, counts, 'bo', label='Data', alpha=0.5)\n    plt.loglog(energies, fitted_counts, 'r-', label='Fitted Model')\n    plt.xlabel('Energy (GeV)')\n    plt.ylabel('Counts')\n    plt.title('Cosmic Ray Energy Spectrum')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\nexcept Exception as e:\n    print(f\"An error occurred during fitting: {str(e)}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "For my exoplanet research, I have orbital periods [1.2, 5.8, 2.4, 8.9, 3.1, 6.7, 4.5, 7.2, 2.8, 5.3] days. How can I create a weighted histogram where the weights are their corresponding masses [0.8, 2.1, 1.2, 3.4, 1.5, 2.8, 1.9, 2.9, 1.3, 2.2] Jupiter masses using Scott's rule for binning?"
         },
         {
            "from": "assistant",
            "value": "import numpy as np\nfrom astropy.stats import histogram\nimport matplotlib.pyplot as plt\n\n# Define the data\nperiods = np.array([1.2, 5.8, 2.4, 8.9, 3.1, 6.7, 4.5, 7.2, 2.8, 5.3])  # days\nmasses = np.array([0.8, 2.1, 1.2, 3.4, 1.5, 2.8, 1.9, 2.9, 1.3, 2.2])   # Jupiter masses\n\n# Create histogram using Scott's rule for bin width\n# Scott's rule: bin_width = 3.5 * std(x) / (n^(1/3))\nbin_width = 3.5 * np.std(periods) / (len(periods)**(1/3))\nbins = int(np.ceil((periods.max() - periods.min()) / bin_width))\n\n# Create and plot the weighted histogram\nplt.hist(periods, bins=bins, weights=masses, color='skyblue', \n         edgecolor='black', alpha=0.7)\nplt.xlabel('Orbital Period (days)')\nplt.ylabel('Sum of Planet Masses (Jupiter masses)')\nplt.title('Weighted Histogram of Exoplanet Orbital Periods')\nplt.show()"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm studying pulsar rotation data and need to calculate the angular standard deviation of phase measurements: [0.2, 1.1, 0.8, 2.1, 1.5, 5.9] radians. How can I use astropy's circular statistics to compute this?"
         },
         {
            "from": "assistant",
            "value": "from astropy.stats import circstd\nimport numpy as np\n\n# Phase measurements in radians\nphases = np.array([0.2, 1.1, 0.8, 2.1, 1.5, 5.9])\n\n# Calculate circular standard deviation\n# circstd assumes angles are in radians by default\nangular_std = circstd(phases)\n\nprint(f\"Angular standard deviation: {angular_std:.3f} radians\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm studying stellar asteroseismology and need to model multiple oscillation modes. How can I combine several Cosine1D models with different frequencies to represent the observed stellar variations?"
         },
         {
            "from": "assistant",
            "value": "from astropy.modeling import models, fitting\nimport numpy as np\n\n# Create synthetic time series data\ntime = np.linspace(0, 10, 1000)  # 10 days of observations\n\n# Define multiple oscillation frequencies (in cycles/day)\nfrequencies = [23.45, 27.12, 30.85]  # Example frequencies\namplitudes = [0.8, 0.5, 0.3]  # Relative amplitudes\nphases = [0.1, 0.5, 0.2]  # Phase shifts in radians\n\n# Initialize combined model\ncombined_model = None\n\n# Combine multiple Cosine1D models with different frequencies\nfor freq, amp, phase in zip(frequencies, amplitudes, phases):\n    # Create individual oscillation mode\n    mode = models.Cosine1D(\n        amplitude=amp,\n        frequency=freq,\n        phase=phase\n    )\n    \n    # Add mode to combined model\n    if combined_model is None:\n        combined_model = mode\n    else:\n        combined_model += mode\n\n# Generate synthetic light curve with multiple modes\nsignal = combined_model(time)\n\n# Add some noise to simulate real observations\nnp.random.seed(42)\nnoise = np.random.normal(0, 0.1, len(time))\nobserved_signal = signal + noise\n\n# The observed_signal now contains multiple oscillation modes\n# typical of stellar pulsations with added noise"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm studying a group of galaxies and have their distance moduli from photometric observations. How can I write code to convert these distance moduli into proper distances in megaparsecs?"
         },
         {
            "from": "assistant",
            "value": "# Import required astropy modules\nfrom astropy.coordinates import Distance\nimport astropy.units as u\nimport numpy as np\n\n# Example distance moduli data (can be replaced with actual observations)\ndistance_moduli = np.array([35.2, 34.8, 35.5, 34.9])\n\n# Convert distance moduli to distances in Mpc\n# Using the formula: d = 10^((m-M)/5 + 1) parsecs\ndistances = Distance(distmod=distance_moduli)\n\n# Convert to Megaparsecs and print results\ndistances_mpc = distances.to(u.Mpc)\n\n# Print results\nfor dm, d in zip(distance_moduli, distances_mpc):\n    print(f\"Distance modulus: {dm:.1f} -> Distance: {d:.2f}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I have a large dataset of galaxy redshift measurements and want to create a histogram that effectively shows their distribution. Given the potential for subtle features in the data, how can I use Knuth's rule for optimal bin selection in Astropy?"
         },
         {
            "from": "assistant",
            "value": "import numpy as np\nfrom astropy.stats import knuth_bin_width\nimport matplotlib.pyplot as plt\n\n# Generate sample redshift data (replace with your actual data)\nnp.random.seed(42)\nredshifts = np.concatenate([\n    np.random.normal(0.5, 0.1, 500),  # Simulating main galaxy population\n    np.random.normal(0.8, 0.05, 200)   # Simulating secondary population\n])\n\n# Calculate optimal bin width using Knuth's rule\nbin_width, bins = knuth_bin_width(redshifts, return_bins=True)\n\n# Create histogram with optimal binning\nplt.figure(figsize=(10, 6))\nplt.hist(redshifts, bins=bins, edgecolor='black', alpha=0.7)\nplt.xlabel('Redshift (z)')\nplt.ylabel('Number of Galaxies')\nplt.title(f'Galaxy Redshift Distribution\\nOptimal bin width = {bin_width:.3f}')\n\n# Add text box with bin information\nplt.text(0.98, 0.95, f'Number of bins: {len(bins)-1}',\n         transform=plt.gca().transAxes,\n         horizontalalignment='right',\n         verticalalignment='top',\n         bbox=dict(facecolor='white', alpha=0.8))\n\nplt.show()"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm analyzing data from the JWST NIRCam instrument (pixel size roughly 0.031 arcsec) and need to verify the pixel scale in my reduced image. Can you show me how to compute the projected pixel scales using my WCS object to confirm the calibration?"
         },
         {
            "from": "assistant",
            "value": "from astropy.wcs import WCS\nimport astropy.units as u\nimport numpy as np\n\ndef check_pixel_scale(wcs_obj):\n    \"\"\"\n    Compute pixel scales from WCS solution for JWST NIRCam image\n    Returns scales in arcsec/pixel\n    \"\"\"\n    # Get pixel scales at image center\n    # Using CD matrix elements to handle possible rotation\n    try:\n        # Get CD matrix elements\n        cd1_1 = wcs_obj.wcs.cd[0,0]\n        cd1_2 = wcs_obj.wcs.cd[0,1] \n        cd2_1 = wcs_obj.wcs.cd[1,0]\n        cd2_2 = wcs_obj.wcs.cd[1,1]\n        \n        # Compute pixel scales using CD matrix\n        scale_x = np.sqrt(cd1_1**2 + cd2_1**2)\n        scale_y = np.sqrt(cd1_2**2 + cd2_2**2)\n        \n        # Convert to arcseconds\n        scale_x *= 3600  # Convert degrees to arcsec\n        scale_y *= 3600\n        \n        print(f\"X pixel scale: {scale_x:.3f} arcsec/pixel\")\n        print(f\"Y pixel scale: {scale_y:.3f} arcsec/pixel\")\n        \n        # Compare to expected NIRCam scale\n        if abs(scale_x - 0.031) < 0.005:\n            print(\"Pixel scale consistent with NIRCam\")\n        else:\n            print(\"WARNING: Pixel scale deviates from expected NIRCam value\")\n            \n    except Exception as e:\n        print(f\"Error computing pixel scale: {str(e)}\")\n\n# Example usage (assuming you have a WCS object):\n# wcs = WCS('your_fits_file.fits')\n# check_pixel_scale(wcs)"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I have photometric measurements of a variable star over 200 epochs, and want to exclude flare events before calculating the baseline statistics. Show me how to use sigma_clipped_stats with 5 iterations maximum, using mean for center calculation and median absolute deviation for spread."
         },
         {
            "from": "assistant",
            "value": "from astropy.stats import sigma_clipped_stats\nimport numpy as np\n\n# Sample data array of photometric measurements\n# In real use, replace this with your actual measurements\nphot_data = np.random.normal(15.0, 0.2, 200)  # Example: 200 measurements centered at mag 15\n\n# Calculate sigma-clipped statistics\n# cenfunc=np.mean uses mean for center calculation\n# stdfunc=mad_std uses median absolute deviation for spread calculation\nmean, median, stddev = sigma_clipped_stats(phot_data, \n                                         maxiters=5,\n                                         cenfunc='mean',\n                                         stdfunc='mad_std')\n\n# The results exclude statistical outliers (like flares)\nprint(f\"Baseline magnitude (mean): {mean:.3f}\")\nprint(f\"Median magnitude: {median:.3f}\")\nprint(f\"Standard deviation: {stddev:.3f}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm analyzing exoplanet transit data and need to create a structured quantity to store both the period (3.5 days) and semi-major axis (0.045 AU) of an exoplanet. How can I use astropy.units.structured to combine these measurements?"
         },
         {
            "from": "assistant",
            "value": "from astropy import units as u\nfrom astropy.table import QTable\nimport numpy as np\n\n# Create a QTable with the orbital parameters\ndata = [{\n    'period': 3.5,\n    'semi_major_axis': 0.045\n}]\n\n# Create the table with units\norbit_table = QTable(\n    data,\n    units={'period': u.day, 'semi_major_axis': u.AU}\n)\n\n# Access individual components\nprint(f\"Period: {orbit_table['period'][0]}\")  # Shows period with units\nprint(f\"Semi-major axis: {orbit_table['semi_major_axis'][0]}\")  # Shows axis with units\n\n# Convert units if needed\nperiod_in_hours = orbit_table['period'].to(u.hour)\nprint(f\"Period in hours: {period_in_hours[0]}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm studying a young stellar disk in my observations. Can you help me create a code snippet to model a uniform brightness disk using Astropy, centered at the peak emission coordinates with a radius matching the observed extent?"
         },
         {
            "from": "assistant",
            "value": "import numpy as np\nfrom astropy.modeling import models, fitting\nfrom astropy.coordinates import SkyCoord\nimport astropy.units as u\n\n# Define disk parameters (example values - adjust as needed)\npeak_ra = 180.5 * u.degree  # Peak RA coordinate\npeak_dec = -30.2 * u.degree # Peak Dec coordinate\ndisk_radius = 2.5 * u.arcsec # Disk radius\namplitude = 1.0  # Peak brightness\n\n# Create coordinate grid for the model (in arcsec)\ny, x = np.mgrid[-5:5:100j, -5:5:100j] * u.arcsec\n\n# Create center position\ncenter = SkyCoord(ra=peak_ra, dec=peak_dec)\n\n# Create disk model using uniform disk (simpler approach)\ndef create_uniform_disk(x, y, radius, amplitude):\n    # Calculate radial distance from center\n    r = np.sqrt(x**2 + y**2)\n    \n    # Create uniform disk (all pixels within radius have constant brightness)\n    disk = np.zeros(x.shape)\n    disk[r <= radius] = amplitude\n    \n    return disk\n\n# Generate the model\n# Convert all quantities to same unit (arcsec) before comparison\ndisk_image = create_uniform_disk(\n    x.to(u.arcsec).value,\n    y.to(u.arcsec).value,\n    disk_radius.to(u.arcsec).value,\n    amplitude\n)\n\n# disk_image now contains the uniform brightness disk model"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "Working with exoplanet data containing columns 'planet_mass', 'orbital_period', and 'stellar_mass' for 25 systems. How do I write code to reorder these columns so that 'stellar_mass' appears first while maintaining the column relationships in the table?"
         },
         {
            "from": "assistant",
            "value": "from astropy.table import Table\nimport numpy as np\n\n# Create sample exoplanet data for demonstration\n# (In practice, this would be your actual data)\nnp.random.seed(42)\ndata = Table({\n    'planet_mass': np.random.uniform(0.1, 10, 25),  # Jupiter masses\n    'orbital_period': np.random.uniform(1, 365, 25), # days\n    'stellar_mass': np.random.uniform(0.5, 2, 25)    # Solar masses\n})\n\n# Reorder columns by specifying new column order\n# Keep all relationships intact by only changing column order\ndata = data[['stellar_mass', 'planet_mass', 'orbital_period']]\n\n# Verify the new column order\nprint(\"New column order:\", data.colnames)"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I have a stack of spectral data from a quasar showing both very bright emission lines and faint continuum features. What's the best way to normalize the visualization to show the full range of spectral features?"
         },
         {
            "from": "assistant",
            "value": "from astropy.visualization import ImageNormalize, LogStretch, AsinhStretch\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_normalized_spectrum(wavelength, flux):\n    \"\"\"\n    Plot spectral data using two different normalizations to show both\n    bright emission lines and faint continuum features\n    \"\"\"\n    # Create figure with two subplots\n    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8))\n    \n    # Plot 1: LogStretch normalization good for emission lines\n    norm1 = ImageNormalize(flux, stretch=LogStretch())\n    ax1.plot(wavelength, flux, 'b-', norm=norm1)\n    ax1.set_ylabel('Flux (Log scale)')\n    ax1.set_title('Logarithmic normalization - Emphasizes emission lines')\n    \n    # Plot 2: AsinhStretch normalization better for faint features\n    norm2 = ImageNormalize(flux, stretch=AsinhStretch(a=0.1))\n    ax2.plot(wavelength, flux, 'r-', norm=norm2)\n    ax2.set_ylabel('Flux (Asinh scale)')\n    ax2.set_xlabel('Wavelength')\n    ax2.set_title('Asinh normalization - Better for faint continuum')\n    \n    plt.tight_layout()\n    return fig\n\n# Example usage (assuming you have wavelength and flux data):\n# wavelength = np.linspace(4000, 9000, 1000)  # Example wavelength array\n# flux = your_spectral_data                    # Your flux measurements\n# fig = plot_normalized_spectrum(wavelength, flux)\n# plt.show()"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm cross-matching data between radio and optical observations of galaxies. What's the proper way to merge these catalogs while handling potential data type mismatches between corresponding columns?"
         },
         {
            "from": "assistant",
            "value": "from astropy.table import Table, join\nfrom astropy.coordinates import SkyCoord\nimport astropy.units as u\n\ndef cross_match_catalogs(radio_file, optical_file, ra_col1='RA', dec_col1='DEC', \n                        ra_col2='ra', dec_col2='dec', match_radius=2*u.arcsec):\n    \"\"\"\n    Cross-match radio and optical catalogs using coordinates\n    Parameters:\n    -----------\n    radio_file, optical_file : str\n        Input catalog file paths\n    ra_col1, dec_col1 : str\n        Column names for coordinates in radio catalog\n    ra_col2, dec_col2 : str\n        Column names for coordinates in optical catalog\n    match_radius : astropy.units.Quantity\n        Maximum separation for matching\n    \"\"\"\n    try:\n        # Read catalogs\n        radio_cat = Table.read(radio_file)\n        optical_cat = Table.read(optical_file)\n        \n        # Create SkyCoord objects for both catalogs\n        radio_coords = SkyCoord(ra=radio_cat[ra_col1], \n                              dec=radio_cat[dec_col1],\n                              unit='deg')\n        optical_coords = SkyCoord(ra=optical_cat[ra_col2], \n                                dec=optical_cat[dec_col2],\n                                unit='deg')\n        \n        # Perform coordinate matching\n        idx, d2d, _ = radio_coords.match_to_catalog_sky(optical_coords)\n        \n        # Create mask for matches within radius\n        matches = d2d <= match_radius\n        \n        # Join tables using indices where matches are found\n        matched_cat = join(radio_cat[matches], \n                         optical_cat[idx[matches]], \n                         join_type='left')\n        \n        return matched_cat\n        \n    except Exception as e:\n        print(f\"Error during cross-matching: {str(e)}\")\n        return None\n\n# Example usage:\n# matched_table = cross_match_catalogs('radio.fits', 'optical.fits')"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I have metallicity measurements [0.15, 0.18, 0.12, 0.14, 0.16, 0.13] for stars in a globular cluster. How do I implement jackknife_resampling to evaluate the reliability of the mean metallicity estimate?"
         },
         {
            "from": "assistant",
            "value": "import numpy as np\nfrom astropy.stats import jackknife_resampling, jackknife_stats\n\n# Metallicity measurements for stars\nmetallicities = np.array([0.15, 0.18, 0.12, 0.14, 0.16, 0.13])\n\n# Perform jackknife resampling\nresamples = jackknife_resampling(metallicities)\n\n# Calculate jackknife statistics (mean, bias, std err)\nmean_estimate, bias, std_err, conf_interval = jackknife_stats(\n    metallicities, \n    np.mean, \n    confidence_level=0.95\n)\n\nprint(f\"Original mean: {np.mean(metallicities):.3f}\")\nprint(f\"Jackknife mean estimate: {mean_estimate:.3f}\")\nprint(f\"Standard error: {std_err:.3f}\")\nprint(f\"95% confidence interval: [{conf_interval[0]:.3f}, {conf_interval[1]:.3f}]\")\n\n# Print individual jackknife sample means for verification\njackknife_means = np.array([np.mean(sample) for sample in resamples])\nprint(\"\\nJackknife sample means:\")\nfor i, mean in enumerate(jackknife_means):\n    print(f\"Sample {i+1}: {mean:.3f}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm analyzing spectra of quasars and have measured their redshifts, but some measurements are contaminated. Can you show me how to compute the typical redshift of my sample using biweight_location? Here's my data with NaN values: [0.25, 0.27, nan, 0.24, 0.26, 0.25, nan, 0.28, 1.5, 0.26]."
         },
         {
            "from": "assistant",
            "value": "from astropy.stats import biweight_location\nimport numpy as np\n\n# Sample redshift data with some NaN values\nredshifts = np.array([0.25, 0.27, np.nan, 0.24, 0.26, 0.25, np.nan, 0.28, 1.5, 0.26])\n\n# Remove NaN values and compute biweight location (robust central tendency)\n# biweight_location is less sensitive to outliers than mean\nclean_redshifts = redshifts[~np.isnan(redshifts)]\ntypical_redshift = biweight_location(clean_redshifts)\n\nprint(f\"Typical redshift of the sample: {typical_redshift:.3f}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I need to model a point spread function in my telescope data that has a FWHM of 1.8 arcseconds. Can you show me how to calculate the corresponding sigma value for a Gaussian fit?"
         },
         {
            "from": "assistant",
            "value": "from astropy.modeling import models\nimport numpy as np\n\n# FWHM (Full Width at Half Maximum) = 1.8 arcsec\nfwhm = 1.8  # arcseconds\n\n# Convert FWHM to sigma using the relation:\n# FWHM = 2.355 * sigma\n# This comes from the relation FWHM = 2*sqrt(2*ln(2))*sigma\nsigma = fwhm / 2.355\n\n# Create a 2D Gaussian model with the calculated sigma\npsf_model = models.Gaussian2D(\n    amplitude=1.0,  # normalized amplitude\n    x_mean=0.0,     # centered at origin\n    y_mean=0.0,\n    x_stddev=sigma,\n    y_stddev=sigma  # assuming circular PSF\n)\n\nprint(f\"For a FWHM of {fwhm} arcseconds:\")\nprint(f\"The corresponding Gaussian sigma is {sigma:.3f} arcseconds\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm working with data from the Hubble Deep Field and need to identify where specific galaxies would appear in my image frame. Show me how to transform their sky coordinates to pixel positions in my image."
         },
         {
            "from": "assistant",
            "value": "from astropy.io import fits\nfrom astropy.wcs import WCS\nfrom astropy.coordinates import SkyCoord\nimport astropy.units as u\nimport os\n\ndef process_fits_image(fits_file):\n    # Check if file exists\n    if not os.path.exists(fits_file):\n        raise FileNotFoundError(f\"The FITS file '{fits_file}' does not exist. Please check the file path.\")\n    \n    try:\n        # Load the FITS image and WCS information\n        with fits.open(fits_file) as hdul:\n            wcs = WCS(hdul[0].header)\n            \n            # Example galaxy coordinates (RA, Dec)\n            # Using HDF coordinates as example\n            galaxies = [\n                SkyCoord('12h36m49.4s', '+62d12m50s'),  # Galaxy 1\n                SkyCoord('12h36m51.8s', '+62d13m54s'),  # Galaxy 2\n            ]\n\n            # Transform sky coordinates to pixel coordinates\n            for galaxy in galaxies:\n                # skycoord_to_pixel returns (x, y) pixel coordinates\n                x, y = wcs.world_to_pixel(galaxy)\n                print(f\"Galaxy at RA={galaxy.ra.to_string(unit=u.hour)}, \"\n                      f\"Dec={galaxy.dec.to_string()} -> \"\n                      f\"Pixel position: x={x:.1f}, y={y:.1f}\")\n                \n    except Exception as e:\n        print(f\"An error occurred while processing the FITS file: {str(e)}\")\n\n# Specify the correct path to your FITS file\nfits_file = 'path/to/your/hubble_deep_field.fits'  # Replace with actual path\n\ntry:\n    process_fits_image(fits_file)\nexcept FileNotFoundError as e:\n    print(e)\n    print(\"Please ensure you have the correct FITS file path.\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm analyzing the magnetic field around a pulsar with a measured value of 2.5e12 Gauss. How can I convert this to Tesla using astropy's CGS units for further comparison with other observations?"
         },
         {
            "from": "assistant",
            "value": "from astropy import units as u\n\n# Define the magnetic field strength in Gauss\nB_gauss = 2.5e12 * u.G\n\n# Convert to Tesla (1 Tesla = 10^4 Gauss)\nB_tesla = B_gauss.to(u.T)\n\nprint(f\"Magnetic field strength:\")\nprint(f\"In Gauss: {B_gauss:.2e}\")\nprint(f\"In Tesla: {B_tesla:.2e}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm analyzing supernova positions in a galaxy cluster and need to convert their celestial coordinates to detector coordinates. How can I transform these positions considering the WCS information from my observation?"
         },
         {
            "from": "assistant",
            "value": "from astropy.wcs import WCS\nfrom astropy.io import fits\nimport numpy as np\n\ndef celestial_to_pixel(ra_dec_list, fits_file):\n    \"\"\"\n    Convert celestial coordinates (RA, Dec) to pixel coordinates using WCS\n    \n    Parameters:\n    ra_dec_list: list of tuples containing (RA, Dec) in degrees\n    fits_file: path to the FITS image with WCS information\n    \n    Returns:\n    pixel_coords: array of pixel coordinates (x, y)\n    \"\"\"\n    try:\n        # Read WCS information from FITS header\n        with fits.open(fits_file) as hdul:\n            wcs = WCS(hdul[0].header)\n        \n        # Convert coordinates\n        # Input should be array of shape (N, 2) for N positions\n        coords = np.array(ra_dec_list)\n        \n        # Transform coordinates (returns pixel x, y)\n        # Origin=0 for 0-based indexing (Python style)\n        pixel_coords = wcs.all_world2pix(coords, 0)\n        \n        return pixel_coords\n    \n    except Exception as e:\n        print(f\"Error in coordinate transformation: {e}\")\n        return None\n\n# Example usage\nif __name__ == \"__main__\":\n    # Example supernova positions (RA, Dec in degrees)\n    supernova_positions = [\n        (197.450, -23.381),\n        (197.892, -23.523),\n        (197.756, -23.498)\n    ]\n    \n    # Convert coordinates\n    pixels = celestial_to_pixel(supernova_positions, 'cluster_image.fits')\n    \n    if pixels is not None:\n        for i, (x, y) in enumerate(pixels):\n            print(f\"Supernova {i+1}: x={x:.2f}, y={y:.2f} pixels\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "Can you help me write code using RedshiftScaleFactor to calculate how the wavelength of H-alpha line (6563 \u00c5) would appear when observing a galaxy at redshift z=0.5?"
         },
         {
            "from": "assistant",
            "value": "# Import required modules from astropy\nfrom astropy import units as u\n\n# Define the rest wavelength of H-alpha line\nh_alpha_rest = 6563 * u.angstrom\n\n# Define the redshift\nz = 0.5\n\n# Calculate observed wavelength: lambda_obs = lambda_rest * (1 + z)\nh_alpha_observed = h_alpha_rest * (1 + z)\n\n# Print the result\nprint(f\"H-alpha line at rest: {h_alpha_rest}\")\nprint(f\"H-alpha line at z={z}: {h_alpha_observed.to(u.angstrom)}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I have exoplanet data in a Table with columns 'planet_id', 'orbital_period', and 'mass_jupiter'. Show me how to use TableColumns to access these columns by both name and index position, particularly to get the 'mass_jupiter' column which is at index 2."
         },
         {
            "from": "assistant",
            "value": "from astropy.table import Table\n\n# Create a sample exoplanet table\nexoplanets = Table([\n    ['b', 'c', 'd'],\n    [10.2, 15.8, 32.5],\n    [1.2, 0.8, 2.1]\n], names=('planet_id', 'orbital_period', 'mass_jupiter'))\n\n# Access columns using TableColumns in different ways\ncols = exoplanets.columns\n\n# Method 1: Access by column name\nmass_col_1 = cols['mass_jupiter']\n\n# Method 2: Access by index position \nmass_col_2 = cols[2]  # mass_jupiter is at index 2\n\n# Print to verify both methods give same result\nprint(\"Using column name:\", mass_col_1)\nprint(\"Using index position:\", mass_col_2)\n\n# Access multiple columns (from the table directly, not from cols)\nperiod_mass = exoplanets[['orbital_period', 'mass_jupiter']]\nprint(\"\\nMultiple columns:\", period_mass)"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm studying exoplanet transits and need to organize my data into a QTable. How do I structure a table containing transit timing, depth, duration, and their associated uncertainties with appropriate astronomical units?"
         },
         {
            "from": "assistant",
            "value": "# Import required AstroPy modules\nfrom astropy.table import QTable\nfrom astropy import units as u\nimport numpy as np\n\n# Create sample transit data with units\n# Each column includes values and their uncertainties\ndata = QTable({\n    'time': [2458000.123, 2458010.456, 2458020.789] * u.d,  # Julian Date\n    'time_err': [0.001, 0.001, 0.002] * u.d,\n    'depth': [0.015, 0.014, 0.015] * u.dimensionless_unscaled,  # Transit depth as fraction\n    'depth_err': [0.001, 0.001, 0.001] * u.dimensionless_unscaled,\n    'duration': [2.5, 2.6, 2.4] * u.hour,  # Transit duration\n    'duration_err': [0.1, 0.1, 0.1] * u.hour\n})\n\n# Add metadata to columns\ndata['time'].description = 'Transit mid-time'\ndata['depth'].description = 'Transit depth'\ndata['duration'].description = 'Transit duration'\n\n# Display the table\nprint(data)"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "For my exoplanet transit timing analysis, I want to convert orbital periods from hours to years. What's the best way to handle this using Astro-py units?"
         },
         {
            "from": "assistant",
            "value": "from astropy import units as u\n\n# Define the orbital periods in hours\nperiods_hours = [24, 48, 72, 168]  # Example periods\n\n# Convert hours to years using astropy units\nperiods_years = (periods_hours * u.hour).to(u.year)\n\n# Print both for comparison\nfor hours, years in zip(periods_hours, periods_years):\n    print(f\"Period: {hours} hours = {years:.6f}\")\n\n# Example of handling a single value conversion\nsingle_period = 8760 * u.hour  # ~1 year\nyears_value = single_period.to(u.year)\nprint(f\"\\nSingle period conversion: {years_value:.6f}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm building a custom model for stellar rotation and need a parameter for angular velocity that internally uses radians/second but should display in degrees/second to users. The default value should be 30 degrees/second. Show me how to create this Parameter with appropriate getter and setter functions."
         },
         {
            "from": "assistant",
            "value": "from astropy import units as u\nfrom astropy.units import Quantity\n\nclass RotationModel:\n    def __init__(self):\n        # Initialize with 30 deg/s, storing internally as rad/s\n        self._omega = (30 * u.deg/u.s).to(u.rad/u.s)\n\n    @property\n    def angular_velocity(self) -> Quantity:\n        \"\"\"Get angular velocity in degrees/second\"\"\"\n        return self._omega.to(u.deg/u.s)\n    \n    @angular_velocity.setter \n    def angular_velocity(self, value: Quantity):\n        \"\"\"Set angular velocity, converting input to rad/s for internal storage\n        \n        Parameters\n        ----------\n        value : astropy.units.Quantity\n            Angular velocity with angular units per time\n        \"\"\"\n        if not isinstance(value, Quantity):\n            raise TypeError(\"Angular velocity must be an Astropy Quantity\")\n        \n        # Convert to rad/s for internal storage\n        self._omega = value.to(u.rad/u.s)\n\n# Example usage\nmodel = RotationModel()\nprint(model.angular_velocity)  # Shows 30 deg/s\n\n# Set new value\nmodel.angular_velocity = 45 * u.deg/u.s\nprint(model.angular_velocity)  # Shows 45 deg/s"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "Working with UV spectral data where flux values are [1.2e-16, 3.4e-16, 8.9e-16, 2.1e-15]. How do I scale this to [0,1] using MinMaxInterval before applying a colormap?"
         },
         {
            "from": "assistant",
            "value": "from astropy.visualization import MinMaxInterval\nimport numpy as np\n\n# Sample UV flux data\nflux_data = np.array([1.2e-16, 3.4e-16, 8.9e-16, 2.1e-15])\n\n# Create MinMaxInterval normalizer\nnorm = MinMaxInterval()\n\n# Scale the data to [0,1] interval\nscaled_flux = norm(flux_data)\n\n# Print original and scaled values\nprint(\"Original flux:\", flux_data)\nprint(\"Scaled flux:\", scaled_flux)\n\n# Now scaled_flux can be used with any colormap\n# For example with matplotlib (if needed):\n# import matplotlib.pyplot as plt\n# plt.scatter(range(len(scaled_flux)), scaled_flux, c=scaled_flux, cmap='viridis')"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm studying the cosmic microwave background radiation and need to work with both energy and frequency representations of the data. How can I convert between these units in my analysis?"
         },
         {
            "from": "assistant",
            "value": "from astropy import units as u\nfrom astropy.constants import h, k_B, c  # Added speed of light constant\nimport numpy as np\n\n# Create sample CMB data\nfrequency = [160, 220, 280] * u.GHz  # Common CMB observation frequencies\ntemperature = 2.725 * u.K  # CMB temperature\n\n# Calculate energy using Planck's law (E = hf)\nenergy = h * frequency\nprint(f\"Energy values: {energy.to(u.eV)}\")\n\n# Calculate frequency from energy (f = E/h)\nsample_energy = 0.001 * u.eV  # Sample energy value\nfreq_from_energy = (sample_energy/h).to(u.GHz)\nprint(f\"\\nFrequency from energy: {freq_from_energy}\")\n\n# Calculate photon energy distribution using Wien's law (E = kT)\nthermal_energy = k_B * temperature\nprint(f\"\\nCharacteristic thermal energy: {thermal_energy.to(u.eV)}\")\n\n# Convert between wavelength and frequency (c = \u03bbf)\nwavelength = c / frequency.to(u.Hz)  # Corrected wavelength calculation\nprint(f\"\\nCorresponding wavelengths: {wavelength.to(u.mm)}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "For my public outreach presentation, I need to show which constellation Polaris (RA=02h 31m 49.09s, Dec=+89d 15m 50.8s) is in. Can you help me write code using get_constellation that returns the full name instead of the abbreviation?"
         },
         {
            "from": "assistant",
            "value": "from astropy.coordinates import SkyCoord\nfrom astropy.coordinates import get_constellation\n\ndef get_full_constellation_name(ra, dec):\n    # Create SkyCoord object from RA/Dec\n    coords = SkyCoord(ra, dec, unit=('hourangle', 'deg'))\n    \n    # Get constellation - since get_constellation() returns the full name,\n    # we don't need the abbreviation dictionary anymore\n    constellation = get_constellation(coords)\n    \n    # Return the constellation name directly\n    return constellation\n\n# Polaris coordinates\npolaris_ra = \"02h31m49.09s\"\npolaris_dec = \"+89d15m50.8s\"\n\n# Get and print constellation  \nconstellation = get_full_constellation_name(polaris_ra, polaris_dec)\nprint(f\"Polaris is in the constellation {constellation}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "While merging pulsar timing data, I'm getting a TableMergeError because one table has periods in seconds (0.0892, 1.275) and another in milliseconds (89.2, 1275). What's the proper way to write code that resolves unit conflicts before merging?"
         },
         {
            "from": "assistant",
            "value": "from astropy.table import Table, vstack\nfrom astropy import units as u\n\n# Sample data - creating two tables with different units\ntable1 = Table({\n    'pulsar': ['PSR1', 'PSR2'],\n    'period': [0.0892, 1.275] * u.second  # periods in seconds\n})\n\ntable2 = Table({\n    'pulsar': ['PSR3', 'PSR4'],\n    'period': [89.2, 1275] * u.millisecond  # periods in milliseconds\n})\n\n# Convert all period values to a common unit (seconds) before merging\ntable2['period'] = table2['period'].to(u.second)\n\n# Now the tables can be safely merged\nmerged_table = vstack([table1, table2])\n\n# Print the merged table to verify\nprint(\"Merged table with consistent units:\")\nprint(merged_table)"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "For my stellar evolution models, I need to calculate the energy released from mass loss during the main sequence phase. How can I write code to convert stellar mass loss to energy output?"
         },
         {
            "from": "assistant",
            "value": "from astropy import units as u\nfrom astropy.constants import c, M_sun\n\ndef mass_loss_energy(initial_mass, final_mass, time_period):\n    \"\"\"\n    Calculate energy released from stellar mass loss using E = mc\u00b2\n    \n    Parameters:\n    initial_mass: Mass at start of main sequence (in solar masses)\n    final_mass: Mass at end of main sequence (in solar masses) \n    time_period: Time period over which mass loss occurs (in years)\n    \n    Returns: Energy release rate in solar luminosities\n    \"\"\"\n    # Convert masses to kg\n    m_i = initial_mass * M_sun\n    m_f = final_mass * M_sun\n    \n    # Calculate mass lost\n    delta_m = m_i - m_f\n    \n    # Calculate total energy using E = mc\u00b2\n    energy = (delta_m * c**2).to('J')\n    \n    # Convert time to seconds\n    time = time_period * u.yr.to('s') * u.s\n    \n    # Calculate power output (energy/time)\n    power = (energy / time).to('W')\n    \n    # Convert to solar luminosities\n    solar_lum = 3.828e26 * u.W  # Solar luminosity in watts\n    power_in_lsun = power / solar_lum\n    \n    return power_in_lsun\n\n# Example usage\nenergy_rate = mass_loss_energy(20, 19.5, 1e6)\nprint(f\"Energy release rate: {energy_rate:.2e} solar luminosities\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm working on a particle physics problem where I need to convert 3.5e-24 grams of matter into its equivalent energy in electron volts (eV). Could you demonstrate the code using astropy's unit conversion system?"
         },
         {
            "from": "assistant",
            "value": "from astropy import units as u\nfrom astropy.constants import c\n\n# Define mass in grams\nmass = 3.5e-24 * u.g\n\n# Calculate energy using E = mc^2\nenergy = (mass * c**2).to(u.eV)\n\nprint(f\"Mass of {mass} is equivalent to {energy:.2e}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "For my stellar population study, I need to model the star formation history as an exponential decay. How do I create a model with an initial star formation rate of 100 solar masses per year and an e-folding time of 2 billion years?"
         },
         {
            "from": "assistant",
            "value": "import numpy as np\nfrom astropy.time import Time, TimeDelta\nimport astropy.units as u\n\n# Set up time array (from 0 to 10 Gyr with 100 points)\nt = np.linspace(0, 10, 100) * u.Gyr\n\n# Parameters for exponential decay\ninitial_sfr = 100 * u.Msun / u.yr  # Initial star formation rate\ntau = 2 * u.Gyr                    # e-folding time\n\n# Calculate star formation rate over time\n# SFR(t) = SFR_0 * exp(-t/\u03c4)\nsfr = initial_sfr * np.exp(-t/tau)\n\n# Convert to solar masses per year (removing units for plotting if needed)\nsfr_value = sfr.to(u.Msun/u.yr).value\n\n# Example of accessing a specific time point\nt_specific = 5 * u.Gyr  # Check SFR at 5 Gyr\nsfr_at_t = initial_sfr * np.exp(-t_specific/tau)\nprint(f\"SFR at {t_specific} = {sfr_at_t:.2f}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm analyzing exoplanet transit data and need to convert the host stars' equatorial coordinates from their string representations (in hours, minutes, seconds for RA and degrees, arcminutes, arcseconds for Dec) into a format I can use for calculations. What's the best way to handle this conversion?"
         },
         {
            "from": "assistant",
            "value": "from astropy.coordinates import SkyCoord\nfrom astropy import units as u\n\ndef convert_equatorial_coords(ra_str, dec_str):\n    \"\"\"\n    Convert RA (HH:MM:SS) and Dec (DD:MM:SS) strings to SkyCoord object\n    \n    Parameters:\n    ra_str: str - Right Ascension in format \"HH:MM:SS\" or \"HH MM SS\"\n    dec_str: str - Declination in format \"DD:MM:SS\" or \"DD MM SS\"\n    \n    Returns:\n    SkyCoord object with converted coordinates\n    \"\"\"\n    try:\n        # Create SkyCoord object from string coordinates\n        coords = SkyCoord(ra_str, dec_str, frame='icrs', \n                         unit=(u.hourangle, u.deg))\n        \n        # Return various useful formats\n        return {\n            'skycoord': coords,\n            'ra_deg': coords.ra.deg,    # RA in degrees\n            'dec_deg': coords.dec.deg,  # Dec in degrees\n            'ra_rad': coords.ra.rad,    # RA in radians\n            'dec_rad': coords.dec.rad   # Dec in radians\n        }\n    except Exception as e:\n        print(f\"Error converting coordinates: {e}\")\n        return None\n\n# Example usage\nra = \"14:29:43.0\"    # Arbitrary star RA\ndec = \"-62:40:46.0\"  # Arbitrary star Dec\n\nresult = convert_equatorial_coords(ra, dec)\nif result:\n    print(f\"RA (deg): {result['ra_deg']:.4f}\")\n    print(f\"Dec (deg): {result['dec_deg']:.4f}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "For my pulsar timing analysis, I need to combine observation dates and times currently stored as separate year, month, day, hour, minute values into a standardized ISO timestamp format. What's the best way to do this?"
         },
         {
            "from": "assistant",
            "value": "from astropy.time import Time\nimport numpy as np\n\n# Example data (replace with your actual data)\nyears = [2023, 2023, 2023]\nmonths = [3, 4, 5]\ndays = [15, 21, 7]\nhours = [14, 8, 22]\nminutes = [30, 45, 15]\n\ndef combine_datetime_to_iso(years, months, days, hours, minutes):\n    \"\"\"\n    Combines separate date/time components into ISO format timestamps\n    using Astropy Time objects\n    \"\"\"\n    # Create datetime strings in ISO format\n    datetime_strings = [f\"{y:04d}-{m:02d}-{d:02d}T{h:02d}:{min:02d}:00\" \n                       for y, m, d, h, min in zip(years, months, days, hours, minutes)]\n    \n    # Convert to Astropy Time objects\n    times = Time(datetime_strings, format='isot', scale='utc')\n    \n    return times\n\n# Convert the dates and get ISO format\ntimes = combine_datetime_to_iso(years, months, days, hours, minutes)\n\n# Print both ISO and MJD format (commonly used in pulsar timing)\nprint(\"ISO timestamps:\", times.isot)\nprint(\"MJD values:\", times.mjd)"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm comparing atmospheric seeing conditions at different potential sites for a new telescope. How do I create EarthLocation objects for several mountain peaks above 3000m elevation?"
         },
         {
            "from": "assistant",
            "value": "from astropy.coordinates import EarthLocation\nimport astropy.units as u\n\n# Define dictionary of high-altitude observatory sites (lat, lon, height)\nmountain_sites = {\n    'Mauna Kea': (19.8208, -155.4681, 4207.0),  # Hawaii\n    'Cerro Paranal': (-24.6275, -70.4044, 3046.0),  # Chile\n    'Sierra Negra': (18.9861, -97.3147, 4580.0),  # Mexico\n    'Mount Graham': (32.7013, -109.8891, 3191.0)  # Arizona\n}\n\n# Create EarthLocation objects for each site\nobservatory_locations = {}\nfor site_name, coords in mountain_sites.items():\n    observatory_locations[site_name] = EarthLocation(\n        lat=coords[0]*u.deg,\n        lon=coords[1]*u.deg,\n        height=coords[2]*u.m\n    )\n\n# Example: Print geocentric coordinates for each site\nfor site_name, location in observatory_locations.items():\n    print(f\"{site_name}: {location.geodetic}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "Working with historical astronomical observations from Greenwich (UTC+0) and converting them to local time in Sydney (UTC+10). How can I use TimezoneInfo to convert a specific observation time '2023-05-12 18:20:00' between these time zones?"
         },
         {
            "from": "assistant",
            "value": "from astropy.time import Time, TimezoneInfo\nfrom datetime import datetime\nfrom astropy import units as u\n\n# Create timezone objects for Greenwich (UTC+0) and Sydney (UTC+10)\nutc = TimezoneInfo(utc_offset=0*u.hour)  # UTC offset using astropy units\nsydney_tz = TimezoneInfo(utc_offset=10*u.hour)  # UTC+10\n\n# Create Time object for the observation time in Greenwich\ngreenwich_time = Time('2023-05-12 18:20:00', format='iso', scale='utc',\n                     location=(-0.0014, 51.4778))  # Greenwich coordinates\n\n# Convert to Sydney time by adding the time difference\n# Updated format string to include microseconds\nsydney_datetime = datetime.strptime(greenwich_time.iso, '%Y-%m-%d %H:%M:%S.%f')\nsydney_datetime = sydney_datetime.replace(tzinfo=utc)\nsydney_local = sydney_datetime.astimezone(sydney_tz)\n\nprint(f\"Greenwich time: {greenwich_time.iso}\")\nprint(f\"Sydney local time: {sydney_local.strftime('%Y-%m-%d %H:%M:%S %Z')}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm analyzing a spectral line profile and need to fit a Gaussian model to it. Can you show me how to create a Gaussian1D model with amplitude=5.2, mean=1550.3, and standard deviation=2.8, then evaluate it over wavelengths from 1540 to 1560 nm with 0.1 nm steps?"
         },
         {
            "from": "assistant",
            "value": "# Import required modules from AstroPy\nfrom astropy.modeling import models\nimport numpy as np\n\n# Create wavelength array (1540 to 1560 nm in 0.1 nm steps)\nwavelengths = np.arange(1540, 1560.1, 0.1)\n\n# Create Gaussian1D model with specified parameters\n# amplitude=5.2 (intensity units)\n# mean=1550.3 (nm)\n# stddev=2.8 (nm)\ngaussian_model = models.Gaussian1D(amplitude=5.2, mean=1550.3, stddev=2.8)\n\n# Evaluate the model over the wavelength range\nintensities = gaussian_model(wavelengths)\n\n# Example of how to use the results\nfrom matplotlib import pyplot as plt\nplt.plot(wavelengths, intensities)\nplt.xlabel('Wavelength (nm)')\nplt.ylabel('Intensity')\nplt.title('Gaussian Spectral Line Profile')"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm analyzing stellar rotation data and need to work with both angular velocities and frequencies. How do I set up my units system to handle automatic conversions between these quantities?"
         },
         {
            "from": "assistant",
            "value": "from astropy import units as u\nfrom astropy.units import cds\nimport numpy as np\n\n# Define rpm unit\nrpm = u.def_unit('rpm', 2 * np.pi * u.rad / (60 * u.s))\nu.add_enabled_units([rpm])\n\n# Set up example rotational quantities\nperiod = 25.4 * u.day  # Rotation period like the Sun\n# Use equivalencies for angular frequency conversions\nomega = (2*np.pi/period).to(u.rad/u.s, equivalencies=u.dimensionless_angles())  # Angular velocity\nfreq = (1/period).to(u.Hz)  # Frequency\n\n# Create a small function to demonstrate conversions\ndef convert_rotation_units(value, from_unit):\n    \"\"\"Convert between different rotational units.\"\"\"\n    # Create quantity with input units\n    quantity = value * from_unit\n    \n    # Convert to common rotational units using equivalencies\n    print(f\"Original: {quantity}\")\n    print(f\"Angular velocity (rad/s): {quantity.to(u.rad/u.s, equivalencies=u.dimensionless_angles())}\")\n    print(f\"Frequency (Hz): {quantity.to(u.Hz, equivalencies=u.dimensionless_angles())}\")\n    print(f\"Period (days): {(1/quantity).to(u.day, equivalencies=u.dimensionless_angles())}\")\n    print(f\"Angular velocity (rpm): {quantity.to(rpm, equivalencies=u.dimensionless_angles())}\")\n\n# Example usage\nprint(\"Solar-like rotation example:\")\nconvert_rotation_units(2.84e-6, u.rad/u.s)  # Approximate solar rotation"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "Working on a black hole simulation - need to convert between solar masses and their energy equivalents in ergs. What's the code to set up this conversion?"
         },
         {
            "from": "assistant",
            "value": "from astropy import constants as const\nfrom astropy import units as u\n\n# Define conversion function for solar mass to energy\ndef mass_to_energy(mass_solar):\n    \"\"\"\n    Convert solar masses to rest mass energy in ergs\n    Using E = mc^2 relationship\n    \n    Parameters:\n        mass_solar: float or array\n            Mass in units of solar masses\n    Returns:\n        Energy in ergs\n    \"\"\"\n    # Convert input solar masses to grams first\n    mass_g = (mass_solar * const.M_sun).to(u.g)\n    \n    # Calculate energy using E = mc^2\n    energy = (mass_g * const.c * const.c).to(u.erg)\n    \n    return energy\n\n# Example usage\nmass = 10  # 10 solar masses\nenergy = mass_to_energy(mass)\nprint(f\"{mass} solar masses = {energy:.2e}\")\n\n# For reverse conversion (energy to mass)\nmass_from_energy = (energy / (const.c * const.c)).to(u.solMass)\nprint(f\"{energy:.2e} ergs = {mass_from_energy:.2f}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I have an array of planetary phase angles arranged in a 2x3 matrix: [[30, 45, 60], [120, 150, 180]] degrees. How can I compute the circular mean along the rows (axis=1) to get the average phase angle for each set?"
         },
         {
            "from": "assistant",
            "value": "from astropy.stats import circmean\nimport numpy as np\nfrom astropy import units as u\n\n# Create the 2x3 matrix of phase angles\nphase_angles = np.array([[30, 45, 60], \n                        [120, 150, 180]])\n\n# Compute circular mean along rows (axis=1)\n# Convert to radians for computation as circmean expects radians\nmeans = circmean(phase_angles*np.pi/180, axis=1)\n\n# Convert results back to degrees\nmeans_deg = means * 180/np.pi\n\nprint(f\"Circular means (degrees): {means_deg}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm preparing an astronomical observation log that needs to follow IAU standards. How can I represent my observation start and end times in ISO 8601 format using Astropy?"
         },
         {
            "from": "assistant",
            "value": "from astropy.time import Time\nfrom datetime import datetime\n\n# Create sample observation start and end times\nobs_start = Time(datetime.utcnow())  # Current time as start\nobs_end = Time(datetime.utcnow()).mjd + 2/24  # 2 hours later\nobs_end = Time(obs_end, format='mjd')  # Convert back to Time object\n\n# Format times in ISO 8601 format (IAU standard)\niso_start = obs_start.iso\niso_end = obs_end.iso\n\n# Print formatted times\nprint(f\"Observation Start (ISO): {iso_start}\")\nprint(f\"Observation End (ISO):   {iso_end}\")\n\n# For higher precision, you can use isot format\nhigh_precision_start = obs_start.isot\nprint(f\"\\nHigh Precision Start (ISOT): {high_precision_start}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "With stellar temperature measurements [5200, 5150, 5300, 5250, 5180] K and luminosity values [1.2, 1.1, 1.3, 1.25, 1.15] solar luminosities, how do I use LevMarLSQFitter to fit a polynomial model and calculate the parameter uncertainties?"
         },
         {
            "from": "assistant",
            "value": "from astropy.modeling import models, fitting\nfrom astropy.modeling.fitting import LevMarLSQFitter\nimport numpy as np\n\n# Define the data\ntemperature = np.array([5200, 5150, 5300, 5250, 5180])\nluminosity = np.array([1.2, 1.1, 1.3, 1.25, 1.15])\n\n# Assume 2% uncertainty in measurements for demonstration\nluminosity_errors = luminosity * 0.02\n\n# Initialize the polynomial model (2nd degree)\npoly_model = models.Polynomial1D(degree=2)\n\n# Initialize the fitter\nfitter = LevMarLSQFitter()\n\n# Fit the model to the data\nfitted_model = fitter(poly_model, temperature, luminosity, weights=1/luminosity_errors)\n\n# Get the parameter uncertainties from the covariance matrix\n# The diagonal elements of the covariance matrix are the variances of parameters\nparam_uncertainties = np.sqrt(np.diag(fitter.fit_info['param_cov']))\n\n# Print results\nprint(\"Fit parameters:\", fitted_model.parameters)\nprint(\"Parameter uncertainties:\", param_uncertainties)\n\n# Calculate R-squared to evaluate fit quality\nresiduals = luminosity - fitted_model(temperature)\nss_res = np.sum(residuals**2)\nss_tot = np.sum((luminosity - np.mean(luminosity))**2)\nr_squared = 1 - (ss_res / ss_tot)\nprint(f\"R-squared: {r_squared:.4f}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm analyzing stellar data from a FITS file at \"/home/data/stars.fits\" and need both the data and header from extension number 2. The data contains photometry measurements. Show me the code to retrieve both."
         },
         {
            "from": "assistant",
            "value": "from astropy.io import fits\nimport os\n\n# Define the file path - replace this with your actual FITS file path\n# Example paths:\n# fits_file = 'stars.fits'  # if file is in current directory\n# fits_file = os.path.expanduser('~/data/stars.fits')  # using home directory\nfits_file = './stars.fits'  # in current working directory\n\n# Check if file exists before attempting to open it\nif not os.path.exists(fits_file):\n    print(f\"The FITS file does not exist at: {fits_file}\")\n    print(\"Current working directory:\", os.getcwd())\n    print(\"Please check the file path and ensure the file is accessible.\")\n    exit(1)\n\ntry:\n    # Open the FITS file and access extension #2\n    # hdul.info() can be used to see available extensions if needed\n    with fits.open(fits_file) as hdul:\n        # Get both data and header from extension 2\n        # Note: Extension counting starts from 0, so 2 means third extension\n        data = hdul[2].data  # Get the data array containing photometry\n        header = hdul[2].header  # Get the header of extension 2\n\n    # At this point:\n    # - 'data' contains the photometry measurements array\n    # - 'header' contains metadata about the measurements\n\n    # File is automatically closed after 'with' block\n\nexcept IndexError:\n    print(f\"Error: Extension #2 not found in the FITS file.\")\n    print(\"Available extensions:\")\n    with fits.open(fits_file) as hdul:\n        hdul.info()\nexcept Exception as e:\n    print(f\"An error occurred while reading the FITS file: {str(e)}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm analyzing supernova light curves and need to apply a distance-dependent scaling to account for the inverse square law. How can I implement this using Astropy's modeling capabilities?"
         },
         {
            "from": "assistant",
            "value": "from astropy.modeling import Model, Parameter\nfrom astropy import units as u\nimport numpy as np\n\nclass InverseSquareLightCurve(Model):\n    \"\"\"Custom model for scaling light curve by inverse square law\"\"\"\n    \n    # Define parameters\n    amplitude = Parameter(default=1.0)  # Base amplitude\n    distance = Parameter(default=10.0)  # Distance in parsecs\n    \n    # Define input name(s)\n    inputs = ('time',)\n    # Define output name(s)\n    outputs = ('flux',)\n    \n    @staticmethod\n    def evaluate(time, amplitude, distance):\n        \"\"\"\n        Scale flux by inverse square of distance\n        \n        Parameters:\n        -----------\n        time : array\n            Time points of light curve\n        amplitude : float\n            Initial flux amplitude\n        distance : float\n            Distance to source in parsecs\n        \"\"\"\n        # Convert distance to physical units\n        d = distance * u.parsec\n        \n        # Calculate scaling factor (1/d\u00b2)\n        scaling = (1 / d.to(u.parsec).value**2)\n        \n        # Return scaled light curve\n        return amplitude * scaling * np.ones_like(time)\n\n# Example usage\ntime_points = np.linspace(0, 100, 1000)  # 100 days of observations\nmodel = InverseSquareLightCurve(amplitude=1e-12, distance=50.0)\nscaled_flux = model(time_points)"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm creating an educational tool to teach astronomy and want to display both the full name and abbreviated versions of constellations for a list of celestial objects. How can I write code to accomplish this?"
         },
         {
            "from": "assistant",
            "value": "from astropy.coordinates import SkyCoord\nfrom astropy.coordinates import get_constellation\n\n# Create example coordinates for some celestial objects\nobjects = [\n    SkyCoord('05h34m31.94s', '+22d00m52.2s'),  # Pleiades\n    SkyCoord('05h55m10.31s', '+07d24m25.4s'),  # Betelgeuse\n    SkyCoord('14h39m36.49s', '-60d50m02.3s'),  # Alpha Centauri\n]\n\n# Get both full names and abbreviations for each object\nfor obj in objects:\n    const_abbrev = get_constellation(obj)\n    const_full = get_constellation(obj, short_name=False)\n    \n    print(f\"Object at {obj.to_string('hmsdms', precision=0)}\")\n    print(f\"Constellation: {const_full} ({const_abbrev})\\n\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm studying variable stars and need to determine period changes. How can I calculate time differences between magnitude peaks and convert them into different time scales for analysis?"
         },
         {
            "from": "assistant",
            "value": "from astropy.time import Time\nimport numpy as np\n\n# Sample peak times in JD\npeak_times_jd = [2458765.4, 2458767.9, 2458770.4, 2458772.9]\n\n# Create Time object for the peaks\npeaks = Time(peak_times_jd, format='jd')\n\n# Calculate time differences between consecutive peaks\ntime_diffs = np.diff(peaks.jd)  # in days\n\n# Convert peak times to different time scales\nmjd_peaks = peaks.mjd  # Modified Julian Date\nunix_peaks = peaks.unix  # Unix timestamp\niso_peaks = peaks.iso   # ISO format\n\n# Calculate average period\navg_period = np.mean(time_diffs)\n\nprint(f\"Time differences between peaks (days): {time_diffs}\")\nprint(f\"Average period: {avg_period:.2f} days\")\nprint(f\"\\nPeak times in different formats:\")\nprint(f\"MJD: {mjd_peaks}\")\nprint(f\"Unix time: {unix_peaks}\")\nprint(f\"ISO: {iso_peaks}\")\n\n# Calculate O-C (Observed minus Calculated) values\nepoch_numbers = np.arange(len(peak_times_jd))\ncalculated_times = peak_times_jd[0] + epoch_numbers * avg_period\no_minus_c = peaks.jd - calculated_times\nprint(f\"\\nO-C values (days): {o_minus_c}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm studying a galaxy cluster and need to get the observation date (keyword 'DATE-OBS') from my FITS file \"galaxy_cluster_a1689.fits\". What's the code to do this?"
         },
         {
            "from": "assistant",
            "value": "from astropy.io import fits\nimport os\n\n# Define the FITS filename\nfits_file = 'galaxy_cluster_a1689.fits'\n\ntry:\n    # Check if file exists before attempting to open it\n    if not os.path.exists(fits_file):\n        print(f\"Error: The file '{fits_file}' does not exist in the current directory.\")\n        print(f\"Current working directory: {os.getcwd()}\")\n    else:\n        # Open the FITS file\n        with fits.open(fits_file) as hdul:\n            # Get the observation date from the primary header\n            obs_date = hdul[0].header['DATE-OBS']\n            print(f\"Observation Date: {obs_date}\")\n\nexcept Exception as e:\n    print(f\"An error occurred while processing the FITS file: {str(e)}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm working with data from a stellar stream survey. How can I create a Cartesian representation of the stream stars' positions to analyze their spatial distribution and include proper motions as differentials?"
         },
         {
            "from": "assistant",
            "value": "from astropy.coordinates import SkyCoord, CartesianRepresentation, CartesianDifferential\nimport astropy.units as u\nimport numpy as np\n\n# Example data for a stellar stream (replace with your actual data)\nra = [120.5, 121.0, 121.5] * u.degree  # Right Ascension\ndec = [-45.2, -45.3, -45.4] * u.degree # Declination\ndistance = [1500, 1520, 1540] * u.pc    # Distance\npm_ra = [2.5, 2.6, 2.4] * u.mas/u.yr   # Proper motion in RA\npm_dec = [-1.8, -1.7, -1.9] * u.mas/u.yr # Proper motion in Dec\n\n# Create SkyCoord object with proper motions\nstream_stars = SkyCoord(\n    ra=ra, \n    dec=dec, \n    distance=distance,\n    pm_ra_cosdec=pm_ra,  # Include proper motion in RA (with cos(dec) factor)\n    pm_dec=pm_dec,       # Include proper motion in Dec\n    frame='icrs'\n)\n\n# Convert to Cartesian representation\ncart_data = stream_stars.cartesian\n\n# Create differentials from proper motions (automatically handled in transform)\nstream_stars_with_pm = stream_stars.cartesian.with_differentials(\n    stream_stars.cartesian.differentials\n)\n\n# Access positions and velocities\nx = cart_data.x\ny = cart_data.y\nz = cart_data.z\nv_x = stream_stars_with_pm.differentials['s'].d_x\nv_y = stream_stars_with_pm.differentials['s'].d_y\nv_z = stream_stars_with_pm.differentials['s'].d_z"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I have an image from a ground-based telescope with known WCS information, and I need help writing code to convert pixel coordinates from the center region of the image into actual sky coordinates (RA/Dec). How can I do this?"
         },
         {
            "from": "assistant",
            "value": "from astropy.wcs import WCS\nfrom astropy.io import fits\nimport os\nimport sys\n\ndef get_image_center_coordinates(fits_file):\n    try:\n        # Check if file exists\n        if not os.path.exists(fits_file):\n            raise FileNotFoundError(f\"The FITS file '{fits_file}' does not exist!\")\n\n        # Load the FITS image and WCS information\n        with fits.open(fits_file) as hdul:\n            hdu = hdul[0]  # Assuming the image is in the primary HDU\n            wcs = WCS(hdu.header)\n\n            # Get image dimensions\n            image_height, image_width = hdu.data.shape\n\n            # Define center pixel coordinates (0-based indexing)\n            center_x = image_width // 2\n            center_y = image_height // 2\n\n            # Convert pixel coordinates to world coordinates (RA, Dec)\n            # pixel_to_world expects coordinates in (x,y) order\n            sky_coords = wcs.pixel_to_world(center_x, center_y)\n\n            # Extract RA and Dec\n            ra = sky_coords.ra  # Right Ascension\n            dec = sky_coords.dec  # Declination\n\n            print(f\"Center pixel coordinates (x,y): ({center_x}, {center_y})\")\n            print(f\"Sky coordinates: RA = {ra.deg:.6f}\u00b0, Dec = {dec.deg:.6f}\u00b0\")\n\n    except FileNotFoundError as e:\n        print(f\"Error: {e}\")\n        sys.exit(1)\n    except Exception as e:\n        print(f\"An error occurred: {str(e)}\")\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    # Replace this with your actual FITS file path\n    fits_file = \"path/to/your/image.fits\"\n    get_image_center_coordinates(fits_file)"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "In my spectroscopic data reduction pipeline, I need to smooth calibration spectra. How can I apply a Box1DKernel with width=9 using 'center' mode to my wavelength-calibrated array 'calib_spectrum', ensuring the kernel is properly normalized?"
         },
         {
            "from": "assistant",
            "value": "from astropy.convolution import Box1DKernel, convolve\nimport numpy as np\n\n# Create some sample spectrum data\n# For example, a simple gaussian spectrum with noise\nx = np.linspace(-10, 10, 100)\ncalib_spectrum = np.exp(-0.5 * x**2) + np.random.normal(0, 0.1, 100)\n\n# Create a Box1DKernel with width=9 pixels\nkernel = Box1DKernel(width=9)\n\ntry:\n    # Apply convolution with normalized kernel and 'center' boundary handling\n    smoothed_spectrum = convolve(\n        calib_spectrum,\n        kernel,\n        boundary='extend',  # 'extend' mode for handling edges\n        normalize_kernel=True  # Ensure kernel is normalized\n    )\nexcept ValueError as e:\n    print(f\"Error during convolution: {e}\")\n    smoothed_spectrum = None\n\n# The smoothed_spectrum now contains the convolved data\n# with proper edge handling and normalized kernel\n\n# Optional: Print or plot results\nif smoothed_spectrum is not None:\n    print(\"Convolution successful\")\n    print(\"Original spectrum shape:\", calib_spectrum.shape)\n    print(\"Smoothed spectrum shape:\", smoothed_spectrum.shape)"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm analyzing the pressure in interstellar medium bubbles with a value of 3.7e-8 Barye. How can I convert this to Pascals using astropy's CGS units for comparison with standard literature values?"
         },
         {
            "from": "assistant",
            "value": "from astropy import units as u\n\n# Define the pressure in Barye (CGS unit of pressure)\npressure_barye = 3.7e-8 * u.barye\n\n# Convert to Pascals (SI unit)\npressure_pascal = pressure_barye.to(u.Pa)\n\n# Print both values for comparison\nprint(f\"Pressure in CGS (Barye): {pressure_barye:.2e}\")\nprint(f\"Pressure in SI (Pascal): {pressure_pascal:.2e}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "Could you help me create a code snippet to smooth out the noise in my galaxy image while preserving the overall structure? I want to use a Gaussian filter for this purpose."
         },
         {
            "from": "assistant",
            "value": "from astropy.io import fits\nfrom astropy.convolution import Gaussian2DKernel, convolve\nimport numpy as np\n\ndef smooth_galaxy_image(image_path, kernel_stddev=2.0):\n    \"\"\"\n    Smooth a galaxy image using Gaussian convolution\n    \n    Parameters:\n    image_path: str - Path to the FITS image file\n    kernel_stddev: float - Standard deviation for Gaussian kernel\n    \"\"\"\n    try:\n        # Load the image data from FITS file\n        with fits.open(image_path) as hdul:\n            data = hdul[0].data\n        \n        # Create a 2D Gaussian kernel\n        kernel = Gaussian2DKernel(stddev=kernel_stddev)\n        \n        # Convolve the image with the Gaussian kernel\n        smoothed_data = convolve(data, kernel)\n        \n        # Save the smoothed image\n        hdu = fits.PrimaryHDU(smoothed_data)\n        hdu.writeto('smoothed_galaxy.fits', overwrite=True)\n        \n        return smoothed_data\n        \n    except Exception as e:\n        print(f\"Error processing image: {str(e)}\")\n        return None\n\n# Example usage\n# smoothed_image = smooth_galaxy_image('galaxy.fits', kernel_stddev=2.0)"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm studying globular clusters and need to highlight the dense stellar cores while preserving the details in the outer regions. Can you provide code to adjust my image's contrast using a power-law transformation?"
         },
         {
            "from": "assistant",
            "value": "from astropy.io import fits\nfrom astropy.visualization import PowerStretch, ImageNormalize\nimport numpy as np\n\n# Read the FITS image\ndef enhance_globular_cluster(filename, power=0.5):\n    \"\"\"\n    Enhance globular cluster image using power-law stretch.\n    power < 1 enhances faint features while preserving bright core details\n    \"\"\"\n    try:\n        # Load the image data\n        with fits.open(filename) as hdul:\n            image_data = hdul[0].data\n            \n        # Create normalization with power-law stretch\n        norm = ImageNormalize(\n            image_data,\n            stretch=PowerStretch(power),\n            clip=False\n        )\n        \n        # Apply the transformation\n        enhanced_image = norm(image_data)\n        \n        return enhanced_image\n        \n    except Exception as e:\n        print(f\"Error processing image: {e}\")\n        return None\n\n# Example usage\nif __name__ == \"__main__\":\n    enhanced = enhance_globular_cluster(\"globular_cluster.fits\", power=0.5)\n    \n    if enhanced is not None:\n        # Save the enhanced image\n        fits.writeto('enhanced_cluster.fits', enhanced, overwrite=True)"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm working with unsigned 16-bit integer data in my FITS file of solar observations, where BZERO=32768 and BSCALE=1. I need to read this data while ensuring it's properly interpreted as uint16. How can I open this FITS file with the correct integer interpretation?"
         },
         {
            "from": "assistant",
            "value": "from astropy.io import fits\nimport numpy as np\nimport os\n\n# Define the fits file path\nfits_file = 'solar_data.fits'\n\n# Check if file exists before trying to open it\nif not os.path.exists(fits_file):\n    print(f\"Error: The file '{fits_file}' does not exist in the current directory.\")\n    print(f\"Current working directory: {os.getcwd()}\")\nelse:\n    try:\n        # Open the FITS file with correct uint16 interpretation\n        with fits.open(fits_file, uint=True) as hdul:\n            # Get the primary data array\n            # uint=True ensures BZERO/BSCALE are properly applied for unsigned integers\n            data = hdul[0].data\n            \n            # Verify the data type is uint16\n            print(f\"Data type: {data.dtype}\")\n            \n            # Optional: Get BZERO and BSCALE values from header\n            bzero = hdul[0].header.get('BZERO', 0)\n            bscale = hdul[0].header.get('BSCALE', 1)\n            print(f\"BZERO={bzero}, BSCALE={bscale}\")\n            \n            # Now 'data' contains the unsigned 16-bit values correctly scaled\n            # The actual pixel values will be: (raw_data * BSCALE) + BZERO\n            \n    except Exception as e:\n        print(f\"An error occurred while reading the FITS file: {str(e)}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "During my variable star research, I want to determine the constellations for multiple stars simultaneously. How can I use get_constellation to find the constellations for these three stars: (RA=[123.45, 67.89, 246.81] deg, Dec=[45.67, -12.34, 78.90] deg)?"
         },
         {
            "from": "assistant",
            "value": "from astropy.coordinates import SkyCoord, get_constellation\nfrom astropy import units as u\n\n# Create a list of coordinates for multiple stars\ncoords = SkyCoord(ra=[123.45, 67.89, 246.81]*u.deg, \n                 dec=[45.67, -12.34, 78.90]*u.deg, \n                 frame='icrs')\n\n# Get constellations for all stars at once\nconstellations = get_constellation(coords)\n\n# Print results in a readable format\nfor i, (coord, const) in enumerate(zip(coords, constellations)):\n    print(f\"Star {i+1}: RA={coord.ra.deg:.2f}\u00b0, Dec={coord.dec.deg:.2f}\u00b0 \u2192 {const}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm studying solar irradiance data and need to convert the measured solar flux to Earth-equivalent units. How can I scale these measurements using the inverse square law distance factor?"
         },
         {
            "from": "assistant",
            "value": "from astropy.coordinates import get_body, get_sun, EarthLocation\nfrom astropy.time import Time\nimport astropy.units as u\nimport numpy as np\n\ndef scale_solar_flux(flux, observation_time, observer_distance):\n    \"\"\"\n    Scale solar flux measurements to Earth-equivalent units using inverse square law\n    \n    Parameters:\n    flux: Solar flux measurement at observer location\n    observation_time: Time of observation\n    observer_distance: Distance from Sun to observer in AU\n    \"\"\"\n    \n    # Get Sun-Earth distance at observation time\n    earth = get_body('earth', observation_time)\n    sun = get_sun(observation_time)\n    earth_sun_dist = earth.separation_3d(sun).to(u.au)\n    \n    # Calculate scaling factor using inverse square law\n    # flux \u221d 1/r^2 where r is distance from Sun\n    scale_factor = (observer_distance / earth_sun_dist)**2\n    \n    # Scale the flux to Earth-equivalent\n    earth_equivalent_flux = flux * scale_factor\n    \n    return earth_equivalent_flux\n\n# Example usage\ntime = Time('2023-01-01 12:00:00')\nmeasured_flux = 1360 * u.W/u.m**2  # Example solar flux measurement\nobserver_dist = 0.7 * u.au  # Example: Observer at Venus orbital distance\n\nearth_flux = scale_solar_flux(measured_flux, time, observer_dist)\nprint(f\"Earth-equivalent flux: {earth_flux:.2f}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I need to work with historical variable star observations that use Besselian epochs. How do I convert these timestamps to a modern time format for my analysis?"
         },
         {
            "from": "assistant",
            "value": "from astropy.time import Time\nimport numpy as np\n\n# Example Besselian epoch timestamps\nbesselian_epochs = [1950.0, 1950.308, 1950.5]  # Example B1950.0 and some dates after\n\n# Convert Besselian epochs to Time objects\n# 'b' format specifier indicates Besselian epoch\ntimes = Time(besselian_epochs, format='byear')\n\n# Convert to different modern time formats\niso_times = times.iso  # ISO format (YYYY-MM-DD HH:MM:SS)\njd = times.jd         # Julian Date\nmjd = times.mjd       # Modified Julian Date\n\n# Print conversions\nfor i, bep in enumerate(besselian_epochs):\n    print(f\"Besselian epoch: {bep}\")\n    print(f\"ISO date: {iso_times[i]}\")\n    print(f\"Julian Date: {jd[i]}\")\n    print(f\"Modified JD: {mjd[i]}\")\n    print(\"---\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "For my exoplanet atmosphere study, I have several molecular absorption lines in wavenumbers (spectroscopic, in cm^-1). I need code to convert the wavenumber 2143.27 cm^-1 to its corresponding frequency in TeraHertz."
         },
         {
            "from": "assistant",
            "value": "# Import necessary constants and units from astropy\nfrom astropy import units as u\nfrom astropy import constants as const\n\n# Define the wavenumber in cm^-1\nwavenumber = 2143.27 * u.cm**(-1)\n\n# Convert wavenumber to frequency\n# Using formula: frequency = wavenumber * speed_of_light\n# Note: AstroPy automatically handles unit conversion to THz\nfrequency = (wavenumber * const.c).to(u.THz)\n\nprint(f\"Wavenumber {wavenumber} corresponds to frequency: {frequency:.3f}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm analyzing data from a variable star monitoring campaign and need to plot the light curve with time in MJD format. What's the code to properly format the time axis?"
         },
         {
            "from": "assistant",
            "value": "# Import required astropy modules\nfrom astropy.time import Time\nimport astropy.units as u\nimport matplotlib.pyplot as plt\n\n# Sample data (replace with your actual data)\nmjd_times = [58934.2, 58934.5, 58934.8, 58935.1, 58935.4]  \nmagnitudes = [12.3, 12.5, 12.2, 12.4, 12.3]\n\n# Convert MJD to astropy Time object\nt = Time(mjd_times, format='mjd')\n\n# Create the light curve plot\nplt.figure(figsize=(10,6))\nplt.plot(mjd_times, magnitudes, 'ko-', markersize=4)\n\n# Format the time axis\nplt.xlabel('Time (MJD)')\nplt.ylabel('Magnitude')\n\n# Invert y-axis (standard for magnitude plots)\nplt.gca().invert_yaxis()\n\n# Add grid for better readability\nplt.grid(True, linestyle='--', alpha=0.7)\n\n# Optional: Add date labels on secondary x-axis\nax2 = plt.gca().twiny()\nax2.set_xlim(plt.gca().get_xlim())\n\n# Convert Time objects to datetime and create labels\ndate_labels = [dt.strftime('%Y-%m-%d') for dt in t.datetime]\nax2.set_xticks(mjd_times)\nax2.set_xticklabels(date_labels, rotation=45)\n\n# Adjust layout to prevent label cutoff\nplt.tight_layout()\nplt.show()"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm creating an interactive sky map and need to determine the constellation names for a list of gamma-ray bursts. Write code to find the constellations for coordinates: RA=[\"12:29:06.70\", \"15:45:30.25\"], Dec=[\"+02:03:08.5\", \"-40:15:35.8\"] using astropy's get_constellation function."
         },
         {
            "from": "assistant",
            "value": "from astropy.coordinates import SkyCoord\nfrom astropy.coordinates import get_constellation\nimport astropy.units as u\n\n# Create lists of RA and Dec coordinates\nra_list = [\"12:29:06.70\", \"15:45:30.25\"]\ndec_list = [\"+02:03:08.5\", \"-40:15:35.8\"]\n\n# Convert strings to SkyCoord objects\n# Using ICRS frame which is the standard reference system for celestial coordinates\ncoords = SkyCoord(ra=ra_list, dec=dec_list, unit=(u.hourangle, u.deg), frame='icrs')\n\n# Get constellation for each coordinate pair\nconstellations = get_constellation(coords)\n\n# Print results\nfor i, (ra, dec, const) in enumerate(zip(ra_list, dec_list, constellations)):\n    print(f\"GRB {i+1} at RA={ra}, Dec={dec} is in the constellation {const}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "How should I set up my code to properly handle timing of a simultaneous multi-wavelength observation campaign coordinated between observatories in Hawaii and Chile?"
         },
         {
            "from": "assistant",
            "value": "from astropy.time import Time, TimeDelta\nfrom astropy.coordinates import EarthLocation, AltAz, SkyCoord\nimport astropy.units as u\nimport numpy as np\n\n# Define observatory locations using explicit coordinates\nmauna_kea = EarthLocation(\n    lat=19.8208*u.deg,\n    lon=-155.4681*u.deg,\n    height=4207.0*u.m\n)\n\nparanal = EarthLocation(\n    lat=-24.6275*u.deg,\n    lon=-70.4044*u.deg,\n    height=2635.0*u.m\n)\n\n# Set up observation parameters\nstart_time = Time('2024-01-15 00:00:00')  # UTC\nduration = TimeDelta(3600 * 4 * u.s)  # 4 hour observation\ncadence = TimeDelta(300 * u.s)  # 5 minute intervals\n\n# Create time array for coordination\ntimes = start_time + TimeDelta(np.arange(0, duration.sec, cadence.sec) * u.s)\n\n# Example target coordinates\ntarget = SkyCoord('12h34m56.7s -45d23m01s')\n\ndef check_visibility(location, times, target):\n    \"\"\"Check target visibility from given location across time range\"\"\"\n    altaz_frames = [AltAz(obstime=t, location=location) for t in times]\n    target_altaz = [target.transform_to(frame) for frame in altaz_frames]\n    \n    # Check if target is observable (above 30 degrees altitude)\n    observable = [altaz.alt > 30*u.deg for altaz in target_altaz]\n    return observable\n\n# Check visibility at both sites\nhawaii_visible = check_visibility(mauna_kea, times, target)\nchile_visible = check_visibility(paranal, times, target)\n\n# Find simultaneous observation windows\nsimultaneous = [h and c for h, c in zip(hawaii_visible, chile_visible)]\n\n# Convert times to UTC (removing timezone conversion as it's not directly supported)\nhawaii_times = times.datetime\nchile_times = times.datetime\n\n# Print results\nfor t, h, c, s in zip(times, hawaii_visible, chile_visible, simultaneous):\n    print(f\"Time: {t.iso}, Hawaii: {h}, Chile: {c}, Simultaneous: {s}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I have spectral data showing an emission line that appears to be Gaussian-shaped. How can I model this line using Astro-py to determine its peak intensity and central wavelength?"
         },
         {
            "from": "assistant",
            "value": "import numpy as np\nfrom astropy.modeling import models, fitting\nfrom astropy import units as u\n\n# Create sample spectral data (replace this with your actual data)\nwavelength = np.linspace(6550, 6570, 100) * u.angstrom\n# Simulate noisy Gaussian data\ntrue_center = 6563 # H-alpha wavelength\nflux = 3e-14 * np.exp(-(wavelength.value - true_center)**2/(2*2**2))\nflux += np.random.normal(0, 0.1e-14, wavelength.shape)\n\n# Initialize a Gaussian model\ng_init = models.Gaussian1D(\n    amplitude=np.max(flux), \n    mean=6563,  # Initial guess for center\n    stddev=2    # Initial guess for width\n)\n\n# Fit the model to the data\nfitter = fitting.LevMarLSQFitter()\ng_fit = fitter(g_init, wavelength.value, flux)\n\n# Get the fitted parameters\npeak_intensity = g_fit.amplitude.value\ncentral_wavelength = g_fit.mean.value\n\nprint(f\"Peak intensity: {peak_intensity:.2e}\")\nprint(f\"Central wavelength: {central_wavelength:.2f} \u00c5\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "As part of my exoplanet transit analysis, I need to determine the precise angular size of my pixels. How can I get this information from my telescope's WCS data?"
         },
         {
            "from": "assistant",
            "value": "from astropy.io import fits\nfrom astropy.wcs import WCS\nimport numpy as np\n\ndef get_pixel_scale(fits_file):\n    \"\"\"\n    Calculate the pixel scale (angular size) from WCS information\n    Returns pixel scales in arcseconds for both axes\n    \"\"\"\n    try:\n        # Open the FITS file and get the WCS info\n        with fits.open(fits_file) as hdul:\n            wcs = WCS(hdul[0].header)\n            \n        # Get the pixel scale matrix\n        pixel_matrix = wcs.pixel_scale_matrix\n        \n        # Calculate pixel scales in degrees\n        x_scale = np.sqrt(pixel_matrix[0,0]**2 + pixel_matrix[0,1]**2)\n        y_scale = np.sqrt(pixel_matrix[1,0]**2 + pixel_matrix[1,1]**2)\n        \n        # Convert to arcseconds\n        x_scale_arcsec = x_scale * 3600  # Convert degrees to arcseconds\n        y_scale_arcsec = y_scale * 3600\n        \n        return x_scale_arcsec, y_scale_arcsec\n    \n    except Exception as e:\n        print(f\"Error reading WCS information: {e}\")\n        return None\n\n# Example usage\nfilename = \"your_image.fits\"\nscales = get_pixel_scale(filename)\nif scales:\n    print(f\"Pixel scales: {scales[0]:.3f}'' x {scales[1]:.3f}'' per pixel\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I need to write code to convert a set of coordinates from the ICRS reference frame to geocentric true ecliptic coordinates for studying the motion of near-Earth asteroids relative to Earth's orbital plane. How can I perform this transformation?"
         },
         {
            "from": "assistant",
            "value": "from astropy.coordinates import ICRS, GeocentricTrueEcliptic\nfrom astropy.coordinates import SkyCoord\nimport astropy.units as u\nfrom astropy.time import Time\n\n# Create example ICRS coordinates\n# Using arbitrary RA/Dec values for demonstration\nicrs_coords = SkyCoord(\n    ra=150.0*u.degree,\n    dec=32.0*u.degree,\n    distance=1.5*u.AU,  # Adding distance for 3D transformation\n    frame='icrs'\n)\n\n# Set observation time (important for true ecliptic transformation)\nobstime = Time('2023-01-01T00:00:00')\n\n# Transform to geocentric true ecliptic coordinates\necliptic_coords = icrs_coords.transform_to(GeocentricTrueEcliptic(obstime=obstime))\n\n# Print results\nprint(f\"Original ICRS coordinates:\")\nprint(f\"RA = {icrs_coords.ra:.2f}, Dec = {icrs_coords.dec:.2f}\")\nprint(\"\\nTransformed ecliptic coordinates:\")\nprint(f\"Lon = {ecliptic_coords.lon:.2f}\")\nprint(f\"Lat = {ecliptic_coords.lat:.2f}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm studying the distribution of molecular clouds in a spiral galaxy. Could you show me how to represent their positions in cylindrical coordinates, considering their distance from the galactic center, angular position, and height above the galactic plane?"
         },
         {
            "from": "assistant",
            "value": "import astropy.units as u\nimport numpy as np\n\n# Sample data for molecular clouds (random example)\n# Distance from galactic center (in parsecs)\nrho = np.array([5000, 6200, 4800, 7100, 5500]) * u.pc\n\n# Azimuthal angle (in radians)\nphi = np.array([0.5, 1.2, 2.1, 4.2, 5.5])\n\n# Height above galactic plane (in parsecs)\nz = np.array([120, -80, 200, -150, 90]) * u.pc\n\n# Convert cylindrical to cartesian coordinates manually\nx = rho * np.cos(phi)\ny = rho * np.sin(phi)\n# z remains the same in this conversion\n\n# Print both coordinate representations\nprint(\"Cylindrical Coordinates (\u03c1, \u03c6, z):\")\nfor i in range(len(rho)):\n    print(f\"Cloud {i+1}: {rho[i]:.0f}, {phi[i]:.2f} rad, {z[i]:.0f}\")\n\nprint(\"\\nCartesian Coordinates (x, y, z):\")\nfor i in range(len(x)):\n    print(f\"Cloud {i+1}: {x[i]:.0f}, {y[i]:.0f}, {z[i]:.0f}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm working with high-resolution solar observatory images and need to reduce small-scale noise while maintaining the larger solar features. How can I implement a circular averaging filter with proper normalization?"
         },
         {
            "from": "assistant",
            "value": "import numpy as np\nfrom astropy.convolution import Kernel2D, convolve\n\ndef circular_mean_filter(data, radius):\n    \"\"\"\n    Create and apply a circular averaging filter to astronomical image data\n    \n    Parameters:\n    data : 2D numpy array - input image\n    radius : int - radius of circular kernel in pixels\n    \"\"\"\n    \n    # Create a circular kernel\n    y, x = np.mgrid[-radius:radius+1, -radius:radius+1]\n    r = np.sqrt(x**2 + y**2)\n    \n    # Define circular mask\n    mask = r <= radius\n    kernel = Kernel2D(mask.astype(float))\n    \n    # Normalize the kernel so sum = 1\n    kernel.normalize()\n    \n    # Apply convolution with edge handling\n    filtered_data = convolve(data, kernel, boundary='extend')\n    \n    return filtered_data\n\n# Example usage:\n# filtered_image = circular_mean_filter(solar_image, radius=5)"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "During data reduction, I noticed some incorrect photometric zeropoints in my image's HDU - how can I programmatically update these values in the FITS header?"
         },
         {
            "from": "assistant",
            "value": "from astropy.io import fits\n\ndef update_zeropoints(fits_file, new_zp, output_file=None):\n    \"\"\"\n    Update photometric zeropoint values in FITS header\n    \n    Parameters:\n        fits_file (str): Input FITS filename\n        new_zp (float): New zeropoint value\n        output_file (str): Output filename. If None, overwrites input\n    \"\"\"\n    try:\n        # Open FITS file\n        with fits.open(fits_file, mode='update') as hdul:\n            \n            # Update primary header zeropoint\n            hdul[0].header['MAGZERO'] = (new_zp, 'Photometric zeropoint')\n            \n            # Also check extension headers if they exist\n            for hdu in hdul[1:]:\n                if 'MAGZERO' in hdu.header:\n                    hdu.header['MAGZERO'] = new_zp\n                    \n            # Save changes\n            if output_file:\n                hdul.writeto(output_file, overwrite=True)\n            else:\n                hdul.flush()\n                \n    except Exception as e:\n        print(f\"Error updating zeropoint: {str(e)}\")\n\n# Example usage\nupdate_zeropoints('image.fits', 25.978, 'updated_image.fits')"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I need to analyze an exoplanet's properties. How can I express its mass of 0.37 Jupiter masses in terms of Earth masses using astropy units?"
         },
         {
            "from": "assistant",
            "value": "from astropy import units as u\n\n# Define the exoplanet mass in Jupiter masses\nexoplanet_mass = 0.37 * u.M_jup\n\n# Convert to Earth masses\nearth_mass = exoplanet_mass.to(u.M_earth)\n\n# Print the result\nprint(f\"Exoplanet mass: {exoplanet_mass:.2f}\")\nprint(f\"In Earth masses: {earth_mass:.1f}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "In preparation for my variable star observations, I need to calculate SNR across different brightness levels. Source signal varies from 0.1 to 1.0 e-/s (5 equal steps), with fixed sky background = 0.15 e-/s/pixel, dark current = 0.03 e-/s/pixel, read noise = 6 e-, and aperture = 18 pixels. How can I compute SNR for each brightness level with 900s exposure time?"
         },
         {
            "from": "assistant",
            "value": "import numpy as np\nfrom astropy.stats import sigma_clipped_stats\n\n# Define parameters\nexposure_time = 900  # seconds\nsky_background = 0.15  # e-/s/pixel\ndark_current = 0.03  # e-/s/pixel\nread_noise = 6  # e-\naperture_pixels = 18  # pixels\nsource_signals = np.linspace(0.1, 1.0, 5)  # e-/s\n\ndef calculate_snr(signal, t, sky, dark, rdnoise, n_pix):\n    \"\"\"Calculate SNR for given parameters using CCD equation\"\"\"\n    # Total source counts\n    source_counts = signal * t\n    \n    # Total noise (sqrt of sum of variances)\n    # Include: photon noise, sky noise, dark noise, read noise\n    total_noise = np.sqrt(\n        source_counts +  # Source photon noise\n        n_pix * (sky + dark) * t +  # Sky and dark current noise\n        n_pix * rdnoise**2  # Read noise\n    )\n    \n    return source_counts / total_noise\n\n# Calculate SNR for each signal level\nfor signal in source_signals:\n    snr = calculate_snr(\n        signal, \n        exposure_time,\n        sky_background,\n        dark_current,\n        read_noise,\n        aperture_pixels\n    )\n    print(f\"Source: {signal:.2f} e-/s -> SNR: {snr:.1f}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I need to compute the barycentric velocities of all major planets in the solar system at a specific epoch for my orbital dynamics simulation. How can I do this efficiently?"
         },
         {
            "from": "assistant",
            "value": "from astropy.time import Time\nfrom astropy.coordinates import get_body_barycentric_posvel\nimport numpy as np\n\ndef get_planetary_velocities(epoch):\n    # List of major planets\n    planets = ['mercury', 'venus', 'earth', 'mars', \n               'jupiter', 'saturn', 'uranus', 'neptune']\n    \n    # Convert epoch to Time object\n    t = Time(epoch)\n    \n    # Dictionary to store velocities\n    velocities = {}\n    \n    # Calculate barycentric velocity for each planet\n    for planet in planets:\n        try:\n            # Get position and velocity relative to solar system barycenter\n            pos_vel = get_body_barycentric_posvel(planet, t)\n            \n            # Extract velocity components (in km/s)\n            vel = pos_vel[1].to('km/s')\n            \n            # Store velocity magnitude and components\n            velocities[planet] = {\n                'velocity_vector': vel,\n                'velocity_magnitude': np.sqrt(np.sum(vel.xyz**2)).value\n            }\n        except Exception as e:\n            print(f\"Error calculating velocity for {planet}: {e}\")\n    \n    return velocities\n\n# Example usage\nepoch = '2023-12-25T00:00:00'\nplanetary_velocities = get_planetary_velocities(epoch)\n\n# Print results\nfor planet, data in planetary_velocities.items():\n    print(f\"{planet.capitalize()}:\")\n    print(f\"Velocity magnitude: {data['velocity_magnitude']:.2f} km/s\")\n    print(f\"Velocity vector: {data['velocity_vector'].xyz.value}\\n\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "For my PSF modeling, I need to create a 2D Gaussian kernel with sigma=3.5 pixels and compare its profile using both 'center' and 'integrate' modes to see the differences in discretization. How can I generate and visualize these two kernels side by side?"
         },
         {
            "from": "assistant",
            "value": "from astropy.convolution import Gaussian2DKernel\nfrom astropy.visualization import astropy_mpl_style\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Create two 2D Gaussian kernels with different modes\nkernel_center = Gaussian2DKernel(x_stddev=3.5, y_stddev=3.5, x_size=21, mode='center')\nkernel_integrate = Gaussian2DKernel(x_stddev=3.5, y_stddev=3.5, x_size=21, mode='integrate')\n\n# Set up the plot\nplt.style.use(astropy_mpl_style)\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n\n# Plot central cross-sections of both kernels\ncenter_idx = kernel_center.shape[0] // 2\nax1.plot(kernel_center.array[center_idx, :], label='Center mode')\nax1.plot(kernel_integrate.array[center_idx, :], label='Integrate mode')\nax1.set_title('Kernel Cross-sections')\nax1.set_xlabel('Pixel')\nax1.set_ylabel('Kernel Value')\nax1.legend()\n\n# Plot 2D difference between kernels to show discretization effects\ndiff = kernel_center.array - kernel_integrate.array\nim = ax2.imshow(diff, cmap='RdBu_r', origin='lower')\nax2.set_title('Difference (Center - Integrate)')\nplt.colorbar(im, ax=ax2)\n\nplt.tight_layout()\nplt.show()"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm studying a galaxy with a surface brightness of 2.5 at the half-light radius of 10 kpc, and I believe it follows a de Vaucouleurs profile. How can I create a Sersic1D model and plot the surface brightness from 0 to 50 kpc?"
         },
         {
            "from": "assistant",
            "value": "import numpy as np\nfrom astropy.modeling.models import Sersic1D\nimport matplotlib.pyplot as plt\n\n# Create a Sersic1D model with n=4 (de Vaucouleurs profile)\n# amplitude=2.5 at r_eff=10 kpc\nmodel = Sersic1D(amplitude=2.5, r_eff=10, n=4)\n\n# Generate radius array from 0.1 to 50 kpc (avoiding r=0)\nr = np.linspace(0.1, 50, 1000)\n\n# Calculate surface brightness profile\nsurface_brightness = model(r)\n\n# Create the plot\nplt.figure(figsize=(8, 6))\nplt.plot(r, surface_brightness, 'b-', label='de Vaucouleurs Profile')\nplt.xlabel('Radius (kpc)')\nplt.ylabel('Surface Brightness')\nplt.title('Galaxy Surface Brightness Profile')\nplt.axvline(x=10, color='r', linestyle='--', label='Half-light radius')\nplt.axhline(y=2.5, color='r', linestyle='--')\nplt.yscale('log')  # Log scale for better visualization\nplt.grid(True)\nplt.legend()\nplt.show()"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I have photometric data [[1000, 1200], [1100, 1300]] in ADU and its corresponding WCS information. How can I create a CCDData object including both the data with proper units and the WCS object (assume we have a simple WCS object called 'wcs_obj')?"
         },
         {
            "from": "assistant",
            "value": "from astropy.nddata import CCDData\nimport astropy.units as u\nimport numpy as np\nfrom astropy.wcs import WCS\n\n# Create numpy array from the photometric data\ndata = np.array([[1000, 1200], [1100, 1300]])\n\n# Create a simple WCS object\nwcs_obj = WCS(naxis=2)  # 2 dimensional WCS\nwcs_obj.wcs.crpix = [1, 1]  # Reference pixel coordinates\nwcs_obj.wcs.cdelt = [1, 1]  # Pixel scale in degrees\nwcs_obj.wcs.crval = [0, 0]  # Reference point coordinates\nwcs_obj.wcs.ctype = [\"RA---TAN\", \"DEC--TAN\"]  # Coordinate types\n\n# Create CCDData object with both data and WCS\n# ADU (Analog-to-Digital Units) is represented as dimensionless units in AstroPy\nccd = CCDData(data=data, \n              unit=u.dimensionless_unscaled,  # For ADU units\n              wcs=wcs_obj)  # Attach the WCS information\n\n# The resulting ccd object now contains both the data and WCS information\n# You can access them using:\n# ccd.data - for the photometric data\n# ccd.wcs  - for the WCS information"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "In my exoplanet research, I have three tables: planetary masses (in Jupiter masses), orbital periods (in days), and host star properties (temperature and metallicity). Each table has data for 200 planets. Show me how to stack these tables horizontally with unique column names in format 'parameter_dataset'. Use sample data for 2 planets."
         },
         {
            "from": "assistant",
            "value": "from astropy.table import Table, hstack\nimport numpy as np\n\n# Create sample data tables for 2 planets\n# Table 1: Planetary masses\nmass_data = Table(\n    {'mass': [1.2, 0.8]},  # Values in Jupiter masses\n    names=['mass']\n)\n\n# Table 2: Orbital periods\nperiod_data = Table(\n    {'period': [3.2, 15.7]},  # Values in days\n    names=['period']\n)\n\n# Table 3: Host star properties\nstar_data = Table({\n    'temperature': [5800, 6200],  # Values in Kelvin\n    'metallicity': [0.05, -0.12]  # [Fe/H] values\n})\n\n# Create lists for renaming columns with dataset identifiers\nmass_names = ['mass_set1']\nperiod_names = ['period_set1']\nstar_names = ['temperature_set1', 'metallicity_set1']\n\n# Rename columns before stacking\nfor old, new in zip(mass_data.colnames, mass_names):\n    mass_data.rename_column(old, new)\nfor old, new in zip(period_data.colnames, period_names):\n    period_data.rename_column(old, new)\nfor old, new in zip(star_data.colnames, star_names):\n    star_data.rename_column(old, new)\n\n# Stack tables horizontally\ncombined_table = hstack([mass_data, period_data, star_data])\n\n# Display the combined table\nprint(combined_table)"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm modeling instrumental bias in my CCD detector that has a constant offset of 234.6 counts across the field. How do I create a Const2D model for this offset and set bounds between 230 and 240 to ensure realistic fitting results?"
         },
         {
            "from": "assistant",
            "value": "from astropy.modeling import models, Fittable2DModel\n\n# Create a 2D constant model for CCD bias\nbias_model = models.Const2D(amplitude=234.6)\n\n# Set bounds for the amplitude parameter between 230-240 counts\nbias_model.amplitude.bounds = (230.0, 240.0)\n\n# The model can now be used for fitting while constrained within realistic bounds\n# Example of getting model value at specific x,y coordinates:\nx, y = 100, 100  # Example pixel coordinates\nbias_value = bias_model(x, y)  # Will return 234.6"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "For my exoplanet transit data, I need to smooth a 2D image with high precision. Show me how to create a Gaussian2DKernel with stddev=2.5 using the 'integrate' mode and an oversampling factor of 20 for better accuracy."
         },
         {
            "from": "assistant",
            "value": "from astropy.convolution import Gaussian2DKernel\nfrom astropy.modeling.models import Gaussian2D\n\n# Create a high-precision 2D Gaussian kernel\n# - stddev=2.5: controls the width of the Gaussian\n# - x_size and y_size will be automatically determined\n# - mode='integrate': uses numerical integration for better accuracy\n# - oversampling factor=20: creates a more precise kernel\ngaussian_kernel = Gaussian2DKernel(\n    x_stddev=2.5,\n    mode='integrate',\n    factor=20\n)\n\n# The kernel is now ready to be used for convolution with your image\n# Example of how to use it (assuming 'image' is your 2D data):\n# from astropy.convolution import convolve\n# smoothed_image = convolve(image, gaussian_kernel)"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm studying pulsar timing and need to define a custom time unit called \\\"pulsar_period\\\" that equals 0.05 seconds. How can I implement this with def_unit while excluding the 'P' prefix to avoid confusion with peta-prefix?"
         },
         {
            "from": "assistant",
            "value": "# Import required modules from astropy\nfrom astropy import units as u\n\n# Define a new custom unit for pulsar period\n# Set prefix=False to prevent automatic prefix handling (avoiding 'P' confusion)\npulsar_period = u.def_unit('pulsar_period', 0.05 * u.second, prefixes=False, \n                          doc=\"Custom unit for pulsar period equal to 0.05 seconds\")\n\n# Example usage\nrotation_time = 3 * pulsar_period\nprint(f\"Rotation time: {rotation_time}\")\nprint(f\"In seconds: {rotation_time.to(u.second)}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "For my research on the Pleiades star cluster, I need to transform the location of a star from spherical coordinates (r=5.3 parsecs, lat=-23.5 degrees, lon=67.8 degrees) to cartesian coordinates. What's the code to do this conversion using astropy?"
         },
         {
            "from": "assistant",
            "value": "from astropy.coordinates import spherical_to_cartesian\nimport astropy.units as u\nimport numpy as np\n\n# Define the spherical coordinates with units\nr = 5.3 * u.parsec\nlat = -23.5 * u.degree\nlon = 67.8 * u.degree\n\n# Transform to cartesian coordinates\n# spherical_to_cartesian expects (r, lat, lon)\nx, y, z = spherical_to_cartesian(r, lat, lon)\n\n# Print results\nprint(f\"Cartesian coordinates:\")\nprint(f\"x = {x:.2f}\")\nprint(f\"y = {y:.2f}\")\nprint(f\"z = {z:.2f}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm working with data from multiple observatories and need to standardize the observation timestamps. How can I convert an array of Unix timestamps to ISO format for a set of gamma-ray burst observations?"
         },
         {
            "from": "assistant",
            "value": "from astropy.time import Time\nimport numpy as np\n\n# Sample Unix timestamps (seconds since 1970-01-01)\nunix_timestamps = np.array([1632145200, 1632148800, 1632152400])\n\n# Convert Unix timestamps to astropy Time object\n# Scale 'utc' specifies Universal Coordinated Time\ntimes = Time(unix_timestamps, format='unix', scale='utc')\n\n# Convert to ISO format\niso_times = times.iso\n\nprint(\"Original Unix timestamps:\", unix_timestamps)\nprint(\"\\nConverted ISO times:\")\nfor t in iso_times:\n    print(t)\n\n# Optional: If you need specific ISO format with T separator\n# iso_times_t = times.isot\n# Or with higher precision\n# high_precision = times.iso[0:23]"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "How can I extract the metadata information from a FITS header for my galaxy observation data and access specific keywords related to the telescope's configuration?"
         },
         {
            "from": "assistant",
            "value": "from astropy.io import fits\nimport os\n\ndef print_fits_header(fits_file_path):\n    \"\"\"\n    Print header information from a FITS file\n    \n    Parameters:\n    fits_file_path (str): Path to the FITS file\n    \"\"\"\n    try:\n        # Check if file exists\n        if not os.path.exists(fits_file_path):\n            raise FileNotFoundError(f\"The file {fits_file_path} does not exist\")\n            \n        # Open the FITS file\n        with fits.open(fits_file_path) as hdul:\n            # Get the primary header\n            header = hdul[0].header\n            \n            # Extract common telescope configuration keywords\n            telescope = header.get('TELESCOP', 'Not specified')\n            instrument = header.get('INSTRUME', 'Not specified')\n            exposure_time = header.get('EXPTIME', 'Not specified')\n            observation_date = header.get('DATE-OBS', 'Not specified')\n            \n            # Print the extracted information\n            print(f\"Telescope: {telescope}\")\n            print(f\"Instrument: {instrument}\")\n            print(f\"Exposure Time: {exposure_time} seconds\")\n            print(f\"Observation Date: {observation_date}\")\n            \n            # Print all header keywords and values\n            print(\"\\nAll Header Information:\")\n            for key, value in header.items():\n                if key:  # Skip empty keywords\n                    print(f\"{key}: {value}\")\n                    \n    except FileNotFoundError as e:\n        print(f\"Error: {e}\")\n        print(\"Please check if the file path is correct\")\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n\n# Example usage:\n# Replace this path with your actual FITS file path\nfits_file_path = 'galaxy_observation.fits'\n\n# If you want to test with a sample FITS file from astropy:\n# from astropy.utils.data import get_pkg_data_filename\n# fits_file_path = get_pkg_data_filename('tutorials/FITS-images/HorseHead.fits')\n\nprint_fits_header(fits_file_path)"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "How can I update the header and data of a FITS file in memory for a specific HDU named \"SPECTRUM\" with a new numpy array [1,2,3,4] and a new exposure time of 1000 seconds?"
         },
         {
            "from": "assistant",
            "value": "from astropy.io import fits\nimport numpy as np\n\n# Create or open a FITS file in memory\nhdul = fits.HDUList()\n\n# Create new data array\nnew_data = np.array([1, 2, 3, 4])\n\n# Try to locate and update existing SPECTRUM HDU, or create new one if not found\ntry:\n    spectrum_hdu = hdul['SPECTRUM']\n    # Update existing HDU\n    spectrum_hdu.data = new_data\n    spectrum_hdu.header['EXPTIME'] = (1000.0, 'Exposure time in seconds')\nexcept KeyError:\n    # Create new HDU if SPECTRUM doesn't exist\n    hdu = fits.ImageHDU(data=new_data, name='SPECTRUM')\n    hdu.header['EXPTIME'] = (1000.0, 'Exposure time in seconds')\n    hdul.append(hdu)\n\n# Note: At this point, hdul contains the updated HDU in memory\n# To save to file, you would use: hdul.writeto('filename.fits', overwrite=True)"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "During my solar flare analysis, I need to measure the duration of various emission phases. How can I compute these time intervals and convert them between different astronomical time scales?"
         },
         {
            "from": "assistant",
            "value": "from astropy.time import Time\nfrom astropy import units as u\n\n# Create time objects for start and end of flare phases\nflare_start = Time('2023-06-15T12:30:45', format='isot', scale='utc')\npeak_time = Time('2023-06-15T12:32:15', scale='utc')\nflare_end = Time('2023-06-15T12:35:00', scale='utc')\n\n# Calculate time intervals for different phases\nrise_phase = (peak_time - flare_start).to(u.second)\ndecay_phase = (flare_end - peak_time).to(u.second)\ntotal_duration = (flare_end - flare_start).to(u.minute)\n\n# Convert times to different scales for analysis\nflare_start_tt = flare_start.tt  # Terrestrial Time\nflare_start_tai = flare_start.tai  # International Atomic Time\n\n# Print results\nprint(f\"Rise phase duration: {rise_phase}\")\nprint(f\"Decay phase duration: {decay_phase}\")\nprint(f\"Total flare duration: {total_duration}\")\nprint(f\"Start time in TT: {flare_start_tt.isot}\")\nprint(f\"Start time in TAI: {flare_start_tai.isot}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I want to create a publication-ready plot of my exoplanet transit light curve data using Astropy's recommended plotting style. How can I set this up?"
         },
         {
            "from": "assistant",
            "value": "# Import required modules\nfrom astropy.visualization import astropy_mpl_style\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom astropy.time import Time\nfrom astropy import units as u  # Add this import for units\n\n# Set the Astropy plotting style\nplt.style.use(astropy_mpl_style)\n\n# Create sample transit data (replace with your actual data)\ntime = Time('2023-01-01 00:00:00') + np.linspace(0, 0.3, 100) * 24 * 3600 * u.second\nflux = 1.0 - 0.02 * np.exp(-((time.jd - time.jd[50])*24*12)**2)\n# Use absolute values for error bars to ensure they're positive\nflux_err = np.abs(np.random.normal(0, 0.002, len(time)))\n\n# Create the plot\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Plot data points with error bars\nax.errorbar(time.jd, flux, yerr=flux_err, fmt='o', color='navy', \n           alpha=0.6, capsize=2, label='Observations')\n\n# Customize the plot\nax.set_xlabel('Time (JD)')\nax.set_ylabel('Relative Flux')\nax.set_title('Exoplanet Transit Light Curve')\nax.grid(True, linestyle='--', alpha=0.5)\nax.legend()\n\n# Adjust layout to prevent label cutting\nplt.tight_layout()\n\n# Save the plot in publication-ready format\nplt.savefig('transit_lightcurve.pdf', dpi=300, bbox_inches='tight')\nplt.show()"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm working on detecting faint nebulae in my deep-field images. How can I create a specialized kernel that enhances extended, low-surface-brightness features while suppressing point sources?"
         },
         {
            "from": "assistant",
            "value": "from astropy.convolution import Gaussian2DKernel, Ring2DKernel, convolve\nimport numpy as np\n\ndef create_nebula_kernel(inner_radius=3, outer_radius=8, stddev=2):\n    \"\"\"\n    Creates a specialized kernel for nebula detection by combining\n    a positive Gaussian with a negative ring to suppress point sources\n    while enhancing extended features.\n    \"\"\"\n    # Create a positive Gaussian kernel for extended features\n    gauss_kernel = Gaussian2DKernel(stddev)\n    \n    # Create a negative ring kernel to suppress point sources\n    ring_kernel = Ring2DKernel(inner_radius, outer_radius)\n    ring_kernel.array *= -0.3  # Reduce ring intensity\n    \n    # Combine the kernels\n    combined_kernel = gauss_kernel + ring_kernel\n    \n    # Normalize to ensure kernel sum is 1\n    combined_kernel.array /= combined_kernel.array.sum()\n    \n    return combined_kernel\n\ndef enhance_nebula(image_data):\n    \"\"\"\n    Enhances nebular features in an astronomical image.\n    \"\"\"\n    # Create specialized kernel\n    kernel = create_nebula_kernel()\n    \n    # Convolve image with kernel\n    enhanced_image = convolve(image_data, kernel, boundary='extend')\n    \n    return enhanced_image\n\n# Example usage:\n# enhanced_data = enhance_nebula(your_image_data)"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I need to append a newly observed galaxy image with dimensions (2048, 2048) pixels to my existing multi-extension FITS file. The data has exposure time of 1800 seconds and was taken through the V-band filter. Show me how to do this with proper header information."
         },
         {
            "from": "assistant",
            "value": "from astropy.io import fits\nimport numpy as np\n\n# Create sample image data (in real case, this would be your actual image)\nnew_galaxy_data = np.random.normal(1000, 50, (2048, 2048))\n\n# Open existing FITS file in 'append' mode\nwith fits.open('existing_file.fits', mode='append') as hdul:\n    \n    # Create header for new extension\n    new_header = fits.Header()\n    new_header['EXPTIME'] = (1800, 'Exposure time in seconds')\n    new_header['FILTER'] = ('V', 'Filter used for observation')\n    new_header['IMAGETYP'] = ('SCIENCE', 'Type of image')\n    new_header['NAXIS1'] = 2048\n    new_header['NAXIS2'] = 2048\n    new_header['BUNIT'] = ('ADU', 'Unit of pixel values')\n    \n    # Create new image HDU with data and header\n    new_hdu = fits.ImageHDU(data=new_galaxy_data, header=new_header, name='GALAXY')\n    \n    # Append new HDU to existing file\n    hdul.append(new_hdu)\n    \n    # File is automatically closed and saved after the 'with' block"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm writing code to analyze exoplanet transit data where I need to work with both astronomical units (AU) and Earth radii. What's the proper way to make these units available alongside the standard metric units?"
         },
         {
            "from": "assistant",
            "value": "# Import necessary astropy units module\nfrom astropy import units as u\n\n# Define some example values with astronomical and Earth-based units\ndistance_to_star = 0.5 * u.AU  # Distance in Astronomical Units\nplanet_radius = 2.1 * u.earthRad  # Planet radius in Earth radii\n\n# Convert AU to kilometers for comparison\ndistance_km = distance_to_star.to(u.km)\n\n# Convert Earth radii to solar radii\nplanet_radius_solar = planet_radius.to(u.R_sun)\n\n# Print results with units\nprint(f\"Distance to star: {distance_to_star} = {distance_km}\")\nprint(f\"Planet radius: {planet_radius} = {planet_radius_solar}\")\n\n# Example of unit arithmetic (calculating planet's orbital circumference)\norbital_circumference = 2 * 3.14159 * distance_to_star\nprint(f\"Orbital circumference: {orbital_circumference}\")\n\n# Converting between unit systems\nearth_radius_meters = (1 * u.earthRad).to(u.m)\nprint(f\"1 Earth radius = {earth_radius_meters}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm analyzing stellar spectra and need to apply a linear stretch to calibrate the flux values. How can I transform the raw spectral data?"
         },
         {
            "from": "assistant",
            "value": "from astropy.visualization import LinearStretch\nfrom astropy.visualization.stretch import BaseStretch\nimport numpy as np\n\ndef calibrate_spectrum(raw_flux, min_percentile=1, max_percentile=99):\n    \"\"\"\n    Apply linear stretch to calibrate raw spectral flux values\n    Parameters:\n        raw_flux: array-like, Raw flux values from stellar spectrum\n        min_percentile: Lower percentile cutoff for stretch\n        max_percentile: Upper percentile cutoff for stretch\n    Returns:\n        Calibrated flux values\n    \"\"\"\n    # Create linear stretch transform\n    transform = LinearStretch()\n    \n    # Normalize data to [0,1] range for the stretch\n    vmin, vmax = np.percentile(raw_flux, [min_percentile, max_percentile])\n    norm_flux = (raw_flux - vmin) / (vmax - vmin)\n    \n    # Apply linear stretch transformation\n    calibrated_flux = transform(norm_flux)\n    \n    return calibrated_flux\n\n# Example usage\nif __name__ == \"__main__\":\n    # Sample raw spectral data\n    raw_data = np.random.normal(1000, 100, 1000)\n    \n    # Apply calibration\n    calibrated_data = calibrate_spectrum(raw_data)"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm analyzing the distribution of stellar luminosities in a globular cluster. Can you show me how to create a power-law model with a specific reference point and fit it to my magnitude-frequency data?"
         },
         {
            "from": "assistant",
            "value": "import numpy as np\nfrom astropy.modeling import models, fitting\nfrom astropy.modeling.powerlaws import PowerLaw1D\n\n# Generate sample magnitude-frequency data \n# (normally you would load your actual data)\nmagnitudes = np.linspace(12, 18, 100)  # apparent magnitudes\nfrequencies = 50 * (10**(-0.3 * (magnitudes-15))) + np.random.normal(0, 2, 100)\n\n# Create power law model\n# Reference point x_0=15 (anchor magnitude)\n# Initial amplitude estimate = 50\n# Initial power law index alpha = -0.3\npower_law = PowerLaw1D(amplitude=50, x_0=15, alpha=-0.3)\n\n# Initialize fitter\nfitter = fitting.LevMarLSQFitter()\n\n# Fit the model to the data\nfitted_model = fitter(power_law, magnitudes, frequencies)\n\n# Print fitted parameters\nprint(f\"Fitted amplitude: {fitted_model.amplitude.value:.2f}\")\nprint(f\"Fitted power law index: {fitted_model.alpha.value:.2f}\")\n\n# Calculate fitted values\nfitted_frequencies = fitted_model(magnitudes)\n\n# The fitted model can now be used to predict frequencies at any magnitude\n# For example, predict frequency at magnitude 16:\nprediction = fitted_model(16)\nprint(f\"Predicted frequency at magnitude 16: {prediction:.2f}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm analyzing pulsar periods [1.2, 0.8, 2.1, 1.5] seconds. Write code to create a Column with proper unit and format the output to display 3 decimal places."
         },
         {
            "from": "assistant",
            "value": "from astropy.table import Column\nfrom astropy import units as u\n\n# Create Column with pulsar periods and units of seconds\npulsar_periods = Column([1.2, 0.8, 2.1, 1.5], \n                       unit=u.s, \n                       name='Period',\n                       format='%.3f')  # Format to 3 decimal places\n\n# Display the column with formatted values\nprint(pulsar_periods)"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "For coordinating simultaneous observations between Beijing Observatory (UTC+8) and Greenwich (UTC+0), help me write code using TimezoneInfo to create these timezone objects and convert the upcoming solar maximum observation time '2024-07-01 14:20:00' between them."
         },
         {
            "from": "assistant",
            "value": "from astropy.time import Time, TimezoneInfo\nfrom datetime import datetime\nimport astropy.units as u  # Import astropy units\n\n# Create timezone objects for Beijing (UTC+8) and Greenwich (UTC+0)\nbeijing_tz = TimezoneInfo(utc_offset=8*3600*u.second)  # UTC+8 in seconds with proper units\ngreenwich_tz = TimezoneInfo()  # UTC+0\n\n# Create time object for solar max observation in Beijing time\nbeijing_time = Time(datetime(2024, 7, 1, 14, 20, 0, tzinfo=beijing_tz))\n\n# Convert to Greenwich time\ngreenwich_time = Time(beijing_time, scale='utc')\n\n# Format output in readable format\nprint(f\"Solar Maximum Observation Time:\")\nprint(f\"Beijing (UTC+8): {beijing_time.datetime.strftime('%Y-%m-%d %H:%M:%S %Z')}\")\nprint(f\"Greenwich (UTC+0): {greenwich_time.datetime.strftime('%Y-%m-%d %H:%M:%S %Z')}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I have intensity data of a galaxy cluster in a 400x400 numpy array with values from 1e-3 to 1e5. How do I create a plot using imshow_norm that applies an asinh stretch to handle the large dynamic range, uses a 'hot' colormap, and shows axis labels in kpc units?"
         },
         {
            "from": "assistant",
            "value": "import numpy as np\nfrom astropy.visualization import ImageNormalize, AsinhStretch\nimport matplotlib.pyplot as plt\n\n# Create sample intensity data (400x400 array)\n# In real application, you would load your actual data here\nsize = 400\nx = np.linspace(-100, 100, size)\ny = np.linspace(-100, 100, size)\nX, Y = np.meshgrid(x, y)\n# Creating a sample gaussian distribution for demonstration\nintensity = np.exp(-(X**2 + Y**2)/(2*30**2))\n\n# Create pixel scale in kpc (example: 0.5 kpc per pixel)\nkpc_per_pixel = 0.5\nxaxis_kpc = np.linspace(-100, 100, 400) # Centered at 0\nyaxis_kpc = np.linspace(-100, 100, 400)\n\n# Create normalization with asinh stretch\nnorm = ImageNormalize(intensity, stretch=AsinhStretch())\n\n# Create figure and plot\nfig, ax = plt.subplots(figsize=(8, 8))\nim = ax.imshow(intensity, norm=norm, cmap='hot', \n               extent=[xaxis_kpc[0], xaxis_kpc[-1], \n                      yaxis_kpc[0], yaxis_kpc[-1]])\n\n# Add labels and colorbar\nax.set_xlabel('Distance (kpc)')\nax.set_ylabel('Distance (kpc)')\nplt.colorbar(im, label='Intensity')\n\nplt.title('Galaxy Cluster Intensity Map')\nplt.show()"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm analyzing the Venus-Earth configuration on 2023-06-21. How can I calculate the barycentric positions of both planets at hourly intervals from 00:00 to 23:00 UTC on that day?"
         },
         {
            "from": "assistant",
            "value": "from astropy.time import Time\nimport astropy.units as u\nfrom astropy.coordinates import get_body_barycentric\nimport numpy as np\n\n# Create array of times for June 21, 2023 (hourly)\nbase_time = Time('2023-06-21 00:00')\nhours = np.arange(24)\ntimes = base_time + hours*u.hour\n\n# Calculate barycentric positions for both planets\nvenus_positions = get_body_barycentric('venus', times)\nearth_positions = get_body_barycentric('earth', times)\n\n# Print positions in AU for each hour\nfor hour, (venus_pos, earth_pos) in enumerate(zip(venus_positions, earth_positions)):\n    print(f\"Hour {hour:02d}:00 UTC\")\n    print(f\"Venus: x={venus_pos.x.to('au'):.3f}, \"\n          f\"y={venus_pos.y.to('au'):.3f}, \"\n          f\"z={venus_pos.z.to('au'):.3f}\")\n    print(f\"Earth: x={earth_pos.x.to('au'):.3f}, \"\n          f\"y={earth_pos.y.to('au'):.3f}, \"\n          f\"z={earth_pos.z.to('au'):.3f}\\n\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm developing a data reduction pipeline for my observatory and need to create a master flat field as an ImageHDU from multiple calibration frames. Can you show me the proper way to construct this?"
         },
         {
            "from": "assistant",
            "value": "from astropy.io import fits\nimport numpy as np\nfrom pathlib import Path\n\ndef create_master_flat(flat_files, output_file='master_flat.fits'):\n    \"\"\"\n    Create a master flat from multiple flat field frames\n    using median combination and normalization\n    \"\"\"\n    # Check if flat_files list is empty\n    if not flat_files:\n        raise ValueError(\"No flat field files found\")\n\n    # Read all flat frames into a list\n    flats = []\n    for file in flat_files:\n        with fits.open(file) as hdul:\n            # Assuming data is in primary HDU\n            data = hdul[0].data\n            if data is None or data.size == 0:\n                continue\n            flats.append(data.astype(float))\n    \n    # Check if any valid flats were read\n    if not flats:\n        raise ValueError(\"No valid flat field data found in the files\")\n    \n    # Stack frames into 3D array\n    flat_stack = np.array(flats)\n    \n    # Create master flat using median combination\n    master_flat = np.median(flat_stack, axis=0)\n    \n    # Check if master_flat contains any valid data\n    if np.all(np.isnan(master_flat)) or master_flat.size == 0:\n        raise ValueError(\"Master flat creation failed - all values are NaN\")\n    \n    # Normalize by dividing by mean value\n    mean_value = np.mean(master_flat[np.isfinite(master_flat)])\n    if mean_value == 0 or np.isnan(mean_value):\n        raise ValueError(\"Cannot normalize: mean value is zero or NaN\")\n    \n    master_flat_normalized = master_flat / mean_value\n    \n    # Create PrimaryHDU (instead of ImageHDU) with master flat\n    hdu = fits.PrimaryHDU(data=master_flat_normalized)\n    hdu.header['IMAGETYP'] = 'MASTER_FLAT'\n    hdu.header['NCOMBINE'] = len(flats)  # Number of frames combined\n    \n    # Write to file\n    hdu.writeto(output_file, overwrite=True)\n    \n    return hdu\n\n# Example usage:\nif __name__ == \"__main__\":\n    try:\n        # Get list of flat field files\n        flat_files = list(Path('.').glob('flat*.fits'))\n        \n        # Print number of files found\n        print(f\"Found {len(flat_files)} flat field files\")\n        \n        master_flat_hdu = create_master_flat(flat_files)\n        print(\"Master flat created successfully\")\n        \n    except ValueError as e:\n        print(f\"Error: {e}\")\n    except Exception as e:\n        print(f\"Unexpected error: {e}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "How do I combine multiple polynomial components and fit them to my stellar continuum data using SimplexLSQFitter to remove the baseline?"
         },
         {
            "from": "assistant",
            "value": "from astropy.modeling import models, fitting\nimport numpy as np\n\n# Create sample data (replace with your actual data)\nx = np.linspace(0, 10, 200)\ntrue_continuum = 3 + 0.5*x + 0.2*x**2 - 0.05*x**3\nnoise = np.random.normal(0, 0.1, x.shape)\ny = true_continuum + noise\n\n# Create compound model with multiple polynomial components\n# Combine Polynomial1D models of different degrees\npoly1 = models.Polynomial1D(degree=1, name='poly1')  # Linear component\npoly2 = models.Polynomial1D(degree=2, name='poly2')  # Quadratic component\npoly3 = models.Polynomial1D(degree=3, name='poly3')  # Cubic component\n\n# Combine the components\ncompound_model = poly1 + poly2 + poly3\n\n# Initialize the fitter\nfitter = fitting.SimplexLSQFitter()\n\n# Fit the model to the data\nfitted_model = fitter(compound_model, x, y)\n\n# Get the fitted continuum\nfitted_continuum = fitted_model(x)\n\n# Remove baseline/continuum from original data\nbaseline_removed = y - fitted_continuum\n\n# The baseline_removed array now contains your continuum-subtracted data"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm working with spectral analysis and need to model a stellar absorption line that shows both Gaussian and Lorentzian components. How can I create a composite model using astropy.modeling that combines a Gaussian (amplitude=2.5, mean=6563\u00c5, stddev=2.0) and Lorentzian (amplitude=1.5, x_0=6563\u00c5, fwhm=3.0) to fit this H\u03b1 line profile?"
         },
         {
            "from": "assistant",
            "value": "from astropy.modeling import models, fitting\nimport numpy as np\n\n# Create wavelength array centered around H-alpha (6563 \u00c5)\nwavelength = np.linspace(6558, 6568, 200)\n\n# Create individual components\ngaussian = models.Gaussian1D(amplitude=2.5, mean=6563, stddev=2.0)\nlorentzian = models.Lorentz1D(amplitude=1.5, x_0=6563, fwhm=3.0)\n\n# Combine models into a composite profile\ncomposite_model = gaussian + lorentzian\n\n# Generate synthetic data with some noise for demonstration\nnp.random.seed(42)  # For reproducibility\nnoise = np.random.normal(0, 0.1, wavelength.shape)\nsynthetic_data = composite_model(wavelength) + noise\n\n# Fit the composite model to the data\nfitter = fitting.LevMarLSQFitter()\nfitted_model = fitter(composite_model, wavelength, synthetic_data)\n\n# Get the fitted parameters\nfitted_gaussian = fitted_model[0]  # First component (Gaussian)\nfitted_lorentz = fitted_model[1]   # Second component (Lorentzian)\n\n# Generate the final fitted profile\nfitted_profile = fitted_model(wavelength)\n\n# Print fitted parameters\nprint(f\"Fitted Gaussian: amplitude={fitted_gaussian.amplitude.value:.2f}, \"\n      f\"mean={fitted_gaussian.mean.value:.2f}, stddev={fitted_gaussian.stddev.value:.2f}\")\nprint(f\"Fitted Lorentzian: amplitude={fitted_lorentz.amplitude.value:.2f}, \"\n      f\"x_0={fitted_lorentz.x_0.value:.2f}, fwhm={fitted_lorentz.fwhm.value:.2f}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm analyzing gamma-ray burst durations [1.2, 0.8, 1.5, 0.9, 1.1, 1.3] seconds and want to test if they follow a log-normal distribution with mean=1.0 and sigma=0.2. How can I use astropy.stats.kuiper to perform this test?"
         },
         {
            "from": "assistant",
            "value": "import numpy as np\nfrom astropy.stats import kuiper\nfrom scipy.stats import lognorm\n\n# Gamma-ray burst durations in seconds\ngrb_durations = np.array([1.2, 0.8, 1.5, 0.9, 1.1, 1.3])\n\n# Create a cumulative distribution function (CDF) for log-normal\ndef lognormal_cdf(x, mean=1.0, sigma=0.2):\n    # Convert mean and sigma of log-normal to those of underlying normal distribution\n    mu = np.log(mean**2 / np.sqrt(mean**2 + sigma**2))\n    s = np.sqrt(np.log(1 + (sigma**2)/(mean**2)))\n    return lognorm.cdf(x, s=s, scale=np.exp(mu))\n\n# Perform Kuiper test\n# The test compares the empirical CDF of the data with the theoretical CDF\nkuiper_statistic, kuiper_pvalue = kuiper(grb_durations, lognormal_cdf)\n\nprint(f\"Kuiper test statistic: {kuiper_statistic:.4f}\")\nprint(f\"P-value: {kuiper_pvalue:.4f}\")\n# If p-value > 0.05, data is consistent with log-normal distribution\nprint(f\"Data is {'consistent' if kuiper_pvalue > 0.05 else 'inconsistent'} with log-normal distribution\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm analyzing solar flare intensity data (stored in variable 'flare_data' with values from 1e-3 to 1e5) and need to apply a square root stretch to better visualize both the core and peripheral regions. Show me how to implement this with proper normalization."
         },
         {
            "from": "assistant",
            "value": "from astropy.visualization import SqrtStretch\nfrom astropy.visualization import ImageNormalize\nimport numpy as np\n\n# Create some sample data (replace this with your actual data loading)\nflare_data = np.random.random((100, 100))  # Example random data\n# If you have actual data, you would load it like:\n# flare_data = fits.getdata('your_file.fits')\n# or however you normally load your data\n\n# Create a normalization instance with sqrt stretch\n# Normalize data to [0,1] range first before applying stretch\nnorm = ImageNormalize(\n    flare_data,\n    stretch=SqrtStretch(),\n    vmin=np.min(flare_data),\n    vmax=np.max(flare_data)\n)\n\n# Apply the normalization to the data\nstretched_data = norm(flare_data)\n\n# stretched_data is now ready for visualization\n# Values are mapped to [0,1] range with sqrt scaling applied"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I have a small stellar spectra array with shape (100, 50) of float32 data and want to update my 'SPECTRUM' HDU extension in memory without closing the file. How can I update the HDU with new header keywords 'EXPTIME'=1200.0 and 'FILTER'='H-alpha'?"
         },
         {
            "from": "assistant",
            "value": "from astropy.io import fits\nimport numpy as np\n\n# Create sample spectral data (100 wavelength points, 50 flux measurements)\nnew_spectrum = np.random.normal(size=(100, 50)).astype(np.float32)\n\n# Assuming you have an open FITS file with existing 'SPECTRUM' extension\ndef update_spectrum_hdu(hdul, new_data):\n    \"\"\"\n    Update spectrum HDU with new data and header keywords\n    hdul: fits.HDUList object containing the open FITS file\n    new_data: new spectral data array to replace existing data\n    \"\"\"\n    try:\n        # Get the 'SPECTRUM' extension\n        spec_hdu = hdul['SPECTRUM']\n        \n        # Update the data array\n        spec_hdu.data = new_data\n        \n        # Update header keywords\n        spec_hdu.header['EXPTIME'] = (1200.0, 'Exposure time in seconds')\n        spec_hdu.header['FILTER'] = ('H-alpha', 'Filter used for observation')\n        \n        # Ensure changes are written to memory\n        spec_hdu.update()\n        \n    except KeyError:\n        print(\"ERROR: 'SPECTRUM' extension not found in FITS file\")\n    except Exception as e:\n        print(f\"Error updating HDU: {str(e)}\")\n\n# Example usage (assuming hdul is your open HDUList):\n# update_spectrum_hdu(hdul, new_spectrum)"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I have X-ray data from Chandra telescope with values ranging from 1e-3 to 0.5. How can I apply a linear stretch with slope=2.5 and intercept=0.1 using LinearStretch to enhance the visibility of faint features in my image visualization?"
         },
         {
            "from": "assistant",
            "value": "from astropy.visualization import LinearStretch, ImageNormalize\nimport numpy as np\n\n# Create sample X-ray data (replace this with your actual data)\nxray_data = np.random.uniform(1e-3, 0.5, size=(100, 100))\n\n# Create a LinearStretch instance with specified slope and intercept\n# Slope controls contrast, intercept shifts the baseline\nstretch = LinearStretch(slope=2.5, intercept=0.1)\n\n# Create normalize object with the stretch\nnorm = ImageNormalize(xray_data, stretch=stretch)\n\n# Apply normalization to the data\nstretched_data = norm(xray_data)\n\n# Optional: Display the image using matplotlib\nimport matplotlib.pyplot as plt\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n\n# Original data\nim1 = ax1.imshow(xray_data, origin='lower')\nax1.set_title('Original X-ray data')\nplt.colorbar(im1, ax=ax1)\n\n# Stretched data\nim2 = ax2.imshow(stretched_data, origin='lower')\nax2.set_title('Linear stretched data')\nplt.colorbar(im2, ax=ax2)\n\nplt.tight_layout()\nplt.show()"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "What's the code to plot an exoplanet's orbital distance from its host star over time, mixing astronomical units (AU) and parsecs in the same figure?"
         },
         {
            "from": "assistant",
            "value": "from astropy import units as u\nfrom astropy.visualization import quantity_support\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Enable plotting with units\nquantity_support()\n\n# Create time array (in days)\ntime = np.linspace(0, 365, 1000) * u.day\n\n# Define orbital parameters\nsemi_major_axis = 1.2 * u.AU  # Semi-major axis\neccentricity = 0.2           # Orbital eccentricity\nperiod = 365 * u.day         # Orbital period\n\n# Calculate orbital distance over time\n# Calculate angle (making sure to handle units properly)\nangle = 2 * np.pi * (time / period) * u.rad  # Multiply by u.rad to make it an angle\ndistance = semi_major_axis * (1 - eccentricity**2) / (1 + eccentricity * np.cos(angle))\n\n# Create figure with two y-axes (AU and parsec)\nfig, ax1 = plt.subplots()\n\n# Plot distance in AU\nax1.plot(time.value, distance.value, 'b-')\nax1.set_xlabel('Time (days)')\nax1.set_ylabel('Distance (AU)')\n\n# Create second y-axis for parsecs\nax2 = ax1.twinx()\nax2.set_ylabel('Distance (pc)')\n\n# Convert AU values to parsec for second axis\ndistance_pc = distance.to(u.pc)\nax2.plot([], [])  # Empty plot to keep color cycle consistent\nax2.set_ylim(distance_pc.min().value, distance_pc.max().value)\n\nplt.title('Exoplanet Orbital Distance')\nplt.show()"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "How can I create a custom point spread function (PSF) kernel using Astro-py to model the blurring effect of atmospheric seeing in my ground-based telescope images?"
         },
         {
            "from": "assistant",
            "value": "import numpy as np\nfrom astropy.modeling.models import Gaussian2D\nfrom astropy.convolution import CustomKernel\n\ndef create_psf_kernel(fwhm, size=25):\n    \"\"\"\n    Create a 2D Gaussian PSF kernel\n    \n    Parameters:\n    -----------\n    fwhm : float\n        Full-width at half-maximum in pixels\n    size : int\n        Size of the kernel array (size x size pixels)\n    \n    Returns:\n    --------\n    CustomKernel object\n    \"\"\"\n    # Convert FWHM to sigma (standard deviation)\n    sigma = fwhm / (2.0 * np.sqrt(2.0 * np.log(2.0)))\n    \n    # Create coordinate grid\n    x = np.arange(0, size, 1, float)\n    y = x[:, np.newaxis]\n    \n    # Center coordinates\n    x0 = y0 = size // 2\n    \n    # Create 2D Gaussian model\n    gaussian_2d = Gaussian2D(amplitude=1, x_mean=x0, y_mean=y0,\n                           x_stddev=sigma, y_stddev=sigma)\n    \n    # Generate the 2D kernel array\n    kernel_array = gaussian_2d(x, y)\n    \n    # Normalize the kernel\n    kernel_array /= kernel_array.sum()\n    \n    # Create CustomKernel object\n    psf_kernel = CustomKernel(kernel_array)\n    \n    return psf_kernel\n\n# Example usage: Create a PSF kernel with FWHM of 3 pixels\nkernel = create_psf_kernel(fwhm=3.0, size=15)"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "When analyzing spectral data from a radio telescope, I want to apply a specialized smoothing filter to enhance specific frequency ranges. How can I define and apply a custom convolution kernel for this?"
         },
         {
            "from": "assistant",
            "value": "from astropy.convolution import CustomKernel, convolve\nimport numpy as np\n\n# Create sample spectral data (replace with your actual data)\nspectral_data = np.random.normal(0, 1, 1000)\n\n# Define a custom radio-frequency optimized kernel\n# This is a modified Gaussian kernel with emphasis on radio frequencies\ndef create_rf_kernel(width=5, alpha=2.0):\n    \"\"\"\n    Creates a specialized kernel for radio frequency smoothing\n    width: kernel size (odd number)\n    alpha: parameter controlling the shape of the kernel\n    \"\"\"\n    x = np.arange(-width//2 + 1, width//2 + 1)\n    # Custom kernel shape optimized for radio spectra\n    kernel = np.exp(-np.abs(x)**alpha / (2 * (width/4)**alpha))\n    return kernel / kernel.sum()  # Normalize\n\n# Create and apply the custom kernel\nkernel_data = create_rf_kernel(width=7, alpha=1.5)\ncustom_kernel = CustomKernel(kernel_data)\n\n# Apply convolution with edge handling\nsmoothed_spectrum = convolve(spectral_data, custom_kernel, boundary='extend')\n\n# Example of how to use different kernel parameters for different frequency ranges\n# For demonstration - normally you'd have actual frequency values\nlow_freq_kernel = create_rf_kernel(width=9, alpha=1.2)  # Broader kernel for low frequencies\nhigh_freq_kernel = create_rf_kernel(width=5, alpha=1.8)  # Narrower kernel for high frequencies\n\ncustom_kernel_low = CustomKernel(low_freq_kernel)\ncustom_kernel_high = CustomKernel(high_freq_kernel)"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "For my binary star analysis, I need to calculate the position angle between the primary (RA=83.12 deg, Dec=22.45 deg) and secondary (RA=83.14 deg, Dec=22.44 deg) components. How can I implement this using Astropy coordinates?"
         },
         {
            "from": "assistant",
            "value": "from astropy.coordinates import SkyCoord\nimport astropy.units as u\n\n# Create SkyCoord objects for both stars\nprimary = SkyCoord(ra=83.12*u.deg, dec=22.45*u.deg)\nsecondary = SkyCoord(ra=83.14*u.deg, dec=22.44*u.deg)\n\n# Calculate position angle (East of North)\npos_angle = primary.position_angle(secondary)\n\n# Convert to degrees for easier interpretation\npos_angle_deg = pos_angle.to(u.deg)\n\nprint(f\"Position angle between components: {pos_angle_deg:.2f}\")\n\n# Also calculate separation for completeness\nseparation = primary.separation(secondary)\nprint(f\"Angular separation: {separation.to(u.arcsec):.2f}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm comparing star positions from multiple catalogs spanning several decades. How can I handle the time differences when one catalog uses Besselian epochs and others use J2000?"
         },
         {
            "from": "assistant",
            "value": "from astropy.time import Time\nfrom astropy.coordinates import SkyCoord\nimport astropy.units as u\n\n# Example star position from an old catalog (B1950.0 epoch)\n# Coordinates are RA=20h 41m 25.91s, Dec=+45d 16m 49.22s\n# Adding example proper motion and radial velocity data\nold_pos = SkyCoord('20h41m25.91s +45d16m49.22s', \n                   frame='fk4',  # FK4 system used for B1950\n                   equinox='B1950.0')\n\n# Convert to J2000 epoch\nj2000_pos = old_pos.transform_to('fk5')  # FK5 is the system for J2000\n\n# If you need to convert to a specific epoch\n# Example: converting to epoch 2015.5\ntarget_epoch = Time('2015.5', format='decimalyear')\n\n# Create SkyCoord with proper motion and radial velocity\ncurrent_pos = SkyCoord(ra=j2000_pos.ra, \n                      dec=j2000_pos.dec,\n                      pm_ra_cosdec=100*u.mas/u.yr,  # Example proper motion in RA\n                      pm_dec=50*u.mas/u.yr,         # Example proper motion in Dec\n                      radial_velocity=20*u.km/u.s,  # Example radial velocity\n                      distance=100*u.pc,            # Example distance\n                      frame='fk5',\n                      equinox='J2000',\n                      obstime=Time('J2000'))\n\n# Now apply space motion\ncurrent_pos = current_pos.apply_space_motion(new_obstime=target_epoch)\n\n# Print results\nprint(f\"Original B1950 position:\\n{old_pos}\")\nprint(f\"J2000 position:\\n{j2000_pos}\")\nprint(f\"Position at epoch 2015.5:\\n{current_pos}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm developing a pipeline to process data from multiple observatories. How do I create a list of EarthLocation objects for major observatories in the Southern Hemisphere using their geographical coordinates?"
         },
         {
            "from": "assistant",
            "value": "from astropy.coordinates import EarthLocation\nimport astropy.units as u\n\n# Dictionary of major Southern Hemisphere observatories with their coordinates\n# Format: 'name': (longitude, latitude, elevation)\nsouthern_observatories = {\n    'ESO Paranal': (-70.4045, -24.6271, 2635.0),   # Chile\n    'ALMA': (-67.7552, -23.0229, 5058.7),          # Chile\n    'SAAO': (20.8108, -32.3766, 1798.0),           # South Africa\n    'AAT': (149.0661, -31.2770, 1164.0),           # Australia\n    'CTIO': (-70.8150, -30.1697, 2215.0)           # Chile\n}\n\n# Create a dictionary of EarthLocation objects\nobservatory_locations = {}\n\nfor name, coords in southern_observatories.items():\n    try:\n        # Create EarthLocation object with longitude, latitude (deg) and height (m)\n        location = EarthLocation.from_geodetic(\n            lon=coords[0]*u.deg,\n            lat=coords[1]*u.deg,\n            height=coords[2]*u.m\n        )\n        observatory_locations[name] = location\n    except Exception as e:\n        print(f\"Error creating location for {name}: {e}\")\n\n# Example usage: access a specific observatory's location\nparanal = observatory_locations['ESO Paranal']"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm studying radial velocity curves of binary stars and need to reduce high-frequency noise. Could you show me how to implement a Gaussian smoothing kernel with stddev=2.0 and factor=15 for better oversampling?"
         },
         {
            "from": "assistant",
            "value": "from astropy.convolution import Gaussian1DKernel, convolve\nimport numpy as np\n\n# Create sample radial velocity data (example)\ntime = np.linspace(0, 10, 1000)  # 10 days of observations\nrv = 50 * np.sin(2*np.pi*time/3.2) + np.random.normal(0, 5, len(time))  # RV curve with noise\n\n# Create Gaussian kernel for smoothing\n# stddev=2.0 controls the width of the Gaussian\n# x_size=15 ensures sufficient kernel width for smoothing\nkernel = Gaussian1DKernel(stddev=2.0, x_size=15)\n\n# Apply convolution to smooth the RV curve\n# Boundary handling is set to 'extend' to avoid edge effects\nsmoothed_rv = convolve(rv, kernel, boundary='extend')\n\n# The smoothed_rv array now contains the filtered radial velocity curve\n# with reduced high-frequency noise while preserving the underlying signal"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I need to process a FITS file containing Hubble Space Telescope images while preserving the original data scaling. What's the best way to load this data without applying automatic BZERO/BSCALE transformations?"
         },
         {
            "from": "assistant",
            "value": "from astropy.io import fits\nimport numpy as np\nimport os\n\ndef process_fits_file(filename):\n    try:\n        # Check if file exists\n        if not os.path.exists(filename):\n            raise FileNotFoundError(f\"The file {filename} does not exist\")\n            \n        # Open FITS file without automatically applying BZERO/BSCALE transformations\n        # do_not_scale_image_data=True preserves raw data values\n        with fits.open(filename, do_not_scale_image_data=True) as hdul:\n            # Get primary HDU\n            hdu = hdul[0]\n            \n            # Get the original scaling parameters from header\n            bzero = hdu.header.get('BZERO', 0)\n            bscale = hdu.header.get('BSCALE', 1)\n            \n            # Get raw data\n            raw_data = hdu.data\n            \n            # If you need the physically meaningful values, apply scaling manually\n            physical_data = np.multiply(raw_data, bscale) + bzero\n            \n            # Access header information\n            header = hdu.header.copy()  # Make a copy of the header\n            \n            return physical_data, header\n            \n    except FileNotFoundError as e:\n        print(f\"Error: {e}\")\n        return None, None\n    except Exception as e:\n        print(f\"An unexpected error occurred: {e}\")\n        return None, None\n\n# Example usage\nfits_file = 'hst_image.fits'  # Replace with your actual FITS file path\ndata, header = process_fits_file(fits_file)\n\nif data is not None:\n    print(\"Successfully processed FITS file\")\n    print(f\"Data shape: {data.shape}\")\n    print(\"Header information:\")\n    print(header)\nelse:\n    print(\"Failed to process FITS file\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm processing multiple exoplanet transit observations and need to add quality flags in the FITS headers. I want these flags to be placed right after the observation date keyword. How should I structure this?"
         },
         {
            "from": "assistant",
            "value": "from astropy.io import fits\nfrom datetime import datetime\n\ndef add_quality_flags(fits_file, quality_value):\n    \"\"\"\n    Add quality flags after DATE-OBS keyword in FITS header\n    quality_value: integer 0-5 indicating data quality (0=best, 5=worst)\n    \"\"\"\n    try:\n        # Open FITS file in update mode\n        with fits.open(fits_file, mode='update') as hdul:\n            header = hdul[0].header\n            \n            # Get index of DATE-OBS keyword\n            date_idx = header.index('DATE-OBS')\n            \n            # Insert quality flag after DATE-OBS\n            header.insert(\n                date_idx + 1,\n                ('QUALITY', quality_value, 'Data quality flag (0=best, 5=worst)'),\n                after='DATE-OBS'\n            )\n            \n            # Add processing timestamp\n            header.insert(\n                date_idx + 2,\n                ('QC_DATE', datetime.now().isoformat(), 'Quality check timestamp'),\n                after='QUALITY'\n            )\n            \n            # Save changes\n            hdul.flush()\n            \n    except Exception as e:\n        print(f\"Error processing {fits_file}: {str(e)}\")\n\n# Example usage\nfits_files = ['transit1.fits', 'transit2.fits', 'transit3.fits']\nfor file in fits_files:\n    # Assuming quality assessment is done elsewhere\n    quality_value = 1  # Example quality value\n    add_quality_flags(file, quality_value)"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I have a coordinate in the Orion Nebula (RA=83.82208 deg, Dec=-5.39111 deg) and a WCS object. How can I convert this specific sky position to pixel coordinates in my image using astropy, assuming 0-based indexing?"
         },
         {
            "from": "assistant",
            "value": "from astropy.coordinates import SkyCoord\nfrom astropy import units as u\nfrom astropy.wcs import WCS\nfrom astropy.io import fits\nimport os\n\ndef convert_coords_to_pixels(fits_file_path):\n    \"\"\"\n    Convert sky coordinates to pixel coordinates for a given FITS file\n    \n    Parameters:\n    fits_file_path (str): Path to the FITS file\n    \n    Returns:\n    tuple: Pixel coordinates (x, y) if successful, None if failed\n    \"\"\"\n    # Check if the file exists before trying to open it\n    if not os.path.exists(fits_file_path):\n        print(f\"Warning: The FITS file '{fits_file_path}' does not exist.\")\n        print(\"Please ensure you have specified the correct file path.\")\n        print(\"Current working directory:\", os.getcwd())\n        print(\"Available files in current directory:\", os.listdir())\n        return None\n\n    try:\n        # Open the FITS file and create a WCS object\n        with fits.open(fits_file_path) as hdul:\n            header = hdul[0].header\n            wcs = WCS(header)\n\n            # Create a SkyCoord object for the Orion Nebula position\n            orion_coord = SkyCoord(ra=83.82208*u.deg, dec=-5.39111*u.deg)\n\n            # Convert sky coordinates to pixel coordinates (0-based indexing)\n            pixel_x, pixel_y = wcs.world_to_pixel(orion_coord)\n\n            return pixel_x, pixel_y\n\n    except FileNotFoundError as e:\n        print(f\"File not found error: {str(e)}\")\n    except Exception as e:\n        print(f\"An unexpected error occurred: {str(e)}\")\n        print(f\"Error type: {type(e).__name__}\")\n    \n    return None\n\n# Define the path to your FITS file\n# Replace this path with the actual path to your FITS file\nfits_file = '/path/to/your/actual/image.fits'  # <-- Replace this with your actual FITS file path\n\n# Try to convert coordinates\nresult = convert_coords_to_pixels(fits_file)\n\nif result is not None:\n    pixel_x, pixel_y = result\n    print(f\"Pixel coordinates: x={pixel_x:.2f}, y={pixel_y:.2f}\")\nelse:\n    print(\"Could not convert coordinates. Please check the file path and try again.\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm studying the light curve of an eclipsing binary system. Can you show me how to combine a Gaussian model for the primary eclipse with a smaller Gaussian for the secondary eclipse using astropy's modeling?"
         },
         {
            "from": "assistant",
            "value": "from astropy.modeling import models, fitting\nimport numpy as np\n\n# Generate sample time points (0 to 1 phase)\ntime = np.linspace(0, 1, 1000)\n\n# Create primary eclipse Gaussian\nprimary = models.Gaussian1D(\n    amplitude=-0.5,  # Depth of primary eclipse\n    mean=0.25,      # Center of primary eclipse\n    stddev=0.05     # Width of primary eclipse\n)\n\n# Create secondary eclipse Gaussian\nsecondary = models.Gaussian1D(\n    amplitude=-0.2,  # Depth of secondary eclipse (smaller)\n    mean=0.75,      # Center of secondary eclipse\n    stddev=0.05     # Width of secondary eclipse\n)\n\n# Combine models using compound model\nbinary_model = primary + secondary\n\n# Generate light curve (adding 1 to normalize the baseline to unity)\nlight_curve = 1 + binary_model(time)\n\n# Note: In real applications, you would fit this to actual data using:\n# fitter = fitting.LevMarLSQFitter()\n# best_fit = fitter(binary_model, time, observed_data)"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm analyzing pulsar timing data and need to create a histogram of pulse periods. What's the best way to determine the bin width statistically rather than by trial and error?"
         },
         {
            "from": "assistant",
            "value": "import numpy as np\nfrom astropy.stats import scott_bin_width, freedman_bin_width\nimport matplotlib.pyplot as plt\n\n# Sample pulsar period data in seconds (replace with your actual data)\nperiods = np.array([0.089, 0.091, 0.087, 0.092, 0.088, 0.090, 0.089, \n                   0.086, 0.088, 0.091, 0.090, 0.087, 0.089, 0.088]) \n\n# Calculate optimal bin width using Scott's rule\n# Suitable for approximately normally distributed data\nscott_bw = scott_bin_width(periods)\n\n# Calculate optimal bin width using Freedman-Diaconis rule\n# More robust to outliers than Scott's rule\nfd_bw = freedman_bin_width(periods)\n\n# Create histogram using Freedman-Diaconis bin width\n# (generally preferred for astronomical time series)\nnum_bins = int(np.ceil((periods.max() - periods.min()) / fd_bw))\n\nplt.hist(periods, bins=num_bins, edgecolor='black')\nplt.xlabel('Pulse Period (s)')\nplt.ylabel('Count')\nplt.title(f'Pulsar Period Histogram\\nBin width = {fd_bw:.6f} s')\nplt.show()\n\nprint(f\"Scott's rule bin width: {scott_bw:.6f} s\")\nprint(f\"Freedman-Diaconis bin width: {fd_bw:.6f} s\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "For my galaxy evolution study, I need to create two Sersic1D models representing a galaxy at different epochs. Can you show me how to plot both profiles (first with n=2.5, r_eff=4 kpc, amplitude=18 and second with n=3.2, r_eff=6 kpc, amplitude=15) on the same graph with different colors and a proper legend?"
         },
         {
            "from": "assistant",
            "value": "from astropy.modeling.models import Sersic1D\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Create radius array in kpc\nr = np.linspace(0, 20, 200)\n\n# Create two Sersic profiles for different epochs\nearly_galaxy = Sersic1D(amplitude=18, r_eff=4, n=2.5)\nlate_galaxy = Sersic1D(amplitude=15, r_eff=6, n=3.2)\n\n# Plot both profiles\nplt.figure(figsize=(10, 6))\nplt.plot(r, early_galaxy(r), 'b-', label='Early epoch (n=2.5)')\nplt.plot(r, late_galaxy(r), 'r--', label='Late epoch (n=3.2)')\n\n# Customize the plot\nplt.xlabel('Radius (kpc)')\nplt.ylabel('Surface Brightness')\nplt.title('Galaxy Evolution: Sersic Profiles Comparison')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n# Use log scale for y-axis as surface brightness profiles are typically shown this way\nplt.yscale('log')\n\n# Show the plot\nplt.show()"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm studying star clusters and want to identify possible binary star systems. How can I find all stellar pairs in my dataset that are separated by less than 5 arcseconds on the sky?"
         },
         {
            "from": "assistant",
            "value": "from astropy.coordinates import SkyCoord\nfrom astropy import units as u\nimport numpy as np\n\ndef find_close_pairs(ra_list, dec_list, max_separation=5*u.arcsec):\n    \"\"\"\n    Find stellar pairs with angular separation less than specified threshold\n    \n    Parameters:\n    ra_list: List of Right Ascension values in degrees\n    dec_list: List of Declination values in degrees\n    max_separation: Maximum separation threshold (default: 5 arcseconds)\n    \"\"\"\n    # Create SkyCoord object for all stars\n    coords = SkyCoord(ra=ra_list*u.degree, dec=dec_list*u.degree)\n    \n    # Initialize list to store pairs\n    close_pairs = []\n    \n    # Compare each star with all others\n    for i in range(len(coords)):\n        # Calculate separation to all other stars\n        separations = coords[i].separation(coords[i+1:])\n        \n        # Find indices where separation is less than threshold\n        close_indices = np.where(separations < max_separation)[0]\n        \n        # Add pairs to list (adjusting indices)\n        for idx in close_indices:\n            close_pairs.append((i, i+1+idx, separations[idx].arcsec))\n    \n    return close_pairs\n\n# Example usage\nif __name__ == \"__main__\":\n    # Sample data (RA and Dec in degrees)\n    ra = np.array([10.5, 10.501, 10.6, 10.7])\n    dec = np.array([-20.1, -20.101, -20.2, -20.3])\n    \n    pairs = find_close_pairs(ra, dec)\n    \n    # Print results\n    for i, j, sep in pairs:\n        print(f\"Stars {i} and {j} are separated by {sep:.2f} arcseconds\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm studying Jupiter's orbital dynamics and need its barycentric position and velocity at multiple time steps over a 6-month period to analyze its influence on nearby asteroids. How can I compute this?"
         },
         {
            "from": "assistant",
            "value": "from astropy.time import Time\nfrom astropy.coordinates import get_body_barycentric_posvel\nimport numpy as np\n\n# Set up time array for 6 months with daily steps\nt_start = Time('2024-01-01')\ndays = np.arange(0, 180)  # 180 days = ~6 months\ntimes = t_start + days\n\n# Get Jupiter's barycentric position and velocity for each time\n# Returns position in AU and velocity in AU/day\npositions = []\nvelocities = []\nfor t in times:\n    pos, vel = get_body_barycentric_posvel('jupiter', t)\n    positions.append(pos.xyz.value)  # Extract Cartesian coordinates in AU\n    velocities.append(vel.xyz.value)  # Extract velocity components in AU/day\n\n# Convert to numpy arrays for easier analysis\npositions = np.array(positions)  # Shape: (180, 3) - [x, y, z] in AU\nvelocities = np.array(velocities)  # Shape: (180, 3) - [vx, vy, vz] in AU/day\n\n# Example: Print Jupiter's position and velocity at first timestep\nprint(f\"Position (AU) at {t_start.iso}:\", positions[0])\nprint(f\"Velocity (AU/day) at {t_start.iso}:\", velocities[0])"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm analyzing data from the Hubble Deep Field with a 2048x2048 array. Could you show me how to display it using imshow_norm with a minimum cutoff at 500 counts and maximum at 10000 counts, while applying a linear stretch to better show the galaxy details?"
         },
         {
            "from": "assistant",
            "value": "import numpy as np\nfrom astropy.visualization import ImageNormalize, LinearStretch\nimport matplotlib.pyplot as plt\n\n# Create sample data array (in real case, this would be your HDF data)\ndata = np.random.normal(1000, 2000, size=(2048, 2048))\n\n# Create normalization object with specified min/max values and linear stretch\nnorm = ImageNormalize(\n    data,\n    vmin=500,      # Minimum cutoff at 500 counts\n    vmax=10000,    # Maximum cutoff at 10000 counts\n    stretch=LinearStretch()\n)\n\n# Display the image\nplt.figure(figsize=(10, 10))\nplt.imshow(data, origin='lower', norm=norm, cmap='viridis')\nplt.colorbar(label='Counts')\nplt.title('Hubble Deep Field')\nplt.show()"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I need to model the temperature variation across a solar flare region, where my detector spans 1000x1000 pixels. Write a code snippet using Polynomial2D to create a second-degree model, with the domain matching my detector size and setting initial coefficient c00 to 5000K."
         },
         {
            "from": "assistant",
            "value": "from astropy.modeling import models, fitting\nimport numpy as np\n\n# Create coordinate grid matching detector dimensions\ny, x = np.mgrid[0:1000, 0:1000]\n\n# Initialize 2D polynomial model\n# c00=5000 sets the base temperature to 5000K\n# degree=2 creates a second-degree polynomial\ntemp_model = models.Polynomial2D(degree=2, c0_0=5000)\n\n# Generate synthetic temperature distribution\n# This creates a 1000x1000 array of temperatures based on the model\ntemp_distribution = temp_model(x, y)\n\n# Example of how to fit this model to actual data:\n# If you have observed_temps data:\n# fitter = fitting.LevMarLSQFitter()\n# fitted_model = fitter(temp_model, x, y, observed_temps)\n\n# Print model parameters\nprint(\"Model parameters:\")\nprint(f\"Base temperature (c0_0): {temp_model.c0_0.value} K\")\nprint(f\"x coefficient (c1_0): {temp_model.c1_0.value}\")\nprint(f\"y coefficient (c0_1): {temp_model.c0_1.value}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "For my exoplanet transit model, I need to add a DC offset. Show me how to create a Const1D model with amplitude 15.5 magnitudes, and fix this parameter so it won't vary during the fitting process."
         },
         {
            "from": "assistant",
            "value": "# Import the required AstroPy modeling class\nfrom astropy.modeling.models import Const1D\n\n# Create a constant model with amplitude 15.5 magnitudes\ndc_offset = Const1D(amplitude=15.5)\n\n# Fix the amplitude parameter so it won't be varied during fitting\ndc_offset.amplitude.fixed = True\n\n# The model is now ready to be used in combination with other models\n# Example of how to evaluate the model at some points:\nimport numpy as np\nx = np.linspace(0, 10, 100)\nconstant_values = dc_offset(x)  # Will be array of 15.5s"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm preparing astronomical images for a publication, and I need to enhance the visibility of diffuse nebulae without oversaturating the bright stars. Can you show me how to apply the appropriate scaling transformation?"
         },
         {
            "from": "assistant",
            "value": "from astropy.io import fits\nfrom astropy.visualization import (ImageNormalize, AsinhStretch,\n                                 MinMaxInterval, ZScaleInterval)\nimport matplotlib.pyplot as plt\n\n# Load the FITS image\ndef enhance_nebula(fits_file, output_file='enhanced_nebula.png'):\n    try:\n        # Open the FITS file and get the data\n        with fits.open(fits_file) as hdul:\n            image_data = hdul[0].data\n        \n        # Create normalization using asinh stretch\n        # AsinhStretch is excellent for bringing out faint features\n        norm = ImageNormalize(\n            image_data,\n            interval=ZScaleInterval(),  # Automatic contrast limits\n            stretch=AsinhStretch(a=0.1)  # 'a' parameter controls stretch intensity\n        )\n        \n        # Create the figure and display the image\n        plt.figure(figsize=(10, 10))\n        plt.imshow(image_data, norm=norm, cmap='viridis')\n        plt.colorbar(label='Intensity')\n        plt.title('Enhanced Nebula Visualization')\n        \n        # Save the enhanced image\n        plt.savefig(output_file, dpi=300, bbox_inches='tight')\n        plt.close()\n        \n        return True\n        \n    except Exception as e:\n        print(f\"Error processing image: {str(e)}\")\n        return False\n\n# Usage example:\n# enhance_nebula('your_nebula.fits')"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I have photometric measurements of 1500 variable stars' magnitudes ranging from 12 to 18 mag. I need to create a histogram of their brightness distribution - how can I determine the optimal bin width using the Freedman-Diaconis rule and plot it with appropriate error bars?"
         },
         {
            "from": "assistant",
            "value": "import numpy as np\nfrom astropy.stats import freedman_bin_width\nimport matplotlib.pyplot as plt\n\n# Simulate sample data (replace with your actual data)\nnp.random.seed(42)\nmagnitudes = np.random.uniform(12, 18, 1500)\n\n# Calculate optimal bin width using Freedman-Diaconis rule\nbin_width = freedman_bin_width(magnitudes)\nn_bins = int(np.ceil((magnitudes.max() - magnitudes.min()) / bin_width))\n\n# Create histogram with optimal binning\ncounts, bin_edges = np.histogram(magnitudes, bins=n_bins)\nbin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n\n# Calculate Poisson errors for each bin\nerrors = np.sqrt(counts)\n\n# Create the histogram plot with error bars\nplt.figure(figsize=(10, 6))\nplt.bar(bin_centers, counts, width=bin_width, alpha=0.5, \n        color='skyblue', yerr=errors, capsize=3)\n\n# Customize the plot\nplt.xlabel('Magnitude')\nplt.ylabel('Number of Stars')\nplt.title(f'Magnitude Distribution (bin width = {bin_width:.2f} mag)')\nplt.grid(True, alpha=0.3)\n\n# Display the plot\nplt.show()"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm working with spectroscopic data and need to create a WCS object for a velocity cube with dimensions 100x100x50. The third axis should be velocity in km/s, ranging from -500 to +500 km/s. How can I set this up using astropy.wcs.WCS?"
         },
         {
            "from": "assistant",
            "value": "from astropy.wcs import WCS\nimport numpy as np\n\n# Create a new WCS object with 3 dimensions\nw = WCS(naxis=3)\n\n# Set basic WCS parameters\nw.wcs.crpix = [50.0, 50.0, 25.0]  # Reference pixel (center of cube)\nw.wcs.cdelt = [-0.1, 0.1, 20.0]    # Pixel scale: spatial (deg) and velocity (km/s)\nw.wcs.crval = [0.0, 0.0, 0.0]      # Reference position and velocity\nw.wcs.ctype = ['RA---TAN', 'DEC--TAN', 'VELO-LSR']  # Axis types\n\n# Set the velocity range (-500 to +500 km/s over 50 pixels)\n# Velocity axis is the third dimension\nvel_start = -500  # km/s\nvel_end = 500     # km/s\nn_vel = 50\n\n# Calculate velocity increment per pixel\ndv = (vel_end - vel_start) / (n_vel - 1)\nw.wcs.cdelt[2] = dv\nw.wcs.crpix[2] = 1\nw.wcs.crval[2] = vel_start\n\n# Create example coordinates to verify\npixel_coords = np.array([[50, 50, 25]])  # Center pixel\nworld_coords = w.wcs_pix2world(pixel_coords, 0)\nprint(f\"Center pixel world coordinates: {world_coords}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "Show me how to create a SupernovaTable class with a discovery_year attribute defaulting to 2023 using TableAttribute, and then create a table instance setting the discovery_year to 2021."
         },
         {
            "from": "assistant",
            "value": "from astropy.table import Table, TableAttribute\n\nclass SupernovaTable(Table):\n    # Define discovery_year as a table attribute with default value 2023\n    discovery_year = TableAttribute(default=2023)\n\n# Create instance with sample data and custom discovery year\nsn_data = [\n    ('SN2021abc', '12:34:56', '-45:67:89', 18.5),\n    ('SN2021xyz', '01:23:45', '+12:34:56', 19.2)\n]\n\n# Create table with columns and set discovery_year to 2021\nsn_table = SupernovaTable(\n    rows=sn_data,\n    names=('name', 'ra', 'dec', 'magnitude'),\n    discovery_year=2021  # Override default value\n)\n\n# Verify the attribute\nprint(f\"Discovery year: {sn_table.discovery_year}\")\nprint(\"\\nTable contents:\")\nprint(sn_table)"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I have separate tables containing radio and optical observations of a sample of quasars, and I want to create a single table that includes all wavelength measurements. What's the proper way to combine these datasets?"
         },
         {
            "from": "assistant",
            "value": "from astropy.table import Table, vstack, join\n\n# Create example tables (replace with your actual data loading)\nradio_data = Table({\n    'source_id': ['QSO1', 'QSO2', 'QSO3'],\n    'radio_flux': [0.5, 0.3, 0.8],\n    'radio_freq': [1.4, 1.4, 1.4]\n})\n\noptical_data = Table({\n    'source_id': ['QSO1', 'QSO2', 'QSO4'],\n    'magnitude': [18.5, 19.2, 20.1],\n    'filter': ['r', 'r', 'r']\n})\n\n# Method 1: Join tables based on source_id\n# This keeps only sources present in both tables\ncombined_inner = join(radio_data, optical_data, keys='source_id', join_type='inner')\n\n# Method 2: Join tables keeping all sources\n# This keeps all sources, filling missing data with masked values\ncombined_outer = join(radio_data, optical_data, keys='source_id', join_type='outer')\n\n# Print the resulting table\nprint(\"Inner join result (matching sources only):\")\nprint(combined_inner)\nprint(\"\\nOuter join result (all sources):\")\nprint(combined_outer)"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm working with ICRS coordinate data and need to create a corresponding WCS object for my catalog analysis. How can I convert my coordinate frame to WCS with a Mercator projection?"
         },
         {
            "from": "assistant",
            "value": "from astropy.wcs import WCS\nfrom astropy.coordinates import ICRS\nimport numpy as np\n\n# Create a basic WCS object with Mercator projection\ndef create_mercator_wcs(ra_range=(0, 360), dec_range=(-85, 85), pixels=(500, 300)):\n    \"\"\"\n    Create a WCS with Mercator projection from ICRS coordinates\n    \n    Parameters:\n    ra_range: tuple of (min, max) RA in degrees\n    dec_range: tuple of (min, max) Dec in degrees (limit to \u00b185\u00b0 for Mercator)\n    pixels: tuple of (width, height) for the output projection\n    \"\"\"\n    \n    # Initialize WCS object\n    w = WCS(naxis=2)\n    \n    # Set basic WCS parameters\n    w.wcs.crpix = [pixels[0]/2.0, pixels[1]/2.0]  # Reference pixel at center\n    w.wcs.cdelt = [(ra_range[1]-ra_range[0])/pixels[0],\n                   (dec_range[1]-dec_range[0])/pixels[1]]  # Pixel scale\n    w.wcs.crval = [(ra_range[1]+ra_range[0])/2.0,\n                   (dec_range[1]+dec_range[0])/2.0]  # Reference coordinate\n    w.wcs.ctype = [\"RA---MER\", \"DEC--MER\"]  # Mercator projection\n    w.wcs.set_pv([(2, 1, 1.0)])  # Mercator projection parameter\n    \n    return w\n\n# Create WCS object\nwcs = create_mercator_wcs()"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "During my spectral analysis, I need to model a rectangular emission line profile centered at 6563 Angstroms (H-alpha) with a width of 10 Angstroms and an amplitude of 2.5. Can you show me how to create this model using Box1D for wavelengths between 6550 and 6580 Angstroms?"
         },
         {
            "from": "assistant",
            "value": "# Import necessary modules from astropy\nfrom astropy.modeling import models\nimport numpy as np\nfrom astropy import units as u\n\n# Create wavelength array between 6550-6580 Angstroms\nwavelengths = np.linspace(6550, 6580, 100) * u.Angstrom\n\n# Create Box1D model for rectangular emission line\n# Parameters: amplitude=2.5, width=10 Angstroms, position=6563 Angstroms\n# Add units to all parameters\nbox_model = models.Box1D(\n    amplitude=2.5,\n    width=10 * u.Angstrom,\n    x_0=6563 * u.Angstrom\n)\n\n# Keep the units when evaluating the model\n# No need to strip units from wavelengths\nprofile = box_model(wavelengths)\n\n# The resulting 'profile' array contains the rectangular emission line values\n# at each wavelength point. The profile will be 2.5 between 6558-6568 Angstroms\n# (center \u00b1 width/2) and 0 elsewhere"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "For my exoplanet research, I have orbital period measurements of 150 exoplanets ranging from 1 to 365 days. How can I determine the optimal bin width using the Freedman-Diaconis method and create a log-scaled histogram of the period distribution?"
         },
         {
            "from": "assistant",
            "value": "import numpy as np\nfrom astropy.stats import freedman_bin_width\nimport matplotlib.pyplot as plt\n\n# Generate sample data (replace this with your actual data)\nnp.random.seed(42)\nperiods = np.random.lognormal(mean=3.0, sigma=1.2, size=150)\nperiods = np.clip(periods, 1, 365)  # Constrain to 1-365 day range\n\n# Calculate optimal bin width using Freedman-Diaconis rule on log-transformed data\nlog_periods = np.log10(periods)\noptimal_width = freedman_bin_width(log_periods)\n\n# Calculate number of bins\nbin_range = np.log10(365) - np.log10(1)\nn_bins = int(np.ceil(bin_range / optimal_width))\n\n# Create histogram with logarithmic scaling\nplt.figure(figsize=(10, 6))\nplt.hist(periods, bins=10**np.linspace(np.log10(1), np.log10(365), n_bins))\nplt.xscale('log')\nplt.xlabel('Orbital Period (days)')\nplt.ylabel('Number of Exoplanets')\nplt.title('Distribution of Exoplanet Orbital Periods')\nplt.grid(True, alpha=0.3)\nplt.show()"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm working on a supernova energy calculation. Can you show me how to convert the measured explosion energy of 2.5 foe to ergs using astropy units?"
         },
         {
            "from": "assistant",
            "value": "from astropy import units as u\n\n# Define the explosion energy in foe (1 foe = 10^51 ergs)\n# Common unit in supernova astronomy\nexplosion_energy = 2.5 * u.foe\n\n# Convert to ergs\nenergy_in_ergs = explosion_energy.to(u.erg)\n\nprint(f\"Supernova explosion energy: {energy_in_ergs:.2e}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm analyzing data from my pulsar search pipeline which found 28 candidates from 150 pointings. What's the code to estimate the detection probability with error bounds at 95% confidence level?"
         },
         {
            "from": "assistant",
            "value": "from astropy.stats import binom_conf_interval\nimport numpy as np\n\n# Input data\nn_detections = 28    # Number of successful detections\nn_trials = 150      # Total number of pointings\nconfidence = 0.95   # 95% confidence level\n\n# Calculate detection probability and confidence intervals\ndetection_prob = n_detections / n_trials\nconf_intervals = binom_conf_interval(n_detections, n_trials, \n                                   confidence_level=confidence,\n                                   interval='wilson')\n\n# Extract lower and upper bounds\nlower_bound, upper_bound = conf_intervals\n\nprint(f\"Detection Probability: {detection_prob:.3f}\")\nprint(f\"95% Confidence Interval: [{lower_bound:.3f}, {upper_bound:.3f}]\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I have measurements of exoplanet transit depths and need to understand the statistical uncertainty in the median depth. Can you show me how to perform bootstrap analysis on this data?"
         },
         {
            "from": "assistant",
            "value": "import numpy as np\nfrom astropy import units as u\nfrom astropy.stats import bootstrap\n\n# Define ppm as a fraction (1e-6) and add it to enabled units\nppm = u.def_unit('ppm', (1e-6 * u.dimensionless_unscaled))\nu.add_enabled_units([ppm])\n\n# Sample transit depth measurements in parts per million (ppm)\ntransit_depths = np.array([1450, 1380, 1520, 1440, 1390, 1480, 1410, 1460]) * ppm\n\n# Function to compute median with units preserved\ndef get_median(data):\n    return np.median(data)\n\n# Perform bootstrap analysis with 10000 resamples\nbootresult = bootstrap(transit_depths, bootnum=10000, bootfunc=get_median)\n\n# Calculate confidence intervals (16th and 84th percentiles for 1-sigma)\nconfidence_interval = np.percentile(bootresult, [16, 84])\n\n# Print results\nprint(f\"Median transit depth: {np.median(transit_depths):.0f}\")\nprint(f\"Bootstrap uncertainty: +{confidence_interval[1]-np.median(transit_depths):.0f}, \"\n      f\"-{np.median(transit_depths)-confidence_interval[0]:.0f}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I need to create a 2D polynomial model of degree 3 to represent the temperature distribution across a stellar surface, with constraints where the temperature can't go below 2000K or above 8000K. Can you show me how to set up Polynomial2D with these bounds?"
         },
         {
            "from": "assistant",
            "value": "from astropy.modeling import models, fitting\nfrom astropy.modeling.parameters import Parameter\nimport numpy as np\n\n# Create a 2D polynomial model of degree 3\npoly_model = models.Polynomial2D(degree=3)\n\n# Set bounds for all coefficients to constrain temperature range\n# We'll set reasonable bounds that will help keep temps between 2000K and 8000K\nfor param_name in poly_model.param_names:\n    # Set parameter bounds to constrain the polynomial\n    # These bounds are approximate and may need adjustment based on your data scale\n    getattr(poly_model, param_name).bounds = (-1e4, 1e4)  # Simplified way to set bounds\n\n# Create a custom class that enforces temperature bounds\nclass BoundedPoly2D(models.Polynomial2D):\n    def evaluate(self, x, y, *coeffs):\n        # Get the regular polynomial result\n        result = super().evaluate(x, y, *coeffs)\n        # Apply temperature bounds\n        return np.clip(result, 2000, 8000)\n\n# Create the bounded model\nbounded_model = BoundedPoly2D(degree=3)\n\n# Example of how to use it:\n# Generate sample grid points\nx, y = np.mgrid[-1:1:20j, -1:1:20j]\n\n# Create sample data (simulated temperature distribution)\nz = 5000 + 1000*np.exp(-(x**2 + y**2))  # Example temperature distribution\nz += np.random.normal(0, 100, x.shape)  # Add some noise\n\n# Fit the model\nfitter = fitting.LevMarLSQFitter()\nfitted_model = fitter(bounded_model, x, y, z)"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "Working with Hubble Space Telescope data, I need to update the 3rd extension containing a raw image array of shape (1024, 1024). The new data has the same shape but was dark-subtracted. Please show how to update this extension while preserving the original WCS information in the header."
         },
         {
            "from": "assistant",
            "value": "from astropy.io import fits\nfrom astropy.wcs import WCS\nimport os\nimport sys\n\ndef process_fits_file(input_file, output_file):\n    # Check if input file exists\n    if not os.path.exists(input_file):\n        print(f\"Error: Input file '{input_file}' does not exist.\")\n        sys.exit(1)\n        \n    try:\n        # Open the original HST FITS file\n        with fits.open(input_file) as hdul:\n            # Verify the file has enough extensions\n            if len(hdul) <= 3:\n                print(f\"Error: FITS file must have at least 4 extensions. Found {len(hdul)}.\")\n                return\n            \n            # Store the WCS information from the 3rd extension\n            wcs = WCS(hdul[3].header)\n            \n            # Here you would process your dark-subtracted data\n            # For this example, we'll just use the existing data\n            dark_subtracted_data = hdul[3].data\n            \n            # Create new ImageHDU with dark-subtracted data and original header\n            new_hdu = fits.ImageHDU(data=dark_subtracted_data, header=hdul[3].header)\n            \n            # Update the header to indicate processing\n            new_hdu.header['HISTORY'] = 'Dark subtraction applied'\n            \n            # Replace the 3rd extension\n            hdul[3] = new_hdu\n            \n            # Write updated FITS file\n            hdul.writeto(output_file, overwrite=True)\n            print(f\"Successfully processed {input_file} and saved to {output_file}\")\n            \n    except Exception as e:\n        print(f\"Error processing FITS file: {str(e)}\")\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    # Example usage\n    input_fits = \"original_hst.fits\"  # Replace with your input file path\n    output_fits = \"updated_hst.fits\"  # Replace with your output file path\n    \n    process_fits_file(input_fits, output_fits)"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "During my stellar proper motion study, I need to measure the positions of nearby stars relative to a stationary background quasar. How can I create a reference frame centered on the quasar?"
         },
         {
            "from": "assistant",
            "value": "from astropy.coordinates import SkyCoord\nfrom astropy import units as u\nfrom astropy.time import Time\nimport numpy as np  # Added numpy import\n\ndef create_quasar_reference_frame(quasar_ra, quasar_dec, star_positions, observation_time):\n    \"\"\"\n    Creates a reference frame centered on a quasar and calculates relative stellar positions\n    \n    Parameters:\n    quasar_ra, quasar_dec: Quasar coordinates in degrees\n    star_positions: List of tuples containing (ra, dec) in degrees\n    observation_time: Time of observation\n    \"\"\"\n    \n    # Create SkyCoord object for the quasar (reference point)\n    quasar = SkyCoord(ra=quasar_ra*u.deg, dec=quasar_dec*u.deg, frame='icrs')\n    \n    # Convert stellar positions to SkyCoord objects\n    stars = [SkyCoord(ra=ra*u.deg, dec=dec*u.deg, frame='icrs') \n            for ra, dec in star_positions]\n    \n    # Calculate separations and position angles relative to quasar\n    relative_positions = []\n    for star in stars:\n        # Get angular separation\n        sep = quasar.separation(star)\n        # Get position angle (East of North)\n        pa = quasar.position_angle(star)\n        \n        relative_positions.append({\n            'separation': sep.arcsec,  # in arcseconds\n            'position_angle': pa.deg,  # in degrees\n            'dx': sep.arcsec * np.sin(pa.rad),  # x offset in arcseconds\n            'dy': sep.arcsec * np.cos(pa.rad)   # y offset in arcseconds\n        })\n    \n    return relative_positions\n\n# Example usage\nquasar_ra = 180.5  # degrees\nquasar_dec = -20.2 # degrees\nstar_positions = [(180.6, -20.3), (180.4, -20.1)]  # List of (ra, dec) tuples\nobs_time = Time('2023-01-01T00:00:00')\n\nrelative_pos = create_quasar_reference_frame(quasar_ra, quasar_dec, \n                                           star_positions, obs_time)"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm analyzing spectroscopic data from multiple epochs and need to ensure all observations are in the same reference frame. How can I extract the coordinate frame information from my WCS metadata?"
         },
         {
            "from": "assistant",
            "value": "from astropy.wcs import WCS\nfrom astropy.io import fits\nimport numpy as np\n\ndef analyze_coordinate_frame(filename):\n    try:\n        # Open the FITS file and get the WCS information\n        with fits.open(filename) as hdul:\n            wcs = WCS(hdul[0].header)\n            \n            # Extract coordinate frame information\n            frame = wcs.celestial.frame.name if wcs.celestial else \"Unknown\"\n            \n            # Get reference system details\n            ref_system = {\n                'frame': frame,\n                'equinox': wcs.wcs.equinox,\n                'radesys': wcs.wcs.radesys,\n                'specsys': wcs.wcs.specsys  # For spectroscopic data\n            }\n            \n            return ref_system\n            \n    except Exception as e:\n        print(f\"Error processing {filename}: {str(e)}\")\n        return None\n\n# Example usage for multiple files\ndef compare_frames(file_list):\n    frames = []\n    for file in file_list:\n        frame_info = analyze_coordinate_frame(file)\n        if frame_info:\n            frames.append(frame_info)\n            \n    # Check if all frames are consistent\n    consistent = all(f['frame'] == frames[0]['frame'] for f in frames)\n    return consistent, frames\n\n# Example file list\nfiles = ['spec1.fits', 'spec2.fits', 'spec3.fits']\nis_consistent, frame_info = compare_frames(files)"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I have a time series of flux measurements (10000 points) from a variable star containing some outliers due to cosmic rays. Show me how to apply sigma clipping with a 4-sigma threshold using the mean as the central tendency, and also return the upper and lower bounds of the clipping to understand the range of valid measurements."
         },
         {
            "from": "assistant",
            "value": "from astropy.stats import sigma_clip\nimport numpy as np\n\n# Simulate some flux measurements with outliers\nnp.random.seed(42)\nflux = np.random.normal(100, 10, 10000)  # Normal distribution of fluxes\nflux[np.random.randint(0, 10000, 50)] *= 5  # Add some cosmic ray hits\n\n# Apply sigma clipping\n# cenfunc='mean' uses the mean as central tendency\n# maxiters=None lets it iterate until convergence\nclipped_flux = sigma_clip(flux, sigma=4, maxiters=None, cenfunc='mean')\n\n# Get the clipping bounds\nlower_bound = clipped_flux.mean() - 4 * clipped_flux.std()\nupper_bound = clipped_flux.mean() + 4 * clipped_flux.std()\n\n# Print statistics\nprint(f\"Number of points clipped: {np.sum(clipped_flux.mask)}\")\nprint(f\"Valid flux range: {lower_bound:.2f} to {upper_bound:.2f}\")\nprint(f\"Percentage of data retained: {(1 - np.sum(clipped_flux.mask)/len(flux))*100:.1f}%\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "For my exoplanet transit observations, I want to compute the photometric precision across multiple epochs while being robust against occasional stellar flares. Can you show me how to calculate this using Astropy?"
         },
         {
            "from": "assistant",
            "value": "from astropy.stats import sigma_clip, mad_std\nimport numpy as np\nfrom astropy.time import Time\n\ndef calculate_photometric_precision(times, fluxes, errors=None):\n    \"\"\"\n    Calculate robust photometric precision from light curve data\n    while excluding stellar flares using sigma clipping\n    \n    Parameters:\n    times: Time array of observations\n    fluxes: Array of flux measurements\n    errors: Optional measurement uncertainties\n    \"\"\"\n    \n    # Convert times to astropy Time object if not already\n    if not isinstance(times, Time):\n        times = Time(times, format='jd')\n    \n    # Sigma clip to remove outliers (flares)\n    clipped_flux = sigma_clip(fluxes, sigma=3, maxiters=5)\n    \n    # Calculate robust standard deviation using median absolute deviation\n    precision = mad_std(clipped_flux.compressed())\n    \n    # If errors provided, calculate expected precision\n    if errors is not None:\n        expected_precision = np.median(errors)\n        \n        return {\n            'measured_precision': precision,\n            'expected_precision': expected_precision,\n            'excess_noise': np.sqrt(precision**2 - expected_precision**2)\n        }\n    \n    return {'measured_precision': precision}\n\n# Example usage:\ntimes = Time(['2458000.1', '2458000.2', '2458000.3'], format='jd')\nfluxes = np.array([1.001, 1.05, 0.999])  # Second point is a flare\nerrors = np.array([0.002, 0.002, 0.002])\n\nprecision_stats = calculate_photometric_precision(times, fluxes, errors)\nprint(f\"Measured precision: {precision_stats['measured_precision']:.6f}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I need to write a code snippet to calculate Saturn's position in the sky as seen from Mauna Kea Observatory (latitude: 19.8968\u00b0 N, longitude: 155.4678\u00b0 W, elevation: 4205 meters) on December 25, 2023 at 10:00 PM HST. How can I do this using astropy's get_body function?"
         },
         {
            "from": "assistant",
            "value": "from astropy.coordinates import EarthLocation, AltAz\nfrom astropy.time import Time\nimport astropy.units as u\nfrom astropy.coordinates import get_body\n\n# Define Mauna Kea Observatory location\nmauna_kea = EarthLocation(\n    lat=19.8968*u.deg,\n    lon=-155.4678*u.deg,\n    height=4205*u.m\n)\n\n# Set the observation time (HST)\n# Note: Adding 10 hours to convert from HST to UTC\nobs_time = Time('2023-12-25 10:00:00') + 10*u.hour\n\n# Get Saturn's position\nsaturn = get_body('saturn', obs_time, mauna_kea)\n\n# Transform to local altitude/azimuth coordinates\naltaz_frame = AltAz(obstime=obs_time, location=mauna_kea)\nsaturn_altaz = saturn.transform_to(altaz_frame)\n\n# Print results\nprint(f\"Saturn's position from Mauna Kea Observatory:\")\nprint(f\"Altitude: {saturn_altaz.alt:.2f}\")\nprint(f\"Azimuth: {saturn_altaz.az:.2f}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "How can I convert proper distance to comoving distance for a set of galaxies at redshifts z=[0.5, 1.0, 1.5, 2.0] using Planck18 parameters? Please provide a code example that returns the results in megaparsecs."
         },
         {
            "from": "assistant",
            "value": "from astropy.cosmology import Planck18\nimport numpy as np\n\n# Define redshifts for galaxies\nz = np.array([0.5, 1.0, 1.5, 2.0])\n\n# Calculate proper distance (in Mpc)\n# Proper distance is the comoving transverse distance divided by (1+z)\ncomoving_distance = Planck18.comoving_distance(z).to('Mpc')\nproper_distance = comoving_distance / (1 + z)\n\n# Print results in a formatted way\nfor i, redshift in enumerate(z):\n    print(f\"z={redshift:.1f}:\")\n    print(f\"  Proper Distance: {proper_distance[i]:.1f}\")\n    print(f\"  Comoving Distance: {comoving_distance[i]:.1f}\")\n\n# Note: Comoving distance = Proper distance * (1 + z)\n# Verification of the relationship\nassert np.allclose(comoving_distance, proper_distance * (1 + z))"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "How do I create a properly scaled visualization of my telescope's CCD image of a star cluster, ensuring both bright stars and faint background objects are visible?"
         },
         {
            "from": "assistant",
            "value": "from astropy.io import fits\nfrom astropy.visualization import ZScaleInterval, ImageNormalize, LogStretch\nimport matplotlib.pyplot as plt\nimport os\n\ndef display_fits_image(fits_path):\n    try:\n        # Check if file exists\n        if not os.path.exists(fits_path):\n            raise FileNotFoundError(f\"The file {fits_path} does not exist.\")\n\n        # Read the FITS image\n        image_data = fits.getdata(fits_path)\n\n        # Create normalization using ZScale and LogStretch\n        # ZScale automatically determines good contrast limits\n        # LogStretch helps reveal both bright and faint features\n        znorm = ImageNormalize(\n            image_data,\n            interval=ZScaleInterval(),\n            stretch=LogStretch()\n        )\n\n        # Create the visualization\n        plt.figure(figsize=(10, 10))\n        plt.imshow(image_data, norm=znorm, cmap='gray')\n        plt.colorbar(label='Flux (log scale)')\n\n        # Add basic information to the plot\n        plt.title(f'Star Cluster Observation\\nFile: {os.path.basename(fits_path)}')\n        plt.xlabel('Pixels')\n        plt.ylabel('Pixels')\n\n        # Display the image\n        plt.show()\n\n    except FileNotFoundError as e:\n        print(f\"Error: {e}\")\n        print(\"Please check if the file path is correct and the file exists.\")\n    except Exception as e:\n        print(f\"An unexpected error occurred: {e}\")\n\n# Usage example:\n# Replace with your actual FITS file path\nfits_file_path = 'path/to/your/star_cluster.fits'\ndisplay_fits_image(fits_file_path)"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I have multiple sets of pixel coordinates and their corresponding sky positions from my telescope observations. How can I create a WCS transformation that maps between them, accounting for the telescope's geometric distortion?"
         },
         {
            "from": "assistant",
            "value": "from astropy.wcs import WCS, Sip\nfrom astropy.modeling.models import Polynomial2D\nimport numpy as np\n\n# Example data - replace with your actual coordinates\npixel_coords = np.array([[100, 100], [200, 200], [300, 300], [400, 400]])  # (x,y)\nsky_coords = np.array([[45.2, -30.1], [45.3, -30.2], \n                      [45.4, -30.3], [45.5, -30.4]])    # (RA,Dec)\n\ndef create_distortion_wcs(pixel_coords, sky_coords, degree=3):\n    \"\"\"\n    Create WCS transformation with SIP distortion correction\n    \n    Parameters:\n    pixel_coords: array of (x,y) pixel coordinates\n    sky_coords: array of (RA,Dec) coordinates in degrees\n    degree: polynomial degree for distortion model (default=3)\n    \"\"\"\n    # Initialize basic WCS\n    w = WCS(naxis=2)\n    \n    # Fit basic WCS parameters (rough linear transformation)\n    x0, y0 = np.mean(pixel_coords, axis=0)\n    ra0, dec0 = np.mean(sky_coords, axis=0)\n    \n    # Set basic WCS parameters\n    w.wcs.crpix = [x0, y0]     # Reference pixel\n    w.wcs.crval = [ra0, dec0]  # Reference sky position\n    w.wcs.ctype = ['RA---TAN-SIP', 'DEC--TAN-SIP']\n    \n    # Set a default CD matrix (scale matrix)\n    w.wcs.cd = np.array([[0.001, 0.0], [0.0, 0.001]])\n    \n    # Create SIP distortion model\n    # Initialize empty arrays for SIP coefficients\n    a_order = b_order = ap_order = bp_order = degree\n    a = np.zeros((a_order + 1, a_order + 1), np.double)\n    b = np.zeros((b_order + 1, b_order + 1), np.double)\n    ap = np.zeros((ap_order + 1, ap_order + 1), np.double)\n    bp = np.zeros((bp_order + 1, bp_order + 1), np.double)\n    \n    # Create Sip object with proper initialization\n    sip = Sip(a, b, ap, bp, w.wcs.crpix)\n    \n    try:\n        # Fit SIP coefficients\n        sip.fit(pixel_coords, sky_coords, w)\n        \n        # Add SIP coefficients to WCS\n        w.sip = sip\n    except Exception as e:\n        print(f\"Warning: SIP fit failed: {str(e)}\")\n        # Continue without distortion correction\n    \n    return w\n\n# Create WCS with distortion correction\nwcs = create_distortion_wcs(pixel_coords, sky_coords)\n\n# Example usage: Convert pixel to sky coordinates\ntest_pixels = np.array([[150, 150]])\nsky = wcs.all_pix2world(test_pixels, 0)\nprint(f\"Sky coordinates (RA, Dec): {sky}\")\n\n# Convert sky to pixel coordinates\npixels = wcs.all_world2pix(sky, 0)\nprint(f\"Pixel coordinates (x, y): {pixels}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "For my exoplanet transit data, I want to add my telescope's wavelength calibration information to the header of a newly created primary HDU. Can you show me how to set this up?"
         },
         {
            "from": "assistant",
            "value": "from astropy.io import fits\nimport numpy as np\n\n# Create a new primary HDU\nprimary_hdu = fits.PrimaryHDU()\n\n# Add wavelength calibration information to header\n# Assuming a linear wavelength solution for this example\nprimary_hdu.header['CTYPE1'] = 'WAVELENGTH'  # Axis type\nprimary_hdu.header['CUNIT1'] = 'Angstrom'    # Wavelength units\nprimary_hdu.header['CRPIX1'] = 1             # Reference pixel\nprimary_hdu.header['CRVAL1'] = 4000.0        # Starting wavelength\nprimary_hdu.header['CDELT1'] = 0.1           # Wavelength increment per pixel\n\n# Add some metadata about the calibration\nprimary_hdu.header['WCSCAL'] = 'SUCCESS'     # Calibration status\nprimary_hdu.header['CALDATE'] = '2024-01-15' # Calibration date\nprimary_hdu.header['COMMENT'] = 'Wavelength calibration based on HeNeAr lamp'\n\n# Create HDUList and write to file\nhdul = fits.HDUList([primary_hdu])\nhdul.writeto('calibrated_transit.fits', overwrite=True)"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm analyzing pulsar timing data and need to assess the uncertainty in my dispersion measure calculations. Could you demonstrate how to implement jackknife resampling for this analysis?"
         },
         {
            "from": "assistant",
            "value": "from astropy import units as u\nimport numpy as np\nfrom astropy.time import Time\n\ndef jackknife_dm_uncertainty(arrival_times, frequencies, dm_values, n_samples=20):\n    \"\"\"\n    Performs jackknife resampling to estimate uncertainty in dispersion measure\n    \n    Parameters:\n    arrival_times: Time array of pulse arrival times\n    frequencies: Array of observing frequencies in MHz\n    dm_values: Array of DM measurements\n    n_samples: Number of jackknife samples\n    \"\"\"\n    \n    # Convert frequencies to MHz if they have units\n    if hasattr(frequencies, 'unit'):\n        frequencies = frequencies.to(u.MHz).value\n        \n    # Get the DM unit to use consistently\n    dm_unit = dm_values.unit\n    # Convert DM values to plain array while preserving units for later\n    dm_values_array = dm_values.to(dm_unit).value\n    \n    n_points = len(dm_values)\n    jackknife_estimates = np.zeros(n_samples)\n    \n    # Perform jackknife resampling\n    for i in range(n_samples):\n        # Randomly exclude ~37% of points\n        mask = np.random.rand(n_points) > 0.37\n        \n        # Calculate DM for this subsample\n        dm_subsample = dm_values_array[mask]\n        jackknife_estimates[i] = np.mean(dm_subsample)\n    \n    # Calculate jackknife statistics\n    dm_mean = np.mean(jackknife_estimates)\n    dm_uncertainty = np.std(jackknife_estimates) * np.sqrt(n_samples - 1)\n    \n    # Return results with proper units\n    return dm_mean * dm_unit, dm_uncertainty * dm_unit\n\n# Example usage\ntimes = Time(['2023-01-01', '2023-01-02', '2023-01-03'])\nfreqs = [1400, 800, 400] * u.MHz\ndms = [56.78, 56.82, 56.75] * u.pc / u.cm**3\n\nmean_dm, uncertainty = jackknife_dm_uncertainty(times, freqs, dms)\nprint(f\"DM = {mean_dm.value:.2f} \u00b1 {uncertainty.value:.2f} {mean_dm.unit}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I have brightness measurements of galaxies in two different filters (V and B magnitudes):\nV = [18.2, 18.5, 18.3, 18.4, 18.6]\nB = [19.4, 19.6, 19.3, 19.5, 19.7]\nHow can I use biweight_scale to compute the scatter in both filters simultaneously using the axis parameter?"
         },
         {
            "from": "assistant",
            "value": "import numpy as np\nfrom astropy.stats import biweight_scale\n\n# Create array of magnitudes\n# Stack V and B magnitudes into a 2D array where each row represents a filter\nmagnitudes = np.array([\n    V := [18.2, 18.5, 18.3, 18.4, 18.6],  # V filter\n    B := [19.4, 19.6, 19.3, 19.5, 19.7]   # B filter\n])\n\n# Calculate biweight scale along axis=1 (across each filter's measurements)\n# axis=1 means compute along each row\nscatter = biweight_scale(magnitudes, axis=1)\n\n# Print results\nprint(f\"Scatter in V filter: {scatter[0]:.3f} mag\")\nprint(f\"Scatter in B filter: {scatter[1]:.3f} mag\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I have flux measurements from 100 stars in my field, with values ranging from 0 to 1000 counts. How can I use median_absolute_deviation to identify potential outliers or variable stars in my dataset?"
         },
         {
            "from": "assistant",
            "value": "from astropy.stats import median_absolute_deviation\nimport numpy as np\n\n# Sample flux data (simulated for 100 stars)\nflux_data = np.random.uniform(0, 1000, 100)\n\n# Calculate the median and MAD of the flux values\nmedian_flux = np.median(flux_data)\nmad = median_absolute_deviation(flux_data)\n\n# Define outlier threshold (typically 3-5 sigma)\nsigma_threshold = 3\n\n# Identify outliers: points that deviate more than threshold*MAD from median\noutlier_mask = np.abs(flux_data - median_flux) > (sigma_threshold * mad)\noutlier_indices = np.where(outlier_mask)[0]\noutlier_fluxes = flux_data[outlier_mask]\n\n# Print results\nprint(f\"Number of outliers found: {len(outlier_indices)}\")\nprint(f\"Indices of outlier stars: {outlier_indices}\")\nprint(f\"Flux values of outliers: {outlier_fluxes}\")\nprint(f\"Median flux: {median_flux:.2f}\")\nprint(f\"MAD: {mad:.2f}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm modeling the gravitational potential in a galaxy cluster and need a 2D polynomial (degree=3) where certain coefficients must remain fixed during fitting. Show me how to create a Polynomial2D model where c20 (x^2 coefficient) is fixed at 0.5 and c02 (y^2 coefficient) is fixed at 0.3."
         },
         {
            "from": "assistant",
            "value": "from astropy.modeling import models, fitting\n\n# Create a 2D polynomial model of degree 3\npoly_model = models.Polynomial2D(degree=3)\n\n# Fix specific coefficients:\n# c20 (x^2 coefficient) = 0.5\n# c02 (y^2 coefficient) = 0.3\npoly_model.c2_0.value = 0.5  # Set x^2 coefficient\npoly_model.c0_2.value = 0.3  # Set y^2 coefficient\npoly_model.c2_0.fixed = True  # Fix x^2 coefficient\npoly_model.c0_2.fixed = True  # Fix y^2 coefficient\n\n# Example of how to use the model (if you have data):\n# fitter = fitting.LevMarLSQFitter()\n# fitted_model = fitter(poly_model, x_data, y_data, z_data)"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm working on deconvolving astronomical images and need to compute the convolution of my image with a PSF kernel, but I want to optimize performance using multiple CPU cores. How can I set this up using astropy's FFT convolution?"
         },
         {
            "from": "assistant",
            "value": "from astropy.convolution import convolve_fft\nfrom astropy.convolution import Gaussian2DKernel\nimport numpy as np\nfrom multiprocessing import cpu_count\n\n# Set number of threads for FFT operations (using available CPU cores)\nimport os\nos.environ['OMP_NUM_THREADS'] = str(cpu_count())\n\n# Create sample image and PSF kernel (replace with your actual data)\nimage = np.random.normal(0, 1, (1024, 1024))\npsf_kernel = Gaussian2DKernel(x_stddev=3)\n\n# Perform FFT convolution with optimization flags\n# normalize=False for better performance if normalization isn't needed\n# allow_huge=True allows larger memory usage for speed\n# preserve_nan=False improves performance if NaN handling isn't required\nconvolved_image = convolve_fft(\n    image,\n    psf_kernel,\n    normalize_kernel=False,\n    allow_huge=True,\n    preserve_nan=False,\n    boundary='wrap'  # fastest boundary handling method\n)"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "In my galaxy spectra analysis, I need to fit the background continuum. Can you demonstrate how to create a constrained 2nd-degree Polynomial1D model where the linear term is tied to be twice the constant term, using these data points: wavelength=[4000, 4500, 5000, 5500, 6000] and flux=[2.1, 2.3, 2.4, 2.6, 2.7]?"
         },
         {
            "from": "assistant",
            "value": "from astropy.modeling import models, fitting\nimport numpy as np\n\n# Define data points\nwavelength = np.array([4000, 4500, 5000, 5500, 6000])\nflux = np.array([2.1, 2.3, 2.4, 2.6, 2.7])\n\n# Create a 2nd degree polynomial model\npoly_model = models.Polynomial1D(degree=2)\n\n# Define the tie function: c1 = 2*c0\n# where c0 is the constant term and c1 is the linear term\ndef tie_linear_term(model):\n    return 2 * model.c0\n\n# Set up the tie\npoly_model.c1.tied = tie_linear_term\n\n# Fit the model\nfitter = fitting.LevMarLSQFitter()\nfitted_model = fitter(poly_model, wavelength, flux)\n\n# Print the results\nprint(f\"Constant term (c0): {fitted_model.c0.value:.3f}\")\nprint(f\"Linear term (c1): {fitted_model.c1.value:.3f}\")\nprint(f\"Quadratic term (c2): {fitted_model.c2.value:.3f}\")\n\n# Generate fitted values for plotting\nfitted_flux = fitted_model(wavelength)"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm studying galaxy brightness distributions in a cluster with measurements [18.5, 18.7, 18.6, 18.9, 16.2, 18.8, 18.7, 18.5] magnitudes. Write code to use median_absolute_deviation to identify potential foreground contamination."
         },
         {
            "from": "assistant",
            "value": "from astropy.stats import median_absolute_deviation\nimport numpy as np\n\n# Galaxy magnitudes data\nmagnitudes = np.array([18.5, 18.7, 18.6, 18.9, 16.2, 18.8, 18.7, 18.5])\n\n# Calculate median and MAD\nmedian_mag = np.median(magnitudes)\nmad = median_absolute_deviation(magnitudes)\n\n# Define outlier threshold (typically 3-sigma)\nsigma_threshold = 3.0\noutlier_threshold = sigma_threshold * mad\n\n# Find potential foreground galaxies (significantly brighter than cluster)\nforeground_mask = np.abs(magnitudes - median_mag) > outlier_threshold\nforeground_candidates = magnitudes[foreground_mask]\n\nprint(f\"Median magnitude: {median_mag:.2f}\")\nprint(f\"MAD: {mad:.2f}\")\nprint(\"Potential foreground galaxies:\", foreground_candidates)"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "For my research on planetary alignments, I need to find the positions of all major planets during the next grand conjunction. Can you help with the code?"
         },
         {
            "from": "assistant",
            "value": "from astropy.time import Time\nfrom astropy.coordinates import get_body, solar_system_ephemeris\nimport numpy as np\nfrom astropy.coordinates import SkyCoord\nfrom astropy import units as u\n\ndef find_planetary_alignment():\n    # Set ephemeris for higher accuracy\n    solar_system_ephemeris.set('de432s')\n    \n    # Define time range (2024-2025 for next notable alignment)\n    times = Time('2024-01-01') + np.linspace(0, 365, 365) * u.day\n    \n    # List of planets to track\n    planets = ['mercury', 'venus', 'mars', 'jupiter', 'saturn']\n    \n    # Find the date with minimum angular separation\n    min_spread = float('inf')\n    best_time = None\n    \n    for time in times:\n        positions = []\n        for planet in planets:\n            # Get planet positions in GCRS frame\n            pos = get_body(planet, time)\n            positions.append(pos)\n        \n        # Convert to SkyCoord for easier angle calculations\n        coords = SkyCoord(positions)\n        \n        # Calculate max angular separation between any two planets\n        max_sep = 0\n        for i in range(len(coords)):\n            for j in range(i + 1, len(coords)):\n                sep = coords[i].separation(coords[j])\n                max_sep = max(max_sep, sep.deg)\n        \n        if max_sep < min_spread:\n            min_spread = max_sep\n            best_time = time\n    \n    # Print results for the best alignment\n    print(f\"Best alignment date: {best_time.iso}\")\n    print(f\"Maximum separation: {min_spread:.2f} degrees\")\n    \n    # Print positions of planets at best alignment\n    for planet in planets:\n        pos = get_body(planet, best_time)\n        print(f\"{planet.capitalize()}: RA={pos.ra.deg:.2f}\u00b0, Dec={pos.dec.deg:.2f}\u00b0\")\n\n# Run the analysis\nfind_planetary_alignment()"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm studying the radial velocities of stars in a globular cluster, and the distribution seems quite skewed. Can you show me how to create a well-binned histogram that accounts for this non-normal distribution in the velocity data?"
         },
         {
            "from": "assistant",
            "value": "import numpy as np\nfrom astropy.stats import scott_bin_width\nimport matplotlib.pyplot as plt\n\n# Generate sample radial velocity data (replace with your actual data)\n# Using a skewed distribution for demonstration\nnp.random.seed(42)\nn_stars = 1000\nradial_velocities = np.random.gamma(4, 10, n_stars) - 20  # km/s\n\n# Calculate optimal bin width using Scott's rule\n# Scott's rule works well for non-normal distributions\nbin_width = scott_bin_width(radial_velocities)\nn_bins = int(np.ceil((radial_velocities.max() - radial_velocities.min()) / bin_width))\n\n# Create and plot the histogram\nplt.figure(figsize=(10, 6))\nplt.hist(radial_velocities, bins=n_bins, density=True, alpha=0.7, \n         color='navy', edgecolor='white')\nplt.xlabel('Radial Velocity (km/s)')\nplt.ylabel('Normalized Frequency')\nplt.title('Distribution of Stellar Radial Velocities')\n\n# Add grid for better readability\nplt.grid(True, alpha=0.3)\nplt.show()\n\n# Print basic statistics\nprint(f\"Number of bins used: {n_bins}\")\nprint(f\"Bin width: {bin_width:.2f} km/s\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "For my variable star photometry project, I need to get the filter type used in each observation from the FITS headers. Can you show me how to retrieve this information?"
         },
         {
            "from": "assistant",
            "value": "from astropy.io import fits\n\ndef get_filter_info(fits_file):\n    \"\"\"\n    Extract filter information from FITS header.\n    Checks common FITS keywords for filter information.\n    \"\"\"\n    try:\n        # Open the FITS file\n        with fits.open(fits_file) as hdul:\n            header = hdul[0].header  # Get primary header\n            \n            # Check common filter keywords (different telescopes use different keywords)\n            filter_keywords = ['FILTER', 'FILTERS', 'FILT', 'FILTNAME']\n            \n            for keyword in filter_keywords:\n                if keyword in header:\n                    return header[keyword]\n            \n            # If no filter keyword found\n            return \"Filter information not found in header\"\n            \n    except Exception as e:\n        return f\"Error reading FITS file: {str(e)}\"\n\n# Example usage\nfits_file = 'variable_star_image.fits'\nfilter_type = get_filter_info(fits_file)\nprint(f\"Filter used: {filter_type}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I have photometric measurements of 1000 stars in a cluster, and some are affected by bad pixels. How can I apply sigma clipping with a 3.5-sigma threshold, using the median absolute deviation (mad_std) as the measure of dispersion instead of standard deviation? I want the output as an unmasked array with the outliers removed."
         },
         {
            "from": "assistant",
            "value": "import numpy as np\nfrom astropy.stats import sigma_clip\n\n# Simulate some sample data (replace this with your actual data)\nnp.random.seed(42)\nmeasurements = np.random.normal(100, 10, 1000)  # Normal distribution\n# Add some outliers to simulate bad pixels\nmeasurements[::50] = 200  # Every 50th value is an outlier\n\n# Perform sigma clipping\n# Using mad_std=True for median absolute deviation\n# masked=False returns an array with outliers removed\ncleaned_data = sigma_clip(\n    measurements,\n    sigma=3.5,\n    maxiters=5,\n    stdfunc='mad_std',\n    masked=False,\n    copy=True\n)\n\n# Print results\nprint(f\"Original data points: {len(measurements)}\")\nprint(f\"Points after clipping: {len(cleaned_data)}\")\nprint(f\"Number of outliers removed: {len(measurements) - len(cleaned_data)}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "\"For my exoplanet transit modeling, I need to fit a sine function to my periodic signal. Show me how to create a Sine1D model with initial parameters (amplitude=0.02, frequency=0.5, phase=0) and set bounds where amplitude must be between 0.01 and 0.03, frequency between 0.4 and 0.6, while keeping phase fixed during the fit.\""
         },
         {
            "from": "assistant",
            "value": "from astropy.modeling import models, fitting\nimport numpy as np\n\n# Create a Sine1D model with initial parameters\nsine_model = models.Sine1D(\n    amplitude=0.02,    # Initial amplitude\n    frequency=0.5,     # Initial frequency\n    phase=0           # Initial phase\n)\n\n# Set parameter bounds and fixed constraints\nsine_model.amplitude.bounds = (0.01, 0.03)    # Amplitude bounds\nsine_model.frequency.bounds = (0.4, 0.6)      # Frequency bounds\nsine_model.phase.fixed = True                 # Fix phase parameter\n\n# Create a fitter object (using LevMarLSQFitter for non-linear least squares fitting)\nfitter = fitting.LevMarLSQFitter()\n\n# Example: Generate sample data for demonstration\nx = np.linspace(0, 10, 100)\ntrue_sine = 0.02 * np.sin(2 * np.pi * 0.5 * x)\nnoise = np.random.normal(0, 0.001, len(x))\ny_data = true_sine + noise\n\n# Fit the model to the data\nfitted_model = fitter(sine_model, x, y_data)\n\n# Print fitted parameters\nprint(f\"Fitted amplitude: {fitted_model.amplitude.value:.4f}\")\nprint(f\"Fitted frequency: {fitted_model.frequency.value:.4f}\")\nprint(f\"Fixed phase: {fitted_model.phase.value:.4f}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "For my variable star photometry project, I need to convert a list of pixel coordinates of reference stars to their celestial coordinates. How can I do this transformation in batch using Astropy?"
         },
         {
            "from": "assistant",
            "value": "from astropy.wcs import WCS\nfrom astropy.io import fits\nimport numpy as np\n\ndef pixel_to_celestial(filename, pixel_coords):\n    \"\"\"\n    Convert pixel coordinates to celestial coordinates (RA, Dec) using WCS\n    \n    Parameters:\n    filename: str - Path to FITS file with WCS information\n    pixel_coords: array-like - List of (x,y) pixel coordinates\n    \n    Returns:\n    tuple - Arrays of (RA, Dec) in degrees\n    \"\"\"\n    try:\n        # Load WCS information from FITS header\n        with fits.open(filename) as hdul:\n            wcs = WCS(hdul[0].header)\n        \n        # Convert pixel coordinates array to numpy array\n        pixels = np.array(pixel_coordinates)\n        \n        # Transform coordinates (returns RA, Dec in degrees)\n        ra, dec = wcs.all_pix2world(pixels[:,0], pixels[:,1], 0)\n        \n        return ra, dec\n        \n    except Exception as e:\n        print(f\"Error in coordinate transformation: {e}\")\n        return None, None\n\n# Example usage\npixel_coordinates = [\n    [100, 100],   # x,y coordinates of first star\n    [200, 150],   # second star\n    [300, 250]    # third star\n]\n\n# Assuming you have a FITS file with WCS information\nfits_file = \"your_image.fits\"\nra, dec = pixel_to_celestial(fits_file, pixel_coordinates)\n\nif ra is not None:\n    for i, (r, d) in enumerate(zip(ra, dec)):\n        print(f\"Star {i+1}: RA = {r:.6f}\u00b0, Dec = {d:.6f}\u00b0\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm analyzing the motion of a satellite galaxy relative to M31 (Andromeda). The center of M31 is at (RA=10.68458\u00b0, Dec=41.26917\u00b0) and the satellite is at (RA=10.75\u00b0, Dec=41.3\u00b0). Show me how to calculate their position angle using astropy coordinates."
         },
         {
            "from": "assistant",
            "value": "from astropy.coordinates import SkyCoord\nimport astropy.units as u\n\n# Create SkyCoord objects for M31 and its satellite\nm31_center = SkyCoord(ra=10.68458*u.deg, dec=41.26917*u.deg)\nsatellite = SkyCoord(ra=10.75*u.deg, dec=41.3*u.deg)\n\n# Calculate position angle (East of North)\n# position_angle() returns angle in radians, convert to degrees\npos_angle = m31_center.position_angle(satellite).to(u.deg)\n\nprint(f\"Position angle between M31 and satellite: {pos_angle:.2f}\")\n\n# Calculate separation (optional but useful)\nseparation = m31_center.separation(satellite).to(u.arcmin)\nprint(f\"Angular separation: {separation:.2f}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "For my exoplanet research paper, I need to express distances in both metric and imperial units. Can you show me how to make all these units available so I can convert between parsecs, light-years, and miles?"
         },
         {
            "from": "assistant",
            "value": "from astropy import units as u\nfrom astropy.units import imperial\n\n# Define some example distances\ndistance_pc = 50 * u.pc  # 50 parsecs\ndistance_ly = 100 * u.lyr  # 100 light years\n\n# Convert between different units\n# Parsecs to light years and miles\npc_to_ly = distance_pc.to(u.lyr)\npc_to_miles = distance_pc.to(imperial.mile)\n\n# Light years to parsecs and miles\nly_to_pc = distance_ly.to(u.pc)\nly_to_miles = distance_ly.to(imperial.mile)\n\n# Print results\nprint(f\"50 parsecs = {pc_to_ly:.2f}\")\nprint(f\"50 parsecs = {pc_to_miles:.2e} miles\")\nprint(f\"100 light years = {ly_to_pc:.2f}\")\nprint(f\"100 light years = {ly_to_miles:.2e} miles\")\n\n# You can also create a custom conversion function\ndef convert_distance(value, from_unit, to_unit):\n    \"\"\"Convert astronomical distances between any units.\"\"\"\n    original = value * u.Unit(from_unit)\n    return original.to(u.Unit(to_unit))\n\n# Example usage\ncustom_distance = convert_distance(75, 'pc', 'lyr')\nprint(f\"\\nCustom conversion: 75 parsecs = {custom_distance:.2f}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I want to create a plot comparing the radii of exoplanets (1.2, 1.5, 2.1 Earth radii) with the radius of Jupiter (11.2 Earth radii). How can I make a scatter plot using astropy units and proper axis labels?"
         },
         {
            "from": "assistant",
            "value": "import numpy as np\nfrom astropy import units as u\nimport matplotlib.pyplot as plt\n\n# Create arrays with planet radii data\nexoplanet_radii = np.array([1.2, 1.5, 2.1]) * u.earthRad\njupiter_radius = 11.2 * u.earthRad\n\n# Create indices for x-axis (just for spacing the points)\nx_positions = np.arange(len(exoplanet_radii))\n\n# Create the scatter plot\nplt.figure(figsize=(8, 6))\n# Plot exoplanets\nplt.scatter(x_positions, exoplanet_radii.value, label='Exoplanets', color='blue')\n# Plot Jupiter as a horizontal line\nplt.axhline(y=jupiter_radius.value, color='r', linestyle='--', label='Jupiter')\n\n# Customize the plot\nplt.ylabel('Radius [Earth Radii]')\nplt.xlabel('Planet Number')\nplt.title('Comparison of Exoplanet Radii with Jupiter')\nplt.legend()\nplt.grid(True, linestyle=':')\n\n# Show the plot\nplt.show()"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "For my pulsar timing analysis, I have multiple radio observations with signal strength measurements. How can I combine these observations to show the pulse profile variations across different observing sessions?"
         },
         {
            "from": "assistant",
            "value": "from astropy.time import Time\nfrom astropy.table import Table, vstack\nfrom astropy.visualization import quantity_support\nimport astropy.units as u\nimport numpy as np\n\n# Create sample pulse profile data (replace with your actual data)\ndef create_sample_observation(n_bins=128, obs_time=None):\n    # Simulate a pulse profile with Gaussian noise\n    phase = np.linspace(0, 1, n_bins)\n    profile = np.exp(-(phase - 0.5)**2 / 0.01) + np.random.normal(0, 0.1, n_bins)\n    \n    # Create table for single observation\n    return Table({\n        'phase': phase,\n        'intensity': profile,\n        'obs_time': [obs_time] * n_bins\n    })\n\n# Generate multiple observations\nobservation_times = Time(['2023-01-01', '2023-02-01', '2023-03-01'])\nobservations = []\n\nfor obs_time in observation_times:\n    obs = create_sample_observation(obs_time=obs_time)\n    observations.append(obs)\n\n# Combine all observations\ncombined_profiles = vstack(observations)\n\n# Calculate average profile and standard deviation\nphase_bins = np.linspace(0, 1, 128)\navg_profile = np.zeros(len(phase_bins))\nstd_profile = np.zeros(len(phase_bins))\n\nfor i, phase in enumerate(phase_bins):\n    mask = (combined_profiles['phase'] == phase)\n    avg_profile[i] = np.mean(combined_profiles['intensity'][mask])\n    std_profile[i] = np.std(combined_profiles['intensity'][mask])\n\n# Create output table with results\nresult_table = Table({\n    'phase': phase_bins,\n    'avg_intensity': avg_profile * u.Jy,  # Assuming intensity is in Janskys\n    'std_intensity': std_profile * u.Jy\n})\n\n# Print first few rows of results\nprint(\"Combined Pulse Profile Statistics:\")\nprint(result_table[:5])"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I need to model the intensity distribution of a star cluster that appears elongated in my observation data. How can I create a 2D Gaussian model to fit this asymmetric distribution?"
         },
         {
            "from": "assistant",
            "value": "import numpy as np\nfrom astropy.modeling import models, fitting\nfrom astropy.io import fits\nimport matplotlib.pyplot as plt\n\n# Create sample data grid (in real usage, this would be your observation data)\ny, x = np.mgrid[-50:51, -50:51]\ndata = np.zeros((101, 101))\n\n# Create an elongated 2D Gaussian model\ngaussian_2d = models.Gaussian2D(\n    amplitude=100,    # Peak intensity\n    x_mean=0,        # Center x position\n    y_mean=0,        # Center y position\n    x_stddev=20,     # Standard deviation in x\n    y_stddev=10,     # Different stddev for y creates elongation\n    theta=np.pi/4    # Rotation angle (45 degrees)\n)\n\n# Generate the model data\ndata = gaussian_2d(x, y)\n\n# Add some noise to simulate real observations\nnp.random.seed(42)\ndata += np.random.normal(0, 2, data.shape)\n\n# Create a new model instance for fitting\ng2d_init = models.Gaussian2D(\n    amplitude=90,    # Initial guess\n    x_mean=0,\n    y_mean=0,\n    x_stddev=15,\n    y_stddev=15,\n    theta=0\n)\n\n# Create fitter and perform the fit\nfitter = fitting.LevMarLSQFitter()\nfitted_model = fitter(g2d_init, x, y, data)\n\n# Print fitted parameters\nprint(f\"Amplitude: {fitted_model.amplitude.value:.2f}\")\nprint(f\"Center: ({fitted_model.x_mean.value:.2f}, {fitted_model.y_mean.value:.2f})\")\nprint(f\"X stddev: {fitted_model.x_stddev.value:.2f}\")\nprint(f\"Y stddev: {fitted_model.y_stddev.value:.2f}\")\nprint(f\"Rotation: {np.rad2deg(fitted_model.theta.value):.2f} degrees\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm working on multi-wavelength observations of a star cluster and need to ensure consistent spatial resolution across different filters. Can you show me how to extract the pixel scales from each image's WCS?"
         },
         {
            "from": "assistant",
            "value": "from astropy.io import fits\nfrom astropy.wcs import WCS\nimport numpy as np\n\ndef get_pixel_scale(fits_file):\n    \"\"\"\n    Extract pixel scale from a FITS image using WCS.\n    Returns pixel scales in arcsec/pixel for both axes.\n    \"\"\"\n    try:\n        # Open FITS file and get WCS\n        with fits.open(fits_file) as hdul:\n            wcs = WCS(hdul[0].header)\n        \n        # Get pixel scales using WCS transformation\n        # Convert from degrees to arcseconds\n        pixel_scales = np.abs(wcs.pixel_scale_matrix) * 3600.0\n        \n        return {\n            'x_scale': pixel_scales[0,0],\n            'y_scale': pixel_scales[1,1],\n            'units': 'arcsec/pixel'\n        }\n    \n    except Exception as e:\n        print(f\"Error processing {fits_file}: {str(e)}\")\n        return None\n\n# Example usage for multiple filters\nfilter_files = {\n    'U': 'u_band.fits',\n    'B': 'b_band.fits',\n    'V': 'v_band.fits'\n}\n\n# Extract scales for each filter\nscales = {}\nfor filter_name, file_path in filter_files.items():\n    scale = get_pixel_scale(file_path)\n    if scale:\n        scales[filter_name] = scale\n        print(f\"Filter {filter_name}: {scale['x_scale']:.3f} x {scale['y_scale']:.3f} {scale['units']}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm analyzing the light profile of an elliptical galaxy and want to create a Sersic1D model with n=6, amplitude=3.0, and r_eff=15 kpc. How do I calculate the surface brightness values at specific radii: 5, 10, 20, and 30 kpc?"
         },
         {
            "from": "assistant",
            "value": "from astropy.modeling.models import Sersic1D\nimport numpy as np\nfrom astropy import units as u\n\n# Create a Sersic1D model with specified parameters\n# n=6 (Sersic index), amplitude=3.0, r_eff=15 kpc\nsersic_model = Sersic1D(amplitude=3.0, r_eff=15, n=6)\n\n# Define radii where we want to calculate surface brightness\nradii = np.array([5, 10, 20, 30])  # kpc\n\n# Calculate surface brightness at specified radii\nsurface_brightness = sersic_model(radii)\n\n# Print results\nfor r, sb in zip(radii, surface_brightness):\n    print(f\"Radius: {r:2d} kpc -> Surface Brightness: {sb:.4f}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm studying stellar absorption features and need to decompose overlapping spectral lines. How can I create a model with multiple Gaussian components to represent these features?"
         },
         {
            "from": "assistant",
            "value": "import numpy as np\nfrom astropy.modeling import models, fitting\nfrom astropy import units as u\n\n# Create synthetic spectral data with overlapping lines\nwavelength = np.linspace(6500, 6600, 1000) * u.angstrom\ncontinuum = 1.0\n\n# Define two overlapping Gaussian absorption features\ng1 = models.Gaussian1D(amplitude=-0.5, mean=6550, stddev=2)\ng2 = models.Gaussian1D(amplitude=-0.3, mean=6553, stddev=3)\n\n# Combine the features and add noise\nspectrum = continuum + g1(wavelength.value) + g2(wavelength.value)\nnoise = np.random.normal(0, 0.02, wavelength.shape)\nspectrum += noise\n\n# Create initial composite model for fitting\ninit_model = models.Gaussian1D(amplitude=-0.4, mean=6550, stddev=2) + \\\n            models.Gaussian1D(amplitude=-0.2, mean=6553, stddev=3)\n\n# Fit the model to the data\nfitter = fitting.LevMarLSQFitter()\nfitted_model = fitter(init_model, wavelength.value, spectrum)\n\n# Get the fitted components\nfitted_g1 = fitted_model[0]\nfitted_g2 = fitted_model[1]\n\n# Print results\nprint(f\"First line: center={fitted_g1.mean.value:.1f}, amplitude={fitted_g1.amplitude.value:.2f}\")\nprint(f\"Second line: center={fitted_g2.mean.value:.1f}, amplitude={fitted_g2.amplitude.value:.2f}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "How can I extract the spectral data from a specific science extension of my FITS cube to analyze emission lines in a galaxy spectrum?"
         },
         {
            "from": "assistant",
            "value": "from astropy.io import fits\nimport numpy as np\nimport os\n\ndef process_fits_cube(filepath):\n    # Check if file exists\n    if not os.path.exists(filepath):\n        raise FileNotFoundError(f\"The file {filepath} does not exist\")\n    \n    try:\n        # Open the FITS cube file\n        hdul = fits.open(filepath)\n        \n        # Extract data from the primary science extension (usually index 1 for spectral data)\n        # First check if the extension exists\n        if len(hdul) < 2:\n            science_data = hdul[0].data  # Use primary HDU if no extensions\n        else:\n            science_data = hdul[1].data\n            \n        # Check if we have 3D data\n        if science_data.ndim != 3:\n            raise ValueError(\"Data is not a 3D cube\")\n            \n        # Get wavelength information from header (if available)\n        header = hdul[1].header if len(hdul) > 1 else hdul[0].header\n        \n        if 'CRVAL3' in header:  # Check for wavelength reference value\n            wave_start = header['CRVAL3']\n            wave_step = header['CDELT3']\n            wave_pixels = np.arange(science_data.shape[0])\n            wavelength = wave_start + wave_step * wave_pixels\n        else:\n            wavelength = np.arange(science_data.shape[0])  # Default to pixel indices\n            print(\"Warning: No wavelength calibration found in header\")\n\n        # Extract spectrum from a specific spatial position (example: center of galaxy)\n        center_y = science_data.shape[1] // 2  # Center y-pixel\n        center_x = science_data.shape[2] // 2  # Center x-pixel\n        spectrum = science_data[:, center_y, center_x]\n\n        # Basic emission line analysis (example: find peaks above certain threshold)\n        threshold = np.mean(spectrum) + 2 * np.std(spectrum)\n        emission_peaks = np.where(spectrum > threshold)[0]\n        \n        # Return results\n        results = {\n            'spectrum': spectrum,\n            'wavelength': wavelength,\n            'emission_peaks': emission_peaks,\n            'center_coordinates': (center_x, center_y)\n        }\n        \n        return results\n        \n    except Exception as e:\n        print(f\"Error processing FITS file: {str(e)}\")\n        raise\n        \n    finally:\n        # Clean up\n        if 'hdul' in locals():\n            hdul.close()\n\n# Example usage\nif __name__ == \"__main__\":\n    try:\n        # Specify the path to your FITS file\n        fits_file = 'path/to/your/galaxy_cube.fits'  # Replace with actual path\n        \n        results = process_fits_cube(fits_file)\n        \n        # Print some basic results\n        print(f\"Found {len(results['emission_peaks'])} emission peaks\")\n        print(f\"Spectrum extracted from position: {results['center_coordinates']}\")\n        \n    except FileNotFoundError:\n        print(\"Please check the file path and ensure the FITS file exists\")\n    except Exception as e:\n        print(f\"An error occurred: {str(e)}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm studying stellar kinematics and need to create an FK4 coordinate for a star with both position (RA=18h 36m 56.3s, Dec=+38d 47m 01.0s) and radial velocity (25 km/s) for the epoch B1950. How do I set this up in astropy?"
         },
         {
            "from": "assistant",
            "value": "from astropy.coordinates import FK4, SkyCoord\nfrom astropy import units as u\n\n# Create FK4 coordinates with position and radial velocity\n# Note: FK4 is an older system, typically used with B1950 epoch\nstar_coords = SkyCoord(\n    ra='18h36m56.3s', \n    dec='+38d47m01.0s',\n    frame=FK4,\n    radial_velocity=25 * u.km/u.s,  # Add radial velocity\n    obstime='B1950'  # Specify B1950 epoch\n)\n\n# Print the resulting coordinate object\nprint(\"Star position (FK4, B1950):\")\nprint(f\"RA: {star_coords.ra.to_string(unit=u.hour, sep='hms')}\")\nprint(f\"Dec: {star_coords.dec.to_string(unit=u.degree, sep='dms')}\")\nprint(f\"Radial velocity: {star_coords.radial_velocity}\")"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm analyzing pulsar timing data and need to create a properly formatted ASCII table with specific precision for period measurements and custom column headers. Can you show me how to output this data?"
         },
         {
            "from": "assistant",
            "value": "from astropy.table import Table\nfrom astropy import units as u\nimport numpy as np\n\n# Sample pulsar timing data\ndata = {\n    'pulsar_name': ['PSR J0534+2200', 'PSR J0537-6910', 'PSR J1813-1246'],\n    'period': [0.033578350254, 0.016122981429, 0.048066875231],\n    'p_dot': [4.21e-13, 5.18e-14, 1.76e-14],\n    'epoch': ['58932.0', '58945.2', '58901.7']\n}\n\n# Create Table object\npsr_table = Table(data)\n\n# Set column formats and units\npsr_table['period'].format = '%.12f'  # 12 decimal precision for period\npsr_table['p_dot'].format = '%.2e'    # Scientific notation for period derivative\npsr_table['epoch'].format = '%s'      # String format for epoch\n\n# Set custom column headers\npsr_table.rename_column('pulsar_name', 'PSR Name')\npsr_table.rename_column('period', 'P (s)')\npsr_table.rename_column('p_dot', 'Pdot (s/s)')\npsr_table.rename_column('epoch', 'MJD')\n\n# Write to ASCII file with a specific format\npsr_table.write('pulsar_timing.dat', format='ascii.fixed_width', \n                delimiter=' ', overwrite=True)\n\n# Display table in notebook/console\nprint(psr_table)"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "For a presentation on diffraction limits, I need to generate a comparison between two Airy disk patterns. Please show me how to create two AiryDisk2D models with the same center position (100, 100) and amplitude (1.0), but different radii (2.0 and 4.0) to demonstrate the effect of aperture size on resolution."
         },
         {
            "from": "assistant",
            "value": "from astropy.modeling.models import AiryDisk2D\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Create coordinate grid\ny, x = np.mgrid[0:200, 0:200]\n\n# Create two Airy disk models with different radii\nairy_small = AiryDisk2D(amplitude=1.0, x_0=100, y_0=100, radius=2.0)\nairy_large = AiryDisk2D(amplitude=1.0, x_0=100, y_0=100, radius=4.0)\n\n# Evaluate models on the grid\npattern_small = airy_small(x, y)\npattern_large = airy_large(x, y)\n\n# Create subplot for comparison\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n\n# Plot both patterns with same intensity scale\nvmax = max(np.max(pattern_small), np.max(pattern_large))\nim1 = ax1.imshow(pattern_small, cmap='viridis', vmax=vmax)\nim2 = ax2.imshow(pattern_large, cmap='viridis', vmax=vmax)\n\n# Add titles and labels\nax1.set_title('Airy Disk (radius=2.0)')\nax2.set_title('Airy Disk (radius=4.0)')\nfig.colorbar(im1, ax=ax1, label='Intensity')\nfig.colorbar(im2, ax=ax2, label='Intensity')\n\nplt.tight_layout()\nplt.show()"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm working with data from multiple observatories that tracked the same supernova. How can I combine their magnitude measurements into a single table, matching the observations by both time and filter band?"
         },
         {
            "from": "assistant",
            "value": "from astropy.table import Table, vstack, join\nfrom astropy.time import Time\nimport numpy as np\n\n# Create example data tables for demonstration\n# Observatory 1 data\nobs1 = Table({\n    'time': Time(['2023-01-01T00:00:00', '2023-01-02T00:00:00', '2023-01-03T00:00:00']),\n    'magnitude': [18.5, 18.3, 18.1],\n    'mag_error': [0.1, 0.1, 0.1],\n    'filter': ['r', 'g', 'r'],\n    'observatory': ['Obs1'] * 3\n})\n\n# Observatory 2 data\nobs2 = Table({\n    'time': Time(['2023-01-01T01:00:00', '2023-01-02T00:00:00', '2023-01-04T00:00:00']),\n    'magnitude': [18.4, 18.2, 17.9],\n    'mag_error': [0.15, 0.12, 0.11],\n    'filter': ['r', 'g', 'r'],\n    'observatory': ['Obs2'] * 3\n})\n\n# Define time matching tolerance (e.g., 1 day)\ntime_tolerance = 1.0  # days\n\n# Convert time columns to Modified Julian Date for easier comparison\nobs1['mjd'] = obs1['time'].mjd\nobs2['mjd'] = obs2['time'].mjd\n\n# Combine tables\ncombined = vstack([obs1, obs2])\n\n# Group by filter and similar timestamps\ngrouped = combined.group_by(['filter'])\nfor key, group in zip(grouped.groups.keys, grouped.groups):\n    # Sort by time\n    group.sort('mjd')\n    \n    # Find observations within time_tolerance\n    unique_times = np.unique(group['mjd'])\n    mask = np.abs(group['mjd'] - group['mjd'][:, np.newaxis]) <= time_tolerance\n    \n# Create final table with matched observations\nfinal_table = Table({\n    'mjd': combined['mjd'],\n    'filter': combined['filter'],\n    'magnitude': combined['magnitude'],\n    'mag_error': combined['mag_error'],\n    'observatory': combined['observatory']\n})\n\n# Sort final table by time and filter\nfinal_table.sort(['mjd', 'filter'])"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   },
   {
      "conversations": [
         {
            "from": "user",
            "value": "I'm analyzing solar flare intensity data and want to reduce the noise while maintaining the sharp transitions. Can you show me how to apply a box smoothing kernel that won't significantly blur the flare edges?"
         },
         {
            "from": "assistant",
            "value": "from astropy.convolution import Box1DKernel, convolve\nimport numpy as np\n\n# Create sample solar flare data (example data)\ntime = np.linspace(0, 100, 1000)\nflare_intensity = np.zeros(1000)\n# Simulate a flare peak with noise\nflare_intensity[400:600] = 100 * np.exp(-(time[400:600]-50)**2/100)\nflare_intensity += np.random.normal(0, 5, 1000)\n\n# Create a narrow box kernel (width=5 to preserve sharp transitions)\nkernel = Box1DKernel(width=5)\n\n# Apply convolution with edge handling\nsmoothed_intensity = convolve(flare_intensity, kernel, boundary='extend')\n\n# The smoothed_intensity array now contains the filtered data\n# with reduced noise but preserved flare edges"
         }
      ],
      "system": "You are an AI coding assistant, specialized for astronomy."
   }
]